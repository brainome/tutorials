{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfebebd9",
   "metadata": {},
   "source": [
    "![](https://www.brainome.ai/wp-content/uploads/2020/08/brainome_logo.png)\n",
    "# 103 Model Selection\n",
    "Brainome creates predictors using these three models.\n",
    "1. Automatic\n",
    "2. Random Forest\n",
    "3. Neural Network\n",
    "4. Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474a59d",
   "metadata": {},
   "source": [
    "## Install brainome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "655a3e12",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: brainome in /opt/conda/lib/python3.9/site-packages (1.6.68)\n",
      "Requirement already satisfied: brainome-linux-python3.9==1.6.* in /opt/conda/lib/python3.9/site-packages (from brainome) (1.6.14)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (2.26.0)\n",
      "Requirement already satisfied: xgboost==1.4.2 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (0.24.2)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (1.9.0)\n",
      "Requirement already satisfied: Jinja2>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (1.21.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from xgboost==1.4.2->brainome-linux-python3.9==1.6.*->brainome) (1.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2>=3.0.0->brainome-linux-python3.9==1.6.*->brainome) (2.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-linux-python3.9==1.6.*->brainome) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-linux-python3.9==1.6.*->brainome) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.4.0->brainome-linux-python3.9==1.6.*->brainome) (3.10.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (2021.5.30)\n"
     ]
    }
   ],
   "source": [
    "# pip install brainome \n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade brainome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33177319",
   "metadata": {},
   "source": [
    "## 1. Automatic Model Selection\n",
    "Brainome can automatically selects the most appropriate for your measurements model type.\n",
    "In titanic's case, it is Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4dc7f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "\u001b[01;1mBrainome Table Compiler v1.006-14-prod\u001b[0m\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:                 y Demo User  (Evaluation)\n",
      "Expiration Date:             2021-12-12   129 days left\n",
      "Maximum File Size:           100 MB\n",
      "Maximum Instances:           20000\n",
      "Maximum Attributes:          100\n",
      "Maximum Classes:             unlimited\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\u001b[01;1mCommand:\u001b[0m\n",
      "    btc data/titanic_train.csv -y -o automatic_predictor.py\n",
      "\n",
      "Start Time:                 08/05/2021, 18:54 UTC\n",
      "\n",
      "Cleaning...done. \n",
      "Splitting into training and validation...done. \n",
      "Pre-training measurements...done. \n",
      "\n",
      "\n",
      "\u001b[01;1mPre-training Measurements\u001b[0m\n",
      "Data:\n",
      "    Input:                      data/titanic_train.csv\n",
      "    Target Column:              Survived\n",
      "    Number of instances:        800\n",
      "    Number of attributes:        11 out of 11\n",
      "    Number of classes:            2\n",
      "\n",
      "Class Balance:                \n",
      "                            died: 61.50%\n",
      "                        survived: 38.50%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          61.50%\n",
      "    Data Sufficiency:             Maybe enough data to generalize. [yellow]\n",
      "\n",
      "Capacity Progression:             at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Ideal Machine Learner:              6,   7,   8,   8,   9,   9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expected Generalization:\n",
      "    Decision Tree:                 2.02 bits/bit\n",
      "    Neural Network:                6.52 bits/bit\n",
      "    Random Forest:                10.13 bits/bit\n",
      "\n",
      "Expected Accuracy:              Training            Validation\n",
      "    Decision Tree:               100.00%                52.50%\n",
      "    Neural Network:                 ----                  ----\n",
      "    Random Forest:               100.00%                80.25%\n",
      "\n",
      "Recommendations:\n",
      "    Warning: Data has high information density. Using effort 5 and larger ( -e 5 ) can improve results.\n",
      "    We recommend using Random Forest -f RF.\n",
      "    If predictor accuracy is insufficient, try using the option -rank to automatically select the important attributes.\n",
      "    If predictor accuracy is insufficient, try using the effort option -e with a value of 5 or more to increase training time.\n",
      "    Defaulting to RF model. Model can be forced with -f parameter. \n",
      "\n",
      "\n",
      "Building classifier...done. \n",
      "Compiling predictor...done. \n",
      "Validating predictor...done. \n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        automatic_predictor.py\n",
      "    Classifier Type:              Random Forest\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          86.84% (416/479 correct)\n",
      "      Validation Accuracy:        80.99% (260/321 correct)\n",
      "      Combined Model Accuracy:    84.50% (676/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         41    bits\n",
      "\n",
      "    Generalization Ratio:          9.74 bits/bit\n",
      "    Percent of Data Memorized:    20.84%\n",
      "    Resilience to Noise:          -1.01 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  279   16 \n",
      "            survived |   47  137 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  175   22 \n",
      "            survived |   39   85 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  279   47  137   16   94.58%   74.46%   85.58%   89.54%   89.86%   81.58%\n",
      "            survived |  137   16  279   47   74.46%   94.58%   89.54%   85.58%   81.31%   68.50%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  175   39   85   22   88.83%   68.55%   81.78%   79.44%   85.16%   74.15%\n",
      "            survived |   85   22  175   39   68.55%   88.83%   79.44%   81.78%   73.59%   58.22%\n",
      "\n",
      "\n",
      "    Attribute Ranking:\n",
      "                                      Feature | Relative Importance\n",
      "                                          Sex :   0.4912\n",
      "                                  Cabin_Class :   0.1242\n",
      "                                 Cabin_Number :   0.0664\n",
      "                              Parent_Children :   0.0599\n",
      "                                          Age :   0.0599\n",
      "                                Ticket_Number :   0.0414\n",
      "                                         Fare :   0.0379\n",
      "                                  PassengerId :   0.0332\n",
      "                               Sibling_Spouse :   0.0298\n",
      "                                         Name :   0.0288\n",
      "                          Port_of_Embarkation :   0.0273\n",
      "         \n",
      "\n",
      "\n",
      "\n",
      "End Time:           08/05/2021, 18:54 UTC\n",
      "Runtime Duration:   8s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!brainome data/titanic_train.csv -y -o automatic_predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f889a13",
   "metadata": {},
   "source": [
    "## 2. Random Forest\n",
    "You can force selection of Random Forest by using the **-f RF** parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de2aa21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "\u001b[01;1mBrainome Table Compiler v1.006-14-prod\u001b[0m\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:                 y Demo User  (Evaluation)\n",
      "Expiration Date:             2021-12-12   129 days left\n",
      "Maximum File Size:           100 MB\n",
      "Maximum Instances:           20000\n",
      "Maximum Attributes:          100\n",
      "Maximum Classes:             unlimited\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\u001b[01;1mCommand:\u001b[0m\n",
      "    btc data/titanic_train.csv -f RF -y -o RF_predictor.py\n",
      "\n",
      "Start Time:                 08/05/2021, 18:54 UTC\n",
      "\n",
      "Cleaning...done. \n",
      "Splitting into training and validation...done. \n",
      "Pre-training measurements...done. \n",
      "\n",
      "\n",
      "\u001b[01;1mPre-training Measurements\u001b[0m\n",
      "Data:\n",
      "    Input:                      data/titanic_train.csv\n",
      "    Target Column:              Survived\n",
      "    Number of instances:        800\n",
      "    Number of attributes:        11 out of 11\n",
      "    Number of classes:            2\n",
      "\n",
      "Class Balance:                \n",
      "                            died: 61.50%\n",
      "                        survived: 38.50%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          61.50%\n",
      "    Data Sufficiency:             Maybe enough data to generalize. [yellow]\n",
      "\n",
      "Capacity Progression:             at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Ideal Machine Learner:              6,   7,   8,   8,   9,   9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expected Generalization:\n",
      "    Decision Tree:                 2.02 bits/bit\n",
      "    Neural Network:                6.52 bits/bit\n",
      "    Random Forest:                10.13 bits/bit\n",
      "\n",
      "Expected Accuracy:              Training            Validation\n",
      "    Decision Tree:               100.00%                52.50%\n",
      "    Neural Network:                 ----                  ----\n",
      "    Random Forest:               100.00%                80.25%\n",
      "\n",
      "Recommendations:\n",
      "    Warning: Data has high information density. Using effort 5 and larger ( -e 5 ) can improve results.\n",
      "    We recommend using Random Forest -f RF.\n",
      "    If predictor accuracy is insufficient, try using the option -rank to automatically select the important attributes.\n",
      "    If predictor accuracy is insufficient, try using the effort option -e with a value of 5 or more to increase training time.\n",
      "    Model type RF given by user. \n",
      "\n",
      "\n",
      "Building classifier...done. \n",
      "Compiling predictor...done. \n",
      "Validating predictor...done. \n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        RF_predictor.py\n",
      "    Classifier Type:              Random Forest\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          86.84% (416/479 correct)\n",
      "      Validation Accuracy:        80.99% (260/321 correct)\n",
      "      Combined Model Accuracy:    84.50% (676/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         41    bits\n",
      "\n",
      "    Generalization Ratio:          9.74 bits/bit\n",
      "    Percent of Data Memorized:    20.84%\n",
      "    Resilience to Noise:          -1.01 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  279   16 \n",
      "            survived |   47  137 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  175   22 \n",
      "            survived |   39   85 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  279   47  137   16   94.58%   74.46%   85.58%   89.54%   89.86%   81.58%\n",
      "            survived |  137   16  279   47   74.46%   94.58%   89.54%   85.58%   81.31%   68.50%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  175   39   85   22   88.83%   68.55%   81.78%   79.44%   85.16%   74.15%\n",
      "            survived |   85   22  175   39   68.55%   88.83%   79.44%   81.78%   73.59%   58.22%\n",
      "\n",
      "\n",
      "    Attribute Ranking:\n",
      "                                      Feature | Relative Importance\n",
      "                                          Sex :   0.4912\n",
      "                                  Cabin_Class :   0.1242\n",
      "                                 Cabin_Number :   0.0664\n",
      "                              Parent_Children :   0.0599\n",
      "                                          Age :   0.0599\n",
      "                                Ticket_Number :   0.0414\n",
      "                                         Fare :   0.0379\n",
      "                                  PassengerId :   0.0332\n",
      "                               Sibling_Spouse :   0.0298\n",
      "                                         Name :   0.0288\n",
      "                          Port_of_Embarkation :   0.0273\n",
      "         \n",
      "\n",
      "\n",
      "\n",
      "End Time:           08/05/2021, 18:54 UTC\n",
      "Runtime Duration:   8s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!brainome data/titanic_train.csv -f RF -y -o RF_predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a19649",
   "metadata": {},
   "source": [
    "### View Random Forest Predictor Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38ac1e9",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "#\n",
      "# This code has been produced by a free evaluation version of Brainome(tm).\n",
      "# Portions of this code copyright (c) 2019-2021 by Brainome, Inc. All Rights Reserved.\n",
      "# Brainome, Inc grants an exclusive (subject to our continuing rights to use and modify models),\n",
      "# worldwide, non-sublicensable, and non-transferable limited license to use and modify this\n",
      "# predictor produced through the input of your data:\n",
      "# (i) for users accessing the service through a free evaluation account, solely for your\n",
      "# own non-commercial purposes, including for the purpose of evaluating this service, and\n",
      "# (ii) for users accessing the service through a paid, commercial use account, for your\n",
      "# own internal  and commercial purposes.\n",
      "# Please contact support@brainome.ai with any questions.\n",
      "# Use of predictions results at your own risk.\n",
      "#\n",
      "# Output of Brainome v1.006-14-prod.\n",
      "# Invocation: brainome data/titanic_train.csv -f RF -y -o RF_predictor.py\n",
      "# Total compiler execution time: 0:00:08.63. Finished on: Aug-05-2021 18:54:18.\n",
      "# This source code requires Python 3.\n",
      "#\n",
      "\"\"\"\n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        RF_predictor.py\n",
      "    Classifier Type:              Random Forest\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          86.84% (416/479 correct)\n",
      "      Validation Accuracy:        80.99% (260/321 correct)\n",
      "      Combined Model Accuracy:    84.50% (676/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         41    bits\n",
      "\n",
      "    Generalization Ratio:          9.74 bits/bit\n",
      "    Percent of Data Memorized:    20.84%\n",
      "    Resilience to Noise:          -1.01 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  279   16 \n",
      "            survived |   47  137 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  175   22 \n",
      "            survived |   39   85 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  279   47  137   16   94.58%   74.46%   85.58%   89.54%   89.86%   81.58%\n",
      "            survived |  137   16  279   47   74.46%   94.58%   89.54%   85.58%   81.31%   68.50%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  175   39   85   22   88.83%   68.55%   81.78%   79.44%   85.16%   74.15%\n",
      "            survived |   85   22  175   39   68.55%   88.83%   79.44%   81.78%   73.59%   58.22%\n",
      "\n",
      "\n",
      "    Attribute Ranking:\n",
      "                                      Feature | Relative Importance\n",
      "                                          Sex :   0.4912\n",
      "                                  Cabin_Class :   0.1242\n",
      "                                 Cabin_Number :   0.0664\n",
      "                              Parent_Children :   0.0599\n",
      "                                          Age :   0.0599\n",
      "                                Ticket_Number :   0.0414\n",
      "                                         Fare :   0.0379\n",
      "                                  PassengerId :   0.0332\n",
      "                               Sibling_Spouse :   0.0298\n",
      "                                         Name :   0.0288\n",
      "                          Port_of_Embarkation :   0.0273\n",
      "         \n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "import math\n",
      "import os\n",
      "import argparse\n",
      "import tempfile\n",
      "import csv\n",
      "import binascii\n",
      "import faulthandler\n",
      "import json\n",
      "from io import StringIO\n",
      "try:\n",
      "    import numpy as np # For numpy see: http://numpy.org\n",
      "    from numpy import array\n",
      "except:\n",
      "    print(\"This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.\")\n",
      "    sys.exit(1)\n",
      "try:\n",
      "    from scipy.sparse import coo_matrix\n",
      "    report_cmat = True\n",
      "except:\n",
      "    print(\"Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.\")\n",
      "    report_cmat = False\n",
      "try:\n",
      "    import multiprocessing\n",
      "    var_dict = {}\n",
      "    default_to_serial = False\n",
      "except:\n",
      "    default_to_serial = True\n",
      "\n",
      "IOBUF = 100000000\n",
      "sys.setrecursionlimit(1000000)\n",
      "TRAINFILE = ['data/titanic_train.csv']\n",
      "mapping = {'died': 0, 'survived': 1}\n",
      "ignorelabels = []\n",
      "ignorecolumns = []\n",
      "target = '' \n",
      "target_column = 11\n",
      "important_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "ignore_idxs = []\n",
      "classifier_type = 'RF'\n",
      "num_attr = 11\n",
      "n_classes = 2\n",
      "model_cap = 41\n",
      "logits_dict = {0: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.36425662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.243536353, 0.191349998, 0.0098128207, 0.331673324, -0.0, -0.255133331, 0.0883153826, 0.298732072, -0.287025005, 0.0765400007, 0.185177416, 0.0153079992]), 1: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.36425662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.243536353, -0.191349998, -0.0098128207, -0.331673324, -0.0, 0.255133331, -0.0883153826, -0.298732072, 0.287025005, -0.0765400007, -0.185177416, -0.0153079992]), 2: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.265477031, 0.0, 0.270277321, 0.0, 0.0, 0.0, 0.0, -0.150360435, 0.00850328244, 0.185843408, -0.0649351925, -0.116907045, 0.0839446634, -0.0411008634, 0.232498676, 0.0817588419, 0.235827729]), 3: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.265477002, 0.0, -0.270277321, 0.0, 0.0, 0.0, 0.0, 0.15036045, -0.00850327779, -0.185843378, 0.0649352148, 0.116907068, -0.0839446336, 0.041100882, -0.232498676, -0.0817588121, -0.235827684]), 4: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.254825503, 0.0, 0.0, -0.123233855, -0.235735834, 0.039184168, -0.188743472, -0.153517619, 0.112402298, 0.196248844, -0.115611948, -0.191678584, 0.120147459, 0.160565287, 0.0143993665]), 5: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.254825503, 0.0, 0.0, 0.123233832, 0.235735834, -0.039184168, 0.188743442, 0.153517663, -0.112402275, -0.196248844, 0.11561197, 0.191678569, -0.120147429, -0.160565287, -0.0143993739]), 6: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.214141831, 0.0, 0.0, 0.0, 0.222953677, 0.0, 0.0, 0.0810240507, -0.186694652, 0.0149209201, -0.165880695, 0.0454831272, 0.233664706, 0.164316222, -0.0880754292, 0.105792671, -0.078477487, 0.0124567663, 0.118342534]), 7: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.214141831, 0.0, 0.0, 0.0, -0.222953692, 0.0, 0.0, -0.0810240358, 0.186694652, -0.0149209267, 0.16588068, -0.0454831496, -0.233664706, -0.164316207, 0.0880754441, -0.105792664, 0.078477487, -0.0124568064, -0.118342534])}\n",
      "right_children_dict = {0: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 1: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 2: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, -1, 17, 19, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 3: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, -1, 17, 19, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 4: array([1, 3, 5, 7, 9, 11, 13, -1, 15, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 5: array([1, 3, 5, 7, 9, 11, 13, -1, 15, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 6: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 7: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])}\n",
      "split_feats_dict = {0: array([3, 1, 9, 7, 4, 7, 7, 0, 0, 7, 8, 0, 7, 9, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 1: array([3, 1, 9, 7, 4, 7, 7, 0, 0, 7, 8, 0, 7, 9, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 2: array([3, 1, 7, 7, 4, 8, 1, 0, 0, 8, 0, 6, 7, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 3: array([3, 1, 7, 7, 4, 8, 1, 0, 0, 8, 0, 6, 7, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 4: array([3, 1, 8, 7, 4, 7, 4, 0, 8, 7, 8, 0, 0, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 5: array([3, 1, 8, 7, 4, 7, 4, 0, 8, 7, 8, 0, 0, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 6: array([3, 1, 8, 0, 8, 7, 7, 8, 0, 7, 2, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 7: array([3, 1, 8, 0, 8, 7, 7, 8, 0, 7, 2, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "split_vals_dict = {0: array([1342256510.0, 2.5, 99463000.0, 11753.5, 6.5, 2621.5, 17458.0, 0.0, 0.0, 1855777020.0, 17.5999985, 340.0, 2666.0, 1777625860.0, 113785.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 1: array([1342256510.0, 2.5, 99463000.0, 11753.5, 6.5, 2621.5, 17458.0, 0.0, 0.0, 1855777020.0, 17.5999985, 340.0, 2666.0, 1777625860.0, 113785.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 2: array([1342256510.0, 2.5, 314014.0, 11753.5, 36.5, 26.2749996, 1.5, 0.0, 0.0, 8.03960037, 0.0, 0.5, 28396.0, 2686373380.0, 299527200.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 3: array([1342256510.0, 2.5, 314014.0, 11753.5, 36.5, 26.2749996, 1.5, 0.0, 0.0, 8.03960037, 0.0, 0.5, 28396.0, 2686373380.0, 299527200.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 4: array([1342256510.0, 2.5, 7.76249981, 11753.5, 6.5, 2666.0, 13.0, 0.0, 13.25, 6683.5, 7.69999981, 543.0, 0.0, 314014.0, 669928000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 5: array([1342256510.0, 2.5, 7.76249981, 11753.5, 6.5, 2666.0, 13.0, 0.0, 13.25, 6683.5, 7.69999981, 543.0, 0.0, 314014.0, 669928000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 6: array([1342256510.0, 2.5, 7.76249981, 208.0, 24.8083496, 2666.0, 27627.5, 22.0, 0.0, 366226.0, 2345232900.0, 543.0, 0.0, 14.8520498, 11.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 7: array([1342256510.0, 2.5, 7.76249981, 208.0, 24.8083496, 2666.0, 27627.5, 22.0, 0.0, 366226.0, 2345232900.0, 543.0, 0.0, 14.8520498, 11.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])}\n",
      "\n",
      "\n",
      "def __convert(cell):\n",
      "    value = str(cell)\n",
      "    try:\n",
      "        result = int(value)\n",
      "        return result\n",
      "    except ValueError:\n",
      "        try:\n",
      "            result=float(value)\n",
      "            if math.isnan(result):\n",
      "                print('NaN value found. Aborting.')\n",
      "                sys.exit(1)\n",
      "            return result\n",
      "        except ValueError:\n",
      "            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def __get_key(val, dictionary):\n",
      "    if dictionary == {}:\n",
      "        return val\n",
      "    for key, value in dictionary.items(): \n",
      "        if val == value:\n",
      "            return key\n",
      "    if val not in dictionary.values:\n",
      "        print(\"Label key does not exist\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __convertclassid(cell, classlist=[]):\n",
      "\n",
      "    value = str(cell)\n",
      "    \n",
      "    if value == '':\n",
      "        print('Empty value encountered for a class label. Aborting.')\n",
      "        sys.exit(1)\n",
      "    \n",
      "    if mapping != {}:\n",
      "        result = -1\n",
      "        try:\n",
      "            result = mapping[cell]\n",
      "        except KeyError:\n",
      "            print(f\"The class label {value} does not exist in the class mapping. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "        return result\n",
      "    \n",
      "    try:\n",
      "        result = float(cell)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "    except:\n",
      "        result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "        if result in classlist:\n",
      "            result = classlist.index(result)\n",
      "        else:\n",
      "            classlist.append(str(result))\n",
      "            result = classlist.index(result)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "    finally:\n",
      "        if result < 0:\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to non-negative integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "    return result\n",
      "\n",
      "\n",
      "def __clean(filename, outfile, headerless=False, testfile=False, trim=False):\n",
      "    classlist = []\n",
      "    outbuf = []\n",
      "    remove_bad_chars = lambda x: x.replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    \n",
      "    with open(filename, encoding='utf-8') as csv_file, open(outfile, \"w+\", encoding='utf-8') as f:\n",
      "        \n",
      "        reader = csv.reader(csv_file)\n",
      "        if not headerless:\n",
      "            next(reader, None)\n",
      "        \n",
      "        for i, row in enumerate(reader):\n",
      "\n",
      "            if row == []:\n",
      "                continue\n",
      "\n",
      "            \n",
      "            expected_row_length = len(important_idxs)\n",
      "            if not trim:\n",
      "                expected_row_length += len(ignorecolumns)\n",
      "            if not testfile:\n",
      "                expected_row_length += 1\n",
      "            actual_row_length = len(row)\n",
      "\n",
      "            if testfile and actual_row_length == expected_row_length + 1:\n",
      "                error_str = f\"We found {actual_row_length} columns but expected {expected_row_length} columns at row {i}. \"\n",
      "                error_str += f\"Please check that the CSV contains no target column otherwise use -validate. Aborting.\"\n",
      "                print(error_str)\n",
      "                sys.exit(1)\n",
      "            \n",
      "            if actual_row_length != expected_row_length:\n",
      "                print(f\"We found {actual_row_length} columns but expected {expected_row_length} columns.\")\n",
      "                sys.exit(1)            \n",
      "\n",
      "            if testfile:\n",
      "                if len(row) == 1:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(row[0])))]\n",
      "                else:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convert(remove_bad_chars(row[-1])))]         \n",
      "            else:\n",
      "                converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convertclassid(row[-1], classlist))]\n",
      "            outbuf.extend(converted_row)\n",
      "\n",
      "            if len(outbuf) < IOBUF:\n",
      "                outbuf.append(os.linesep)\n",
      "            else:\n",
      "                print(''.join(outbuf), file=f)\n",
      "                outbuf = []\n",
      "        \n",
      "        print(''.join(outbuf), end=\"\", file=f)\n",
      "\n",
      "    n_classes_found = len(classlist)\n",
      "    if not testfile and n_classes_found < 2:\n",
      "        print(f\"Only {n_classes_found} classes were found. Aborting.\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):\n",
      "    stats = {}\n",
      "    if labels is None:\n",
      "        labels = np.array(list(set(list(y_true.astype('int')))))\n",
      "    else:\n",
      "        labels = np.asarray(labels)\n",
      "        if np.all([l not in y_true for l in labels]):\n",
      "            raise ValueError(\"At least one label specified must be in y_true\")\n",
      "    n_labels = labels.size\n",
      "\n",
      "    for class_i in range(n_labels):\n",
      "        stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}\n",
      "        class_i_indices = np.argwhere(y_true==class_i)\n",
      "        not_class_i_indices = np.argwhere(y_true!=class_i)\n",
      "        stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices] == class_i))\n",
      "        stats[int(class_i)]['FN'] = int(np.sum(y_pred[class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['FP'] = int(np.sum(y_pred[not_class_i_indices] == class_i))\n",
      "\n",
      "    if not report_cmat:\n",
      "        if json:\n",
      "            return np.array([]), stats\n",
      "        else:\n",
      "            sys.exit(0)\n",
      "\n",
      "    if sample_weight is None:\n",
      "        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n",
      "    else:\n",
      "        sample_weight = np.asarray(sample_weight)\n",
      "    if y_true.shape[0]!=y_pred.shape[0]:\n",
      "        raise ValueError(\"y_true and y_pred must be of the same length\")\n",
      "\n",
      "    if normalize not in ['true', 'pred', 'all', None]:\n",
      "        raise ValueError(\"normalize must be one of {'true', 'pred', 'all', None}\")\n",
      "\n",
      "\n",
      "    label_to_ind = {y: x for x, y in enumerate(labels)}\n",
      "    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\n",
      "    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\n",
      "    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n",
      "    y_pred = y_pred[ind]\n",
      "    y_true = y_true[ind]\n",
      "\n",
      "    sample_weight = sample_weight[ind]\n",
      "    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\n",
      "        dtype = np.int64\n",
      "    else:\n",
      "        dtype = np.float64\n",
      "    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()\n",
      "\n",
      "    with np.errstate(all='ignore'):\n",
      "        if normalize == 'true':\n",
      "            cm = cm / cm.sum(axis=1, keepdims=True)\n",
      "        elif normalize == 'pred':\n",
      "            cm = cm / cm.sum(axis=0, keepdims=True)\n",
      "        elif normalize == 'all':\n",
      "            cm = cm / cm.sum()\n",
      "        cm = np.nan_to_num(cm)\n",
      "    return cm, stats\n",
      "\n",
      "\n",
      "def __predict(arr, headerless, csvfile, trim=False):\n",
      "    with open(csvfile, 'r', encoding='utf-8') as csvinput:\n",
      "        reader = csv.reader(csvinput)\n",
      "        if not headerless:\n",
      "            if trim:\n",
      "                header = ','.join([x for i, x in enumerate(next(reader, None)) if i in important_idxs] + ['Prediction'])\n",
      "            else:\n",
      "                header = ','.join(next(reader, None) + ['Prediction'])\n",
      "            print(header)\n",
      "        outputs = __classify(arr)\n",
      "        for i, row in enumerate(reader):\n",
      "            pred = str(__get_key(int(outputs[i]), mapping))\n",
      "            if trim:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]\n",
      "            else:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for field in row]            \n",
      "            row.append(pred)\n",
      "            print(','.join(row))\n",
      "\n",
      "\n",
      "def __preprocess_and_clean_in_memory(arr):\n",
      "    if not isinstance(arr, list) and not isinstance(arr, np.ndarray):\n",
      "        print(f'The input to \\'predict\\' must be a list or np.ndarray but an input of type {type(arr).__name__} was found.')\n",
      "        sys.exit(1)\n",
      "    clean_arr = np.zeros((len(arr), len(important_idxs)))\n",
      "    for i, row in enumerate(arr):\n",
      "        try:\n",
      "            row_used_cols_only = [row[i] for i in important_idxs]\n",
      "        except IndexError:\n",
      "            error_str = f\"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {num_attr}).\"\n",
      "            if len(arr) == num_attr and len(arr[0]) != num_attr:\n",
      "                error_str += \"\\n\\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' \"\n",
      "                error_str += \"rather than as an element of a list. Make sure that even single instances \"\n",
      "                error_str += \"are enclosed in a list. Example: predict_in_memory(0) is invalid but \"\n",
      "                error_str += \"predict_in_memory([0]) is valid.\"\n",
      "            print(error_str)\n",
      "            sys.exit(1)\n",
      "        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]\n",
      "    return clean_arr\n",
      "\n",
      "\n",
      "def __evaluate_tree(xs, split_vals, split_feats, right_children, logits):\n",
      "    if xs is None:\n",
      "        xs = np.frombuffer(var_dict['X']).reshape(var_dict['X_shape'])\n",
      "\n",
      "    current_node_per_row = np.zeros(xs.shape[0]).astype('int')\n",
      "    values = np.empty(xs.shape[0])\n",
      "    values.fill(np.nan)\n",
      "\n",
      "    while np.isnan(values).any():\n",
      "\n",
      "        row_idxs_at_leaf = np.argwhere(np.logical_and(right_children[current_node_per_row] == -1, np.isnan(values))).reshape(-1)\n",
      "        row_idxs_at_branch = np.argwhere(right_children[current_node_per_row] != -1).reshape(-1)\n",
      "\n",
      "        if row_idxs_at_leaf.shape[0] > 0:\n",
      "\n",
      "            values[row_idxs_at_leaf] = logits[current_node_per_row[row_idxs_at_leaf]].reshape(-1)\n",
      "            current_node_per_row[row_idxs_at_leaf] = -1\n",
      "\n",
      "        if row_idxs_at_branch.shape[0] > 0:\n",
      "\n",
      "            split_values_per_row = split_vals[current_node_per_row[row_idxs_at_branch]].astype('float64')\n",
      "            split_features_per_row = split_feats[current_node_per_row[row_idxs_at_branch]].astype('int')\n",
      "            feature_val_per_row = xs[row_idxs_at_branch, split_features_per_row].reshape(-1)\n",
      "\n",
      "            branch_nodes = current_node_per_row[row_idxs_at_branch]\n",
      "            current_node_per_row[row_idxs_at_branch] = np.where(feature_val_per_row < split_values_per_row,\n",
      "                                                                right_children[branch_nodes].astype('int'),\n",
      "                                                                (right_children[branch_nodes] + 1).astype('int'))\n",
      "\n",
      "    return values\n",
      "\n",
      "\n",
      "def __build_logit_func(n_trees, clss):\n",
      "\n",
      "    def __logit_func(xs, serial, data_shape, pool=None):\n",
      "        if serial:\n",
      "            sum_of_leaf_values = np.zeros(xs.shape[0])\n",
      "            for booster_index in range(clss, n_trees, n_classes):\n",
      "                sum_of_leaf_values += __evaluate_tree(xs, split_vals_dict[booster_index], split_feats_dict[booster_index],\n",
      "                                                right_children_dict[booster_index], logits_dict[booster_index])\n",
      "        else:\n",
      "            sum_of_leaf_values = np.sum(list(pool.starmap(__evaluate_tree,\n",
      "                                            [(None, split_vals_dict[booster_index], split_feats_dict[booster_index],\n",
      "                                              right_children_dict[booster_index], logits_dict[booster_index])\n",
      "                                    for booster_index in range(clss, n_trees, n_classes)])), axis=0)\n",
      "        return sum_of_leaf_values\n",
      "\n",
      "    return __logit_func\n",
      "\n",
      "\n",
      "def __init_worker(X, X_shape):\n",
      "    var_dict['X'] = X\n",
      "    var_dict['X_shape'] = X_shape\n",
      "\n",
      "\n",
      "def __classify(rows, return_probabilities=False, force_serial=False):\n",
      "    if force_serial:\n",
      "        serial = True\n",
      "    else:\n",
      "        serial = default_to_serial\n",
      "    if isinstance(rows, list):\n",
      "        rows = np.array(rows)\n",
      "\n",
      "    logits = [__build_logit_func(8, clss) for clss in range(n_classes)]\n",
      "\n",
      "    if serial:\n",
      "        o = np.array([logits[class_index](rows, True, rows.shape) for class_index in range(n_classes)]).T\n",
      "    else:\n",
      "        shared_arr = multiprocessing.RawArray('d', rows.shape[0] * rows.shape[1])\n",
      "        shared_arr_np = np.frombuffer(shared_arr, dtype=rows.dtype).reshape(rows.shape)\n",
      "        np.copyto(shared_arr_np, rows)\n",
      "\n",
      "        procs = multiprocessing.cpu_count()\n",
      "        pool = multiprocessing.Pool(processes=procs, initializer=__init_worker, initargs=(shared_arr, rows.shape))\n",
      "        o = np.array([logits[class_index](None, False, rows.shape, pool) for class_index in range(n_classes)]).T\n",
      "\n",
      "    if return_probabilities:\n",
      "        \n",
      "        argument = o[:, 0] - o[:, 1]\n",
      "        p0 = 1.0 / (1.0 + np.exp(-argument)).reshape(-1, 1)\n",
      "        p1 = 1.0 - p0\n",
      "        output = np.concatenate((p0, p1), axis=1)\n",
      "        \n",
      "    else:\n",
      "        output = np.argmax(o,axis=1)\n",
      "    return output\n",
      "\n",
      "\n",
      "def __validate_kwargs(kwargs):\n",
      "    for key in kwargs:\n",
      "        if key not in ['return_probabilities', 'force_serial']:\n",
      "        \n",
      "            print(f'{key} is not a keyword argument for Brainome\\'s {classifier_type} predictor. Please see the documentation.')\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def predict(arr, remap=True, **kwargs):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : list[list]\n",
      "        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'.\n",
      "\n",
      "    remap : bool\n",
      "        If True and 'return_probs' is False, remaps the output to the original class\n",
      "        label. If 'return_probs' is True this instead adds a header indicating which\n",
      "        original class label each column of output corresponds to.\n",
      "    \n",
      "    **kwargs :\n",
      "        return_probabilities : bool\n",
      "            If true, return class membership probabilities instead of classifications.\n",
      "        force_serial : bool\n",
      "            If true, model inference is done in serial rather than in parallel. This is\n",
      "            useful if calling \"predict\" repeatedly inside a for-loop.\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    output : np.ndarray\n",
      "        A numpy array of\n",
      "\n",
      "            1. Class predictions if 'return_probabilities' is False.\n",
      "            2. Class probabilities if 'return_probabilities' is True.\n",
      "        \"\"\"\n",
      "    kwargs = kwargs or {}\n",
      "    __validate_kwargs(kwargs)\n",
      "    remove_bad_chars = lambda x: str(x).replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    arr = [[remove_bad_chars(field) for field in row] for row in arr]\n",
      "    arr = __preprocess_and_clean_in_memory(arr)\n",
      "    output = __classify(arr, **kwargs)\n",
      "    if remap:\n",
      "        if len(output.shape) > 1: # probabilities were returned\n",
      "            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)\n",
      "            output = np.concatenate((header, output), axis=0)\n",
      "        else:\n",
      "            output = np.array([__get_key(prediction, mapping) for prediction in output])\n",
      "    return output\n",
      "\n",
      "\n",
      "def validate(cleanarr):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    cleanarr : np.ndarray\n",
      "        An array of float values that has undergone each pre-\n",
      "        prediction step.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    count : int\n",
      "        A count of the number of instances in cleanarr.\n",
      "\n",
      "    correct_count : int\n",
      "        A count of the number of correctly classified instances in\n",
      "        cleanarr.\n",
      "\n",
      "    numeachclass : dict\n",
      "        A dictionary mapping each class to its number of instances.\n",
      "\n",
      "    outputs : np.ndarray\n",
      "        The output of the predictor's '__classify' method on cleanarr.\n",
      "    \"\"\"\n",
      "    outputs = __classify(cleanarr[:, :-1])\n",
      "    count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0\n",
      "    correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))\n",
      "    count = outputs.shape[0]\n",
      "    num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))\n",
      "    num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))\n",
      "    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs\n",
      "    \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))\n",
      "    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')\n",
      "    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')\n",
      "    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')\n",
      "    parser.add_argument('-json', action=\"store_true\", default=False, help=\"report measurements as json\")\n",
      "    parser.add_argument('-trim', action=\"store_true\", help=\"If true, the prediction will not output ignored columns.\")\n",
      "    args = parser.parse_args()\n",
      "    faulthandler.enable()\n",
      "\n",
      "    if args.validate:\n",
      "        args.trim = True\n",
      "\n",
      "    is_testfile = not args.validate\n",
      "    \n",
      "    cleanfile = tempfile.NamedTemporaryFile().name\n",
      "    __clean(args.csvfile, cleanfile, args.headerless, is_testfile, trim=args.trim)\n",
      "    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')\n",
      "    if len(cleanarr.shape) == 1:\n",
      "        if args.trim and len(important_idxs) == 1:\n",
      "            cleanarr = cleanarr.reshape(-1, 1)\n",
      "        elif len(open(cleanfile, 'r').read().splitlines()) == 1:\n",
      "            cleanarr = cleanarr.reshape(1, -1)\n",
      "\n",
      "    if not args.trim and ignorecolumns != []:\n",
      "        cleanarr = cleanarr[:, important_idxs].reshape(-1, len(important_idxs))\n",
      "\n",
      "    if not args.validate:\n",
      "        __predict(cleanarr, args.headerless, args.csvfile, trim=args.trim)\n",
      "    else:\n",
      "        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(cleanarr)\n",
      "        \n",
      "        true_labels = cleanarr[:, -1]\n",
      "        classcounts = np.bincount(cleanarr[:, -1].astype('int32')).reshape(-1)\n",
      "        classbalance = (classcounts[np.argwhere(classcounts > 0)] / cleanarr.shape[0]).reshape(-1).tolist()\n",
      "        best_guess = round(100.0 * np.max(classbalance), 2)\n",
      "        H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))\n",
      "        modelacc = int(float(correct_count * 10000) / count) / 100.0\n",
      "\n",
      "        if args.json:\n",
      "            FN = float(num_FN) * 100.0 / float(count)\n",
      "            FP = float(num_FP) * 100.0 / float(count)\n",
      "            TN = float(num_TN) * 100.0 / float(count)\n",
      "            TP = float(num_TP) * 100.0 / float(count)\n",
      "\n",
      "            if int(num_TP + num_FN) != 0:\n",
      "                TPR = num_TP / (num_TP + num_FN)  # Sensitivity, Recall\n",
      "            if int(num_TN + num_FP) != 0:\n",
      "                TNR = num_TN / (num_TN + num_FP)  # Specificity\n",
      "            if int(num_TP + num_FP) != 0:\n",
      "                PPV = num_TP / (num_TP + num_FP)  # Recall\n",
      "            if int(num_FN + num_TP) != 0:\n",
      "                FNR = num_FN / (num_FN + num_TP)  # Miss rate\n",
      "            if int(2 * num_TP + num_FP + num_FN) != 0:\n",
      "                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN)  # F1 Score\n",
      "            if int(num_TP + num_FN + num_FP) != 0:\n",
      "                TS = num_TP / (num_TP + num_FN + num_FP)  # Critical Success Index\n",
      "            json_dict = {'instance_count': count,\n",
      "                         'classifier_type': classifier_type,\n",
      "                         'classes': n_classes,\n",
      "                         'number_correct': correct_count,\n",
      "                         'accuracy': {\n",
      "                             'best_guess': best_guess,\n",
      "                             'improvement': modelacc - best_guess,\n",
      "                             'model_accuracy': modelacc,\n",
      "                         },\n",
      "                         'false_negative_instances': num_FN,\n",
      "                         'false_positive_instances': num_FP,\n",
      "                         'true_positive_instances': num_TP,\n",
      "                         'true_negative_instances': num_TN,\n",
      "                         'false_negatives': FN,\n",
      "                         'false_positives': FP,\n",
      "                         'true_negatives': TN,\n",
      "                         'true_positives': TP,\n",
      "                         'model_capacity': model_cap,\n",
      "                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,\n",
      "                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,\n",
      "                         'shannon_entropy_of_labels': H,\n",
      "                         'classbalance': classbalance} \n",
      "        else:\n",
      "            print(\"Classifier Type:                    Random Forest\")\n",
      "            print(f\"System Type:                        {n_classes}-way classifier\")\n",
      "            print()\n",
      "            print(\"Accuracy:\")\n",
      "            print(\"    Best-guess accuracy:            {:.2f}%\".format(best_guess))\n",
      "            print(\"    Model accuracy:                 {:.2f}%\".format(modelacc) + \" (\" + str(int(correct_count)) + \"/\" + str(count) + \" correct)\")\n",
      "            print(\"    Improvement over best guess:    {:.2f}%\".format(modelacc - best_guess) + \" (of possible \" + str(round(100 - best_guess, 2)) + \"%)\")\n",
      "            print()\n",
      "            print(\"Model capacity (MEC):               {:.0f} bits\".format(model_cap))\n",
      "            if classifier_type == '\\'NN\\'':\n",
      "                print(\"Model Capacity Utilized:            {:.0f} bits\".format(cap_utilized))  # noqa\n",
      "            print(\"Generalization ratio:               {:.2f}\".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + \" bits/bit\")\n",
      "\n",
      "        mtrx, stats = __confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)\n",
      "\n",
      "        if args.json:\n",
      "            json_dict['confusion_matrix'] = mtrx.tolist()\n",
      "            json_dict['multiclass_stats'] = stats\n",
      "            print(json.dumps(json_dict))\n",
      "        else:\n",
      "            mtrx = mtrx.astype('str')\n",
      "            labels = np.array(list(mapping.keys())).reshape(-1, 1)\n",
      "            mtrx = np.concatenate((labels, mtrx), axis=1).astype('str')\n",
      "            max_TP_len, max_FP_len, max_TN_len, max_FN_len = 0, 0, 0, 0\n",
      "            max_class_name_len = len('target') + 2\n",
      "            for classs in mapping.keys():\n",
      "                max_class_name_len = max(max_class_name_len, len(classs))\n",
      "            for key in stats.keys():\n",
      "                class_stats = stats[key]\n",
      "                max_TP_len, max_FP_len, max_TN_len, max_FN_len = max(max_TP_len, len(str(class_stats['TP']))), max(max_FP_len, len(str(class_stats['FP']))), max(\n",
      "                    max_TN_len, len(str(class_stats['TN']))), max(max_FN_len, len(str(class_stats['FN'])))\n",
      "            print()\n",
      "            print(\"Confusion Matrix:\")\n",
      "            print()\n",
      "            max_len_value = int(np.max(np.vectorize(len)(mtrx)))\n",
      "            max_pred_len = (int(mtrx.shape[1]) - 1) * max_len_value\n",
      "\n",
      "            print(\" \" * 4 + \"{:>{}} |{:^{}}\".format(\"Actual\", max_class_name_len, \"Predicted\", max_pred_len))\n",
      "            print(\" \" * 4 + \"-\" * (max_class_name_len + max_pred_len + mtrx.shape[1] + 1))\n",
      "            for row in mtrx:\n",
      "                print(str(\" \" * 4 + \"{:>{}}\".format(row[0], max_class_name_len)) + \" |\" + \"{:^{}}\".format(\n",
      "                    (' '.join([str('{:>{}}'.format(i, max_len_value)) for i in row[1:]])), max_pred_len))\n",
      "            print()\n",
      "            print(\"Accuracy by Class:\")\n",
      "            print()\n",
      "            print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format('target',\n",
      "                                                                                                              max_class_name_len,\n",
      "                                                                                                              'TP', max_TP_len,\n",
      "                                                                                                              'FP', max_FP_len,\n",
      "                                                                                                              'TN', max_TN_len,\n",
      "                                                                                                              'FN', max_FN_len,\n",
      "                                                                                                              'TPR', 'TNR',\n",
      "                                                                                                              'PPV', 'NPV',\n",
      "                                                                                                              'F1', 'TS'))\n",
      "            print(\" \" * 4 + \"-\" * max_class_name_len + ' | ' + \"-\" * (\n",
      "                max_TP_len) + ' ' + \"-\" * max_FP_len + ' ' + \"-\" * max_TN_len + ' ' + \"-\" * max_FN_len + (' ' + 7 * \"-\") * 6)\n",
      "            for raw_class in mapping.keys():\n",
      "                class_stats = stats[int(mapping[raw_class])]\n",
      "                TPR = class_stats['TP'] / (class_stats['TP'] + class_stats['FN']) if int(\n",
      "                    class_stats['TP'] + class_stats['FN']) != 0 else 0\n",
      "                TNR = class_stats['TN'] / (class_stats['TN'] + class_stats['FP']) if int(\n",
      "                    class_stats['TN'] + class_stats['FP']) != 0 else 0\n",
      "                PPV = class_stats['TP'] / (class_stats['TP'] + class_stats['FP']) if int(\n",
      "                    class_stats['TP'] + class_stats['FP']) != 0 else 0\n",
      "                NPV = class_stats['TN'] / (class_stats['TN'] + class_stats['FN']) if int(\n",
      "                    class_stats['TN'] + class_stats['FN']) != 0 else 0\n",
      "                F1 = 2 * class_stats['TP'] / (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                TS = class_stats['TP'] / (class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format(raw_class,\n",
      "                                                                                                                  max_class_name_len,\n",
      "                                                                                                                  class_stats['TP'],\n",
      "                                                                                                                  max_TP_len,\n",
      "                                                                                                                  class_stats['FP'],\n",
      "                                                                                                                  max_FP_len,\n",
      "                                                                                                                  class_stats['TN'],\n",
      "                                                                                                                  max_TN_len,\n",
      "                                                                                                                  class_stats['FN'],\n",
      "                                                                                                                  max_FN_len,\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TPR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TNR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * PPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * NPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * F1, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TS, 2))))\n",
      "            \n",
      "    os.remove(cleanfile)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('RF_predictor.py', 'r') as data:\n",
    "    print(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72975533",
   "metadata": {},
   "source": [
    "## 3. Neural Network\n",
    "Force the selection of Neural Network by using the **-f NN** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f7c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "\u001b[01;1mBrainome Table Compiler v1.006-14-prod\u001b[0m\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:                 y Demo User  (Evaluation)\n",
      "Expiration Date:             2021-12-12   129 days left\n",
      "Maximum File Size:           100 MB\n",
      "Maximum Instances:           20000\n",
      "Maximum Attributes:          100\n",
      "Maximum Classes:             unlimited\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\u001b[01;1mCommand:\u001b[0m\n",
      "    btc data/titanic_train.csv -f NN -y -o NN_predictor.py\n",
      "\n",
      "Start Time:                 08/05/2021, 18:54 UTC\n",
      "\n",
      "Cleaning...done. \n",
      "Splitting into training and validation...done. \n",
      "Pre-training measurements...done. \n",
      "\n",
      "\n",
      "\u001b[01;1mPre-training Measurements\u001b[0m\n",
      "Data:\n",
      "    Input:                      data/titanic_train.csv\n",
      "    Target Column:              Survived\n",
      "    Number of instances:        800\n",
      "    Number of attributes:        11 out of 11\n",
      "    Number of classes:            2\n",
      "\n",
      "Class Balance:                \n",
      "                            died: 61.50%\n",
      "                        survived: 38.50%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          61.50%\n",
      "    Data Sufficiency:             Maybe enough data to generalize. [yellow]\n",
      "\n",
      "Capacity Progression:             at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Ideal Machine Learner:              6,   7,   8,   8,   9,   9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expected Generalization:\n",
      "    Decision Tree:                 2.02 bits/bit\n",
      "    Neural Network:                6.52 bits/bit\n",
      "    Random Forest:                10.13 bits/bit\n",
      "\n",
      "Expected Accuracy:              Training            Validation\n",
      "    Decision Tree:               100.00%                52.50%\n",
      "    Neural Network:                 ----                  ----\n",
      "    Random Forest:               100.00%                80.25%\n",
      "\n",
      "Recommendations:\n",
      "    Warning: Data has high information density. Using effort 5 and larger ( -e 5 ) can improve results.\n",
      "    We recommend using Random Forest -f RF.\n",
      "    If predictor accuracy is insufficient, try using the option -rank to automatically select the important attributes.\n",
      "    If predictor accuracy is insufficient, try using the effort option -e with a value of 5 or more to increase training time.\n",
      "    Model type NN given by user. \n",
      "\n",
      "\n",
      "Architecting model...done. \n",
      "Priming model...done. \n",
      "Compiling predictor...done. \n",
      "Validating predictor...done. \n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        NN_predictor.py\n",
      "    Classifier Type:              Neural Network\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          63.25% (303/479 correct)\n",
      "      Validation Accuracy:        61.68% (198/321 correct)\n",
      "      Combined Model Accuracy:    62.62% (501/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         27    bits\n",
      "\n",
      "    Generalization Ratio:         10.78 bits/bit\n",
      "    Percent of Data Memorized:    18.83%\n",
      "    Resilience to Noise:          -1.05 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  295    0 \n",
      "            survived |  176    8 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  195    2 \n",
      "            survived |  121    3 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  295  176    8    0  100.00%    4.35%   62.63%  100.00%   77.02%   62.63%\n",
      "            survived |    8    0  295  176    4.35%  100.00%  100.00%   62.63%    8.33%    4.35%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  195  121    3    2   98.98%    2.42%   61.71%   60.00%   76.02%   61.32%\n",
      "            survived |    3    2  195  121    2.42%   98.98%   60.00%   61.71%    4.65%    2.38%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "End Time:           08/05/2021, 18:54 UTC\n",
      "Runtime Duration:   39s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!brainome data/titanic_train.csv -f NN -y -o NN_predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2325ad",
   "metadata": {},
   "source": [
    "### View Neural Network Predictor Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfbf5913",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "#\n",
      "# This code has been produced by a free evaluation version of Brainome(tm).\n",
      "# Portions of this code copyright (c) 2019-2021 by Brainome, Inc. All Rights Reserved.\n",
      "# Brainome, Inc grants an exclusive (subject to our continuing rights to use and modify models),\n",
      "# worldwide, non-sublicensable, and non-transferable limited license to use and modify this\n",
      "# predictor produced through the input of your data:\n",
      "# (i) for users accessing the service through a free evaluation account, solely for your\n",
      "# own non-commercial purposes, including for the purpose of evaluating this service, and\n",
      "# (ii) for users accessing the service through a paid, commercial use account, for your\n",
      "# own internal  and commercial purposes.\n",
      "# Please contact support@brainome.ai with any questions.\n",
      "# Use of predictions results at your own risk.\n",
      "#\n",
      "# Output of Brainome v1.006-14-prod.\n",
      "# Invocation: brainome data/titanic_train.csv -f NN -y -o NN_predictor.py\n",
      "# Total compiler execution time: 0:00:38.64. Finished on: Aug-05-2021 18:54:59.\n",
      "# This source code requires Python 3.\n",
      "#\n",
      "\"\"\"\n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        NN_predictor.py\n",
      "    Classifier Type:              Neural Network\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          63.25% (303/479 correct)\n",
      "      Validation Accuracy:        61.68% (198/321 correct)\n",
      "      Combined Model Accuracy:    62.62% (501/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         27    bits\n",
      "\n",
      "    Generalization Ratio:         10.78 bits/bit\n",
      "    Percent of Data Memorized:    18.83%\n",
      "    Resilience to Noise:          -1.05 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  295    0 \n",
      "            survived |  176    8 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  195    2 \n",
      "            survived |  121    3 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  295  176    8    0  100.00%    4.35%   62.63%  100.00%   77.02%   62.63%\n",
      "            survived |    8    0  295  176    4.35%  100.00%  100.00%   62.63%    8.33%    4.35%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  195  121    3    2   98.98%    2.42%   61.71%   60.00%   76.02%   61.32%\n",
      "            survived |    3    2  195  121    2.42%   98.98%   60.00%   61.71%    4.65%    2.38%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "import math\n",
      "import os\n",
      "import argparse\n",
      "import tempfile\n",
      "import csv\n",
      "import binascii\n",
      "import faulthandler\n",
      "import json\n",
      "from io import StringIO\n",
      "try:\n",
      "    import numpy as np # For numpy see: http://numpy.org\n",
      "    from numpy import array\n",
      "except:\n",
      "    print(\"This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.\")\n",
      "    sys.exit(1)\n",
      "try:\n",
      "    from scipy.sparse import coo_matrix\n",
      "    report_cmat = True\n",
      "except:\n",
      "    print(\"Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.\")\n",
      "    report_cmat = False\n",
      "\n",
      "IOBUF = 100000000\n",
      "sys.setrecursionlimit(1000000)\n",
      "TRAINFILE = ['data/titanic_train.csv']\n",
      "mapping = {'died': 0, 'survived': 1}\n",
      "ignorelabels = []\n",
      "ignorecolumns = []\n",
      "list_of_cols_to_normalize = [2, 3, 7, 9, 10]\n",
      "column_mappings = [{5293964.0: 0, 20712820.0: 1, 37928527.0: 2, 58955647.0: 3, 63697425.0: 4, 64717776.0: 5, 65646919.0: 6, 71130238.0: 7, 81657709.0: 8, 88615496.0: 9, 91532435.0: 10, 96205804.0: 11, 96946420.0: 12, 110752517.0: 13, 123439672.0: 14, 145378440.0: 15, 151624095.0: 16, 158406314.0: 17, 171347414.0: 18, 173002906.0: 19, 177123997.0: 20, 187787893.0: 21, 191118323.0: 22, 199649830.0: 23, 207496510.0: 24, 212136475.0: 25, 215818984.0: 26, 216863160.0: 27, 218649093.0: 28, 223264149.0: 29, 245413226.0: 30, 252765391.0: 31, 257893424.0: 32, 274986842.0: 33, 288289083.0: 34, 303913207.0: 35, 310765304.0: 36, 312968621.0: 37, 345304815.0: 38, 346354900.0: 39, 346778979.0: 40, 350477423.0: 41, 351934297.0: 42, 405927763.0: 43, 408326397.0: 44, 420712425.0: 45, 459168817.0: 46, 459394421.0: 47, 461494209.0: 48, 484234360.0: 49, 492600013.0: 50, 503237553.0: 51, 513481237.0: 52, 513497186.0: 53, 516562391.0: 54, 522596730.0: 55, 529581881.0: 56, 561915757.0: 57, 568311847.0: 58, 585920657.0: 59, 590423253.0: 60, 592960905.0: 61, 597755178.0: 62, 597779791.0: 63, 600924386.0: 64, 602365446.0: 65, 612869533.0: 66, 622192825.0: 67, 631722873.0: 68, 640080544.0: 69, 648065000.0: 70, 659956646.0: 71, 674690455.0: 72, 675975151.0: 73, 682941722.0: 74, 699211678.0: 75, 716025648.0: 76, 723069469.0: 77, 728076223.0: 78, 731336644.0: 79, 731725830.0: 80, 753169735.0: 81, 772581899.0: 82, 780221612.0: 83, 799720936.0: 84, 802220717.0: 85, 805918432.0: 86, 813001191.0: 87, 822195092.0: 88, 839480668.0: 89, 844150669.0: 90, 846459163.0: 91, 849220641.0: 92, 875579862.0: 93, 881510304.0: 94, 882012983.0: 95, 883185014.0: 96, 888397923.0: 97, 889298309.0: 98, 901588894.0: 99, 910145201.0: 100, 930807969.0: 101, 931567756.0: 102, 937864788.0: 103, 940632791.0: 104, 946803822.0: 105, 948393078.0: 106, 964921180.0: 107, 1019497752.0: 108, 1023213050.0: 109, 1029287071.0: 110, 1041578351.0: 111, 1042508728.0: 112, 1055932691.0: 113, 1058517907.0: 114, 1059540582.0: 115, 1061388152.0: 116, 1068082752.0: 117, 1084595236.0: 118, 1085331001.0: 119, 1108757062.0: 120, 1116861033.0: 121, 1118402842.0: 122, 1120235374.0: 123, 1129752734.0: 124, 1132706315.0: 125, 1145275273.0: 126, 1160890961.0: 127, 1164227696.0: 128, 1196696091.0: 129, 1243646615.0: 130, 1251629020.0: 131, 1273865803.0: 132, 1277055658.0: 133, 1279762199.0: 134, 1293742516.0: 135, 1294846643.0: 136, 1302129313.0: 137, 1302694264.0: 138, 1312349651.0: 139, 1316225739.0: 140, 1321486527.0: 141, 1321747667.0: 142, 1336512431.0: 143, 1339187210.0: 144, 1342062427.0: 145, 1342166168.0: 146, 1354373255.0: 147, 1385171242.0: 148, 1395976955.0: 149, 1398606071.0: 150, 1408887874.0: 151, 1416853684.0: 152, 1417440248.0: 153, 1419348639.0: 154, 1429237775.0: 155, 1441752978.0: 156, 1448928844.0: 157, 1452582843.0: 158, 1454978611.0: 159, 1455598123.0: 160, 1458597503.0: 161, 1459962474.0: 162, 1466155472.0: 163, 1479726731.0: 164, 1493496938.0: 165, 1519458863.0: 166, 1553559790.0: 167, 1568582973.0: 168, 1586519675.0: 169, 1593492491.0: 170, 1604489293.0: 171, 1614833727.0: 172, 1635836524.0: 173, 1667827196.0: 174, 1684413700.0: 175, 1698471429.0: 176, 1698612220.0: 177, 1704313834.0: 178, 1724043234.0: 179, 1741801946.0: 180, 1757313818.0: 181, 1774034694.0: 182, 1792629029.0: 183, 1800199092.0: 184, 1852896616.0: 185, 1855018109.0: 186, 1860658763.0: 187, 1864213813.0: 188, 1865864791.0: 189, 1866762581.0: 190, 1866918722.0: 191, 1867251963.0: 192, 1871682263.0: 193, 1877934411.0: 194, 1878529033.0: 195, 1903153579.0: 196, 1909573419.0: 197, 1912958742.0: 198, 1918992507.0: 199, 1919867870.0: 200, 1927677166.0: 201, 1942475744.0: 202, 1956794592.0: 203, 1967558780.0: 204, 1967907931.0: 205, 1970507979.0: 206, 1974616559.0: 207, 1984302284.0: 208, 1984719963.0: 209, 1987421865.0: 210, 1995477463.0: 211, 1996035324.0: 212, 1996600261.0: 213, 2000883311.0: 214, 2010609196.0: 215, 2024741060.0: 216, 2048597385.0: 217, 2048767330.0: 218, 2053341594.0: 219, 2080105274.0: 220, 2082087335.0: 221, 2086916069.0: 222, 2105978509.0: 223, 2108911334.0: 224, 2114441529.0: 225, 2146707669.0: 226, 2153267816.0: 227, 2157421105.0: 228, 2164984309.0: 229, 2204898816.0: 230, 2206797775.0: 231, 2207777338.0: 232, 2216657351.0: 233, 2221880780.0: 234, 2233796337.0: 235, 2235297438.0: 236, 2243076972.0: 237, 2245627418.0: 238, 2251014352.0: 239, 2251870406.0: 240, 2251972092.0: 241, 2254083497.0: 242, 2269953588.0: 243, 2279572177.0: 244, 2282155242.0: 245, 2297708381.0: 246, 2307543999.0: 247, 2309880824.0: 248, 2319592631.0: 249, 2321077986.0: 250, 2330647355.0: 251, 2347430691.0: 252, 2349881785.0: 253, 2349966148.0: 254, 2351500973.0: 255, 2356530268.0: 256, 2357893999.0: 257, 2361600118.0: 258, 2363407697.0: 259, 2379811689.0: 260, 2380663784.0: 261, 2406172823.0: 262, 2413924660.0: 263, 2431464523.0: 264, 2436382531.0: 265, 2438258037.0: 266, 2438609159.0: 267, 2478470108.0: 268, 2500419607.0: 269, 2513246780.0: 270, 2523071815.0: 271, 2535296113.0: 272, 2544277816.0: 273, 2549434184.0: 274, 2550106013.0: 275, 2554612691.0: 276, 2556679904.0: 277, 2560993658.0: 278, 2562619927.0: 279, 2565123500.0: 280, 2572682503.0: 281, 2575948686.0: 282, 2576235231.0: 283, 2578921928.0: 284, 2582004952.0: 285, 2585174480.0: 286, 2595509181.0: 287, 2605177786.0: 288, 2615713364.0: 289, 2620774951.0: 290, 2621973994.0: 291, 2624582914.0: 292, 2629916675.0: 293, 2630221551.0: 294, 2631415166.0: 295, 2651525826.0: 296, 2651649521.0: 297, 2659763042.0: 298, 2660970894.0: 299, 2665804236.0: 300, 2687347794.0: 301, 2697325724.0: 302, 2707241898.0: 303, 2722687724.0: 304, 2723090349.0: 305, 2740133795.0: 306, 2747326497.0: 307, 2751997289.0: 308, 2766226079.0: 309, 2770921263.0: 310, 2778849020.0: 311, 2785761214.0: 312, 2799403716.0: 313, 2812598099.0: 314, 2822262848.0: 315, 2840074322.0: 316, 2848737087.0: 317, 2849411758.0: 318, 2852092519.0: 319, 2856348654.0: 320, 2860163242.0: 321, 2862166167.0: 322, 2869524374.0: 323, 2871789956.0: 324, 2878808647.0: 325, 2879539750.0: 326, 2880209633.0: 327, 2880259400.0: 328, 2889930591.0: 329, 2897464527.0: 330, 2900863331.0: 331, 2913931507.0: 332, 2927695562.0: 333, 2928739736.0: 334, 2931207982.0: 335, 2933026855.0: 336, 2935256788.0: 337, 2940264480.0: 338, 2941555484.0: 339, 2942639148.0: 340, 2946602557.0: 341, 2950069458.0: 342, 2966658913.0: 343, 2968494211.0: 344, 2996497065.0: 345, 2999841806.0: 346, 3012030154.0: 347, 3016285314.0: 348, 3044253616.0: 349, 3046029384.0: 350, 3057987810.0: 351, 3062335486.0: 352, 3064189168.0: 353, 3069816779.0: 354, 3072205390.0: 355, 3090762313.0: 356, 3120876018.0: 357, 3122991212.0: 358, 3130085946.0: 359, 3143997629.0: 360, 3144816944.0: 361, 3152620018.0: 362, 3167065512.0: 363, 3169994115.0: 364, 3180444477.0: 365, 3185750991.0: 366, 3194243310.0: 367, 3206958702.0: 368, 3215448853.0: 369, 3220432517.0: 370, 3240362405.0: 371, 3255557379.0: 372, 3256119865.0: 373, 3256231801.0: 374, 3266335654.0: 375, 3282956770.0: 376, 3291365342.0: 377, 3322476467.0: 378, 3334513079.0: 379, 3341895417.0: 380, 3344361288.0: 381, 3347155496.0: 382, 3383429031.0: 383, 3393012686.0: 384, 3400846096.0: 385, 3409834537.0: 386, 3424448926.0: 387, 3441126726.0: 388, 3445767696.0: 389, 3446737368.0: 390, 3449041597.0: 391, 3482361546.0: 392, 3486601573.0: 393, 3504078617.0: 394, 3511079981.0: 395, 3512622853.0: 396, 3539275416.0: 397, 3539426020.0: 398, 3546637657.0: 399, 3551192826.0: 400, 3569041479.0: 401, 3573721271.0: 402, 3574157434.0: 403, 3580164090.0: 404, 3599593732.0: 405, 3607886042.0: 406, 3614619796.0: 407, 3620205589.0: 408, 3628510910.0: 409, 3633258821.0: 410, 3645422008.0: 411, 3686419810.0: 412, 3690424289.0: 413, 3701043313.0: 414, 3704330001.0: 415, 3710297205.0: 416, 3719496725.0: 417, 3722116310.0: 418, 3740155494.0: 419, 3755346993.0: 420, 3763425105.0: 421, 3772124015.0: 422, 3783232826.0: 423, 3793953756.0: 424, 3796074051.0: 425, 3802765926.0: 426, 3807385696.0: 427, 3824308110.0: 428, 3829770218.0: 429, 3833264875.0: 430, 3834495495.0: 431, 3852219155.0: 432, 3862849887.0: 433, 3862945763.0: 434, 3874244043.0: 435, 3883471541.0: 436, 3896872135.0: 437, 3898493751.0: 438, 3903200896.0: 439, 3904829021.0: 440, 3907660493.0: 441, 3914310419.0: 442, 3916796207.0: 443, 3935871884.0: 444, 3937002971.0: 445, 3941468415.0: 446, 3944123893.0: 447, 3957304816.0: 448, 3961638311.0: 449, 3965249036.0: 450, 3968411545.0: 451, 3981995600.0: 452, 3983700453.0: 453, 3995723443.0: 454, 4030810382.0: 455, 4050949689.0: 456, 4051393568.0: 457, 4056060860.0: 458, 4069374756.0: 459, 4072407052.0: 460, 4088287584.0: 461, 4091780273.0: 462, 4098759529.0: 463, 4116171884.0: 464, 4130241991.0: 465, 4164151970.0: 466, 4181618755.0: 467, 4182447269.0: 468, 4187790576.0: 469, 4213085746.0: 470, 4214918729.0: 471, 4229769115.0: 472, 4239686084.0: 473, 4247466665.0: 474, 4250194144.0: 475, 4255272083.0: 476, 4284893670.0: 477, 4286802825.0: 478, 1991359137.0: 479, 2443355381.0: 480, 1176069340.0: 481, 1609072530.0: 482, 2986438105.0: 483, 76207139.0: 484, 3926926093.0: 485, 1042909409.0: 486, 3591183444.0: 487, 1426255607.0: 488, 3444299801.0: 489, 1775791425.0: 490, 526506166.0: 491, 1870419366.0: 492, 50303386.0: 493, 1712535010.0: 494, 2798354526.0: 495, 95213063.0: 496, 3324145201.0: 497, 449459500.0: 498, 3695836240.0: 499, 3618541572.0: 500, 1825924669.0: 501, 391509426.0: 502, 206276338.0: 503, 1604853436.0: 504, 242525583.0: 505, 4180982484.0: 506, 2877548697.0: 507, 3947964011.0: 508, 4058475945.0: 509, 3081475493.0: 510, 1935048507.0: 511, 4128697684.0: 512, 534693750.0: 513, 183329855.0: 514, 2256584022.0: 515, 3122427156.0: 516, 1667481170.0: 517, 2190034181.0: 518, 2669286816.0: 519, 3301436209.0: 520, 2642416566.0: 521, 69762647.0: 522, 626750337.0: 523, 3896464720.0: 524, 644276314.0: 525, 3527699655.0: 526, 2439906773.0: 527, 370316392.0: 528, 2071269338.0: 529, 1370063053.0: 530, 2568010046.0: 531, 3044703428.0: 532, 988201081.0: 533, 2479206774.0: 534, 4220271566.0: 535, 1102592872.0: 536, 853587486.0: 537, 1245496069.0: 538, 2762140153.0: 539, 2949132901.0: 540, 1940811450.0: 541, 743172040.0: 542, 4198497417.0: 543, 2877969812.0: 544, 1601185450.0: 545, 3317167097.0: 546, 4107164609.0: 547, 3858029017.0: 548, 3798642944.0: 549, 709932269.0: 550, 1176948131.0: 551, 1474487330.0: 552, 1529012526.0: 553, 3780579449.0: 554, 1784043020.0: 555, 3406657361.0: 556, 1833914870.0: 557, 4129667996.0: 558, 1958718147.0: 559, 906281622.0: 560, 1252794603.0: 561, 2432215372.0: 562, 4200468024.0: 563, 2877702439.0: 564, 1444353578.0: 565, 372715304.0: 566, 2093264839.0: 567, 3177178009.0: 568, 2337445921.0: 569, 1292887873.0: 570, 464103141.0: 571, 3792957240.0: 572, 4292270130.0: 573, 3180942973.0: 574, 3858476870.0: 575, 967517375.0: 576, 63456976.0: 577, 2008898691.0: 578, 1540526703.0: 579, 1106083312.0: 580, 765381883.0: 581, 1718032917.0: 582, 3161393809.0: 583, 1843938447.0: 584, 3897833986.0: 585, 3018306788.0: 586, 785044418.0: 587, 3114904761.0: 588, 2350116146.0: 589, 3283677820.0: 590, 113503754.0: 591, 195342075.0: 592, 3186009937.0: 593, 3673034070.0: 594, 535337885.0: 595, 2988374417.0: 596, 3496030201.0: 597, 2143807062.0: 598, 957005456.0: 599, 2704073564.0: 600, 3964103388.0: 601, 4081093051.0: 602, 2435350653.0: 603, 3679742646.0: 604, 1038624469.0: 605, 419154244.0: 606, 1954891810.0: 607, 2190846545.0: 608, 196501456.0: 609, 1098231306.0: 610, 801753430.0: 611, 537154236.0: 612, 1788554681.0: 613, 3175319779.0: 614, 1704646814.0: 615, 2343561024.0: 616, 2764257972.0: 617, 1686301624.0: 618, 1620681458.0: 619, 4132765360.0: 620, 4219488636.0: 621, 3760068104.0: 622, 1058731968.0: 623, 4133701394.0: 624, 150469751.0: 625, 3031392919.0: 626, 987242435.0: 627, 4150064244.0: 628, 3572724118.0: 629, 1496012894.0: 630, 846861651.0: 631, 1783641394.0: 632, 3371173992.0: 633, 1340861078.0: 634, 1885455574.0: 635, 533521261.0: 636, 1197587133.0: 637, 3798931224.0: 638, 563877922.0: 639, 1991655352.0: 640, 872331253.0: 641, 4165362964.0: 642, 2247415573.0: 643, 3868017790.0: 644, 2223684810.0: 645, 1509735843.0: 646, 3857242462.0: 647, 636676421.0: 648, 569752148.0: 649, 522544571.0: 650, 1927461743.0: 651, 1661817264.0: 652, 2067534593.0: 653, 2319415260.0: 654, 1679579796.0: 655, 3614980082.0: 656, 1390495911.0: 657, 4147638480.0: 658, 1408380131.0: 659, 1748553833.0: 660, 3368944859.0: 661, 1869048156.0: 662, 2602309287.0: 663, 3888473606.0: 664, 1503746736.0: 665, 3461184158.0: 666, 2789729266.0: 667, 778126000.0: 668, 600560118.0: 669, 755428420.0: 670, 2954907351.0: 671, 2483473118.0: 672, 3341235014.0: 673, 1930346734.0: 674, 196276904.0: 675, 1444526898.0: 676, 3639603027.0: 677, 3801261504.0: 678, 3745812452.0: 679, 3991150992.0: 680, 1932095558.0: 681, 4159238473.0: 682, 1421810829.0: 683, 1005143300.0: 684, 2036054235.0: 685, 1642251885.0: 686, 3169176052.0: 687, 2368041679.0: 688, 2854923316.0: 689, 1794850983.0: 690, 887617949.0: 691, 3441180941.0: 692, 1138923494.0: 693, 1986790188.0: 694, 3275742987.0: 695, 44383796.0: 696, 2894922400.0: 697, 2184453420.0: 698, 2562950160.0: 699, 650059989.0: 700, 179405791.0: 701, 2835435743.0: 702, 50575162.0: 703, 4207113191.0: 704, 814098106.0: 705, 1795447524.0: 706, 1830294765.0: 707, 18578055.0: 708, 2439454821.0: 709, 2346055361.0: 710, 4187891899.0: 711, 2280896268.0: 712, 2929558679.0: 713, 4145457972.0: 714, 1681081880.0: 715, 800441542.0: 716, 602248594.0: 717, 2522977591.0: 718, 1599523904.0: 719, 2350875971.0: 720, 2358031551.0: 721, 349367443.0: 722, 1509142300.0: 723, 509545614.0: 724, 281941261.0: 725, 1862307821.0: 726, 3762990763.0: 727, 2113677425.0: 728, 2606559068.0: 729, 1645128890.0: 730, 4179435853.0: 731, 4287656819.0: 732, 170780334.0: 733, 1390115221.0: 734, 3116136844.0: 735, 3047943227.0: 736, 4074885271.0: 737, 2781383188.0: 738, 153492239.0: 739, 511972916.0: 740, 890264039.0: 741, 643240769.0: 742, 911944064.0: 743, 2669880511.0: 744, 1057668830.0: 745, 261301438.0: 746, 4062415539.0: 747, 3515802505.0: 748, 464584523.0: 749, 2552862953.0: 750, 4274536835.0: 751, 1555251108.0: 752, 2246365091.0: 753, 3149365851.0: 754, 235723576.0: 755, 2649434319.0: 756, 4257510395.0: 757, 4269748073.0: 758, 3117100920.0: 759, 3481052330.0: 760, 2125574907.0: 761, 2259804170.0: 762, 707560981.0: 763, 399463896.0: 764, 1095197749.0: 765, 2881570338.0: 766, 1974057912.0: 767, 1173928333.0: 768, 2854193963.0: 769, 3482687255.0: 770, 3020196122.0: 771, 1725340092.0: 772, 157173721.0: 773, 4214051870.0: 774, 4233073454.0: 775, 588049520.0: 776, 1034923554.0: 777, 3691393995.0: 778, 1925921106.0: 779, 3718365230.0: 780, 2292646849.0: 781, 2400993941.0: 782, 216991797.0: 783, 3100877179.0: 784, 3719722818.0: 785, 1884829528.0: 786, 1165774387.0: 787, 3693120537.0: 788, 2279733118.0: 789, 2029347108.0: 790, 3996217575.0: 791, 2022073089.0: 792, 674575189.0: 793, 3885558007.0: 794, 3335379356.0: 795, 4002856060.0: 796, 3646163218.0: 797, 3088216931.0: 798, 1137163743.0: 799}, {1249151596.0: 0, 1435361449.0: 1}, {1601.0: 0, 2620.0: 1, 2623.0: 2, 2624.0: 3, 2627.0: 4, 2631.0: 5, 2641.0: 6, 2647.0: 7, 2650.0: 8, 2651.0: 9, 2653.0: 10, 2659.0: 11, 2661.0: 12, 2663.0: 13, 2665.0: 14, 2666.0: 15, 2668.0: 16, 2669.0: 17, 2672.0: 18, 2678.0: 19, 2680.0: 20, 2685.0: 21, 2686.0: 22, 2687.0: 23, 2689.0: 24, 2690.0: 25, 2691.0: 26, 2693.0: 27, 2694.0: 28, 2695.0: 29, 2697.0: 30, 2699.0: 31, 2700.0: 32, 3411.0: 33, 3460.0: 34, 4133.0: 35, 4135.0: 36, 4136.0: 37, 5727.0: 38, 7267.0: 39, 7534.0: 40, 7540.0: 41, 7546.0: 42, 7553.0: 43, 7598.0: 44, 8471.0: 45, 8475.0: 46, 9234.0: 47, 11668.0: 48, 11751.0: 49, 11752.0: 50, 11753.0: 51, 11755.0: 52, 11765.0: 53, 11767.0: 54, 11771.0: 55, 11813.0: 56, 11967.0: 57, 12233.0: 58, 12749.0: 59, 13049.0: 60, 13213.0: 61, 13214.0: 62, 13502.0: 63, 14311.0: 64, 14312.0: 65, 14973.0: 66, 16966.0: 67, 16988.0: 68, 17421.0: 69, 17453.0: 70, 17463.0: 71, 17465.0: 72, 17764.0: 73, 19877.0: 74, 19928.0: 75, 19943.0: 76, 19950.0: 77, 19952.0: 78, 19988.0: 79, 19996.0: 80, 21440.0: 81, 26360.0: 82, 26707.0: 83, 27042.0: 84, 27267.0: 85, 27849.0: 86, 28213.0: 87, 28228.0: 88, 28403.0: 89, 28424.0: 90, 28664.0: 91, 28665.0: 92, 29103.0: 93, 29104.0: 94, 29105.0: 95, 29106.0: 96, 29750.0: 97, 29751.0: 98, 31027.0: 99, 35273.0: 100, 35281.0: 101, 36209.0: 102, 36568.0: 103, 36865.0: 104, 36866.0: 105, 36928.0: 106, 36963.0: 107, 36967.0: 108, 36973.0: 109, 54636.0: 110, 65304.0: 111, 65306.0: 112, 110152.0: 113, 110413.0: 114, 111240.0: 115, 111320.0: 116, 111361.0: 117, 111427.0: 118, 112059.0: 119, 112277.0: 120, 113028.0: 121, 113043.0: 122, 113050.0: 123, 113051.0: 124, 113056.0: 125, 113059.0: 126, 113501.0: 127, 113503.0: 128, 113505.0: 129, 113514.0: 130, 113760.0: 131, 113773.0: 132, 113776.0: 133, 113783.0: 134, 113784.0: 135, 113786.0: 136, 113787.0: 137, 113788.0: 138, 113789.0: 139, 113794.0: 140, 113796.0: 141, 113803.0: 142, 113806.0: 143, 220845.0: 144, 223596.0: 145, 226593.0: 146, 226875.0: 147, 230080.0: 148, 230136.0: 149, 230433.0: 150, 231919.0: 151, 231945.0: 152, 233639.0: 153, 234604.0: 154, 234818.0: 155, 237442.0: 156, 237736.0: 157, 237798.0: 158, 239853.0: 159, 239865.0: 160, 243847.0: 161, 243880.0: 162, 244278.0: 163, 244310.0: 164, 244358.0: 165, 244361.0: 166, 244367.0: 167, 244373.0: 168, 248698.0: 169, 248706.0: 170, 248727.0: 171, 248731.0: 172, 248738.0: 173, 248747.0: 174, 250644.0: 175, 250646.0: 176, 250647.0: 177, 250648.0: 178, 250649.0: 179, 250653.0: 180, 250655.0: 181, 312991.0: 182, 315037.0: 183, 315082.0: 184, 315088.0: 185, 315089.0: 186, 315093.0: 187, 315096.0: 188, 315151.0: 189, 323592.0: 190, 323951.0: 191, 330923.0: 192, 330931.0: 193, 330932.0: 194, 330959.0: 195, 330980.0: 196, 335097.0: 197, 336439.0: 198, 341826.0: 199, 343120.0: 200, 345364.0: 201, 345572.0: 202, 345763.0: 203, 345764.0: 204, 345767.0: 205, 345773.0: 206, 345774.0: 207, 345780.0: 208, 345781.0: 209, 345783.0: 210, 347054.0: 211, 347062.0: 212, 347067.0: 213, 347069.0: 214, 347071.0: 215, 347073.0: 216, 347074.0: 217, 347077.0: 218, 347081.0: 219, 347082.0: 220, 347085.0: 221, 347088.0: 222, 347466.0: 223, 347742.0: 224, 347743.0: 225, 348121.0: 226, 348123.0: 227, 348124.0: 228, 349205.0: 229, 349207.0: 230, 349208.0: 231, 349209.0: 232, 349214.0: 233, 349218.0: 234, 349219.0: 235, 349221.0: 236, 349222.0: 237, 349223.0: 238, 349225.0: 239, 349233.0: 240, 349234.0: 241, 349236.0: 242, 349237.0: 243, 349239.0: 244, 349240.0: 245, 349242.0: 246, 349244.0: 247, 349246.0: 248, 349249.0: 249, 349252.0: 250, 349254.0: 251, 349256.0: 252, 349909.0: 253, 349910.0: 254, 349912.0: 255, 350025.0: 256, 350034.0: 257, 350042.0: 258, 350046.0: 259, 350047.0: 260, 350060.0: 261, 350404.0: 262, 350406.0: 263, 350417.0: 264, 358585.0: 265, 363291.0: 266, 364498.0: 267, 364506.0: 268, 364516.0: 269, 364846.0: 270, 364848.0: 271, 364849.0: 272, 364850.0: 273, 364851.0: 274, 365222.0: 275, 365226.0: 276, 367226.0: 277, 367230.0: 278, 367655.0: 279, 370129.0: 280, 370365.0: 281, 370369.0: 282, 370370.0: 283, 370371.0: 284, 370375.0: 285, 370377.0: 286, 371110.0: 287, 371362.0: 288, 373450.0: 289, 374746.0: 290, 374887.0: 291, 376564.0: 292, 376566.0: 293, 382652.0: 294, 383121.0: 295, 384461.0: 296, 392096.0: 297, 394140.0: 298, 3101264.0: 299, 3101265.0: 300, 3101267.0: 301, 3101277.0: 302, 3101278.0: 303, 3101281.0: 304, 3101295.0: 305, 3101296.0: 306, 3101298.0: 307, 76777977.0: 308, 140508824.0: 309, 152993313.0: 310, 155025754.0: 311, 165296503.0: 312, 324219018.0: 313, 387454294.0: 314, 400773925.0: 315, 417439342.0: 316, 428932144.0: 317, 451247265.0: 318, 459622912.0: 319, 517393445.0: 320, 520149647.0: 321, 521193205.0: 322, 539736263.0: 323, 556207774.0: 324, 569688203.0: 325, 578268689.0: 326, 661592221.0: 327, 745884362.0: 328, 824149063.0: 329, 831457738.0: 330, 913457246.0: 331, 945486485.0: 332, 1024935327.0: 333, 1036475434.0: 334, 1196370261.0: 335, 1204664076.0: 336, 1242592521.0: 337, 1310645587.0: 338, 1339864932.0: 339, 1497450520.0: 340, 1513195675.0: 341, 1669014081.0: 342, 1683226652.0: 343, 1689199847.0: 344, 1716576979.0: 345, 1720607818.0: 346, 1782825888.0: 347, 1787791406.0: 348, 1791472115.0: 349, 1843547191.0: 350, 1846008657.0: 351, 1877389048.0: 352, 1923795190.0: 353, 2071433066.0: 354, 2072310710.0: 355, 2098059033.0: 356, 2117775820.0: 357, 2179956254.0: 358, 2247020454.0: 359, 2264899535.0: 360, 2287895551.0: 361, 2301203405.0: 362, 2326156761.0: 363, 2340820763.0: 364, 2383496428.0: 365, 2393218453.0: 366, 2441110378.0: 367, 2466494005.0: 368, 2481551162.0: 369, 2553274278.0: 370, 2565002966.0: 371, 2567056182.0: 372, 2666080958.0: 373, 2766067434.0: 374, 2903624228.0: 375, 2966369767.0: 376, 3116002924.0: 377, 3208878909.0: 378, 3244757043.0: 379, 3281577038.0: 380, 3340248101.0: 381, 3380228305.0: 382, 3413062402.0: 383, 3458587115.0: 384, 3518505671.0: 385, 3630521121.0: 386, 3764266095.0: 387, 3808984457.0: 388, 3830916057.0: 389, 3884013890.0: 390, 3894903607.0: 391, 3926100622.0: 392, 3955671087.0: 393, 3958177390.0: 394, 3999479884.0: 395, 4006252765.0: 396, 4024821312.0: 397, 4027900200.0: 398, 4042421561.0: 399, 4085701012.0: 400, 4142419592.0: 401, 4157391951.0: 402, 4162195315.0: 403, 4201804795.0: 404, 4224301822.0: 405, 4239475757.0: 406, 4264164619.0: 407, 4275005722.0: 408, 349231.0: 409, 343095.0: 410, 330877.0: 411, 13509.0: 412, 347068.0: 413, 2664.0: 414, 350407.0: 415, 2128415201.0: 416, 368703.0: 417, 250643.0: 418, 371060.0: 419, 474937058.0: 420, 349201.0: 421, 4292064498.0: 422, 6563.0: 423, 1659195903.0: 424, 4062380886.0: 425, 220367.0: 426, 367229.0: 427, 347080.0: 428, 113767.0: 429, 347078.0: 430, 236171.0: 431, 370372.0: 432, 28425.0: 433, 3825501859.0: 434, 3488398301.0: 435, 537058436.0: 436, 375596892.0: 437, 237671.0: 438, 113798.0: 439, 113792.0: 440, 166144051.0: 441, 350029.0: 442, 4579.0: 443, 382649.0: 444, 3490264620.0: 445, 113807.0: 446, 343276.0: 447, 234360.0: 448, 234686.0: 449, 229236.0: 450, 113781.0: 451, 236853.0: 452, 495499448.0: 453, 776370318.0: 454, 3378608561.0: 455, 312992.0: 456, 362316.0: 457, 113800.0: 458, 113510.0: 459, 349206.0: 460, 367232.0: 461, 364500.0: 462, 2550824685.0: 463, 573361098.0: 464, 239856.0: 465, 237565.0: 466, 350050.0: 467, 31028.0: 468, 315086.0: 469, 343275.0: 470, 363592.0: 471, 2662.0: 472, 349216.0: 473, 345778.0: 474, 3101276.0: 475, 248740.0: 476, 112379.0: 477, 349253.0: 478, 374910.0: 479, 349251.0: 480, 4137.0: 481, 363294.0: 482, 342826.0: 483, 349210.0: 484, 315084.0: 485, 2644170819.0: 486, 315094.0: 487, 4010626683.0: 488, 1799997044.0: 489, 334912.0: 490, 2236065624.0: 491, 349228.0: 492, 953911282.0: 493, 349241.0: 494, 2628.0: 495, 347061.0: 496, 244252.0: 497, 3266458466.0: 498, 349245.0: 499, 364512.0: 500, 2799830701.0: 501, 330909.0: 502, 347464.0: 503, 239855.0: 504, 4113388503.0: 505, 372622.0: 506, 36864.0: 507, 330979.0: 508, 347076.0: 509, 349204.0: 510, 350035.0: 511, 330935.0: 512, 29108.0: 513, 693.0: 514, 326145279.0: 515, 65303.0: 516, 239854.0: 517, 3615059432.0: 518, 350052.0: 519, 349215.0: 520, 324669.0: 521, 350036.0: 522, 312993.0: 523, 350048.0: 524, 1349003275.0: 525, 218629.0: 526, 110465.0: 527, 3735629129.0: 528, 112052.0: 529, 290968133.0: 530, 364511.0: 531, 347064.0: 532, 113509.0: 533, 12460.0: 534, 29011.0: 535, 2929226755.0: 536, 3770260546.0: 537, 219533.0: 538, 2674.0: 539, 2648.0: 540, 343675607.0: 541, 345770.0: 542, 345769.0: 543, 349224.0: 544, 13507.0: 545, 265596470.0: 546, 7545.0: 547, 349203.0: 548, 349247.0: 549, 349227.0: 550, 3388888457.0: 551, 349243.0: 552, 250651.0: 553, 2677.0: 554, 34218.0: 555, 3235329434.0: 556, 33638.0: 557, 3497674992.0: 558, 3752194971.0: 559, 2908.0: 560, 3924695592.0: 561, 3475599282.0: 562, 113804.0: 563, 13568.0: 564, 111426.0: 565, 28551.0: 566, 3090046756.0: 567, 315153.0: 568, 161116554.0: 569, 17474.0: 570, 14313.0: 571, 1444953608.0: 572, 4004161078.0: 573, 2003.0: 574, 2376368940.0: 575, 36947.0: 576, 13567.0: 577, 237668.0: 578, 1939126639.0: 579, 347083.0: 580, 2649.0: 581, 110564.0: 582, 382651.0: 583, 4096752855.0: 584, 3381051804.0: 585, 2626.0: 586, 2573469914.0: 587, 28220.0: 588, 2711656918.0: 589, 335677.0: 590, 31418.0: 591, 113572.0: 592, 1194733749.0: 593, 4138.0: 594, 240929.0: 595, 2266223038.0: 596, 367231.0: 597, 250652.0: 598, 35851.0: 599, 4134.0: 600, 347470.0: 601, 110813.0: 602, 386525.0: 603, 17464.0: 604, 370373.0: 605, 184130260.0: 606, 330958.0: 607, 35852.0: 608, 19947.0: 609, 24160.0: 610, 345779.0: 611, 228414.0: 612, 350043.0: 613, 2926.0: 614, 248733.0: 615, 330919.0: 616, 397200431.0: 617, 1901813274.0: 618, 237789.0: 619, 2314553085.0: 620, 111428.0: 621, 230434.0: 622, 244270.0: 623, 11769.0: 624}, {0.0: 0, 52685841.0: 1, 146240166.0: 2, 205131284.0: 3, 234382371.0: 4, 236510344.0: 5, 268880308.0: 6, 294286260.0: 7, 344567039.0: 8, 350442850.0: 9, 352784134.0: 10, 390181215.0: 11, 406601631.0: 12, 443944295.0: 13, 449814752.0: 14, 525466502.0: 15, 573614902.0: 16, 583005055.0: 17, 634533734.0: 18, 650104445.0: 19, 672055250.0: 20, 705322269.0: 21, 732716015.0: 22, 746832736.0: 23, 791522300.0: 24, 828071588.0: 25, 852690539.0: 26, 878352406.0: 27, 899215905.0: 28, 915355450.0: 29, 1019807271.0: 30, 1099433900.0: 31, 1169185336.0: 32, 1271805617.0: 33, 1300635826.0: 34, 1345740050.0: 35, 1379057629.0: 36, 1381654457.0: 37, 1389692912.0: 38, 1463809486.0: 39, 1516284306.0: 40, 1557071804.0: 41, 1596964631.0: 42, 1610931551.0: 43, 1644183440.0: 44, 1699466202.0: 45, 1728182562.0: 46, 1752234819.0: 47, 1836137969.0: 48, 1855785467.0: 49, 1941857582.0: 50, 1951568171.0: 51, 2275065867.0: 52, 2281614542.0: 53, 2336598774.0: 54, 2583969418.0: 55, 2601207467.0: 56, 2645558274.0: 57, 2742024627.0: 58, 2743615902.0: 59, 2746444292.0: 60, 2766505351.0: 61, 2780845981.0: 62, 2851636557.0: 63, 2853715430.0: 64, 2899574541.0: 65, 2944820864.0: 66, 2971786811.0: 67, 2999019152.0: 68, 3003321511.0: 69, 3058066393.0: 70, 3098649155.0: 71, 3187964512.0: 72, 3218892378.0: 73, 3230220419.0: 74, 3236768838.0: 75, 3266023967.0: 76, 3327004361.0: 77, 3536283403.0: 78, 3703114626.0: 79, 3756823055.0: 80, 3839168801.0: 81, 3847858977.0: 82, 3953242029.0: 83, 3990548621.0: 84, 4047937050.0: 85, 4072316652.0: 86, 4130496179.0: 87, 4160194878.0: 88, 4178414821.0: 89, 4232223328.0: 90, 4253085783.0: 91, 1434012567.0: 92, 749073203.0: 93, 2997160533.0: 94, 2553768918.0: 95, 1438172137.0: 96, 2331549845.0: 97, 1773310324.0: 98, 1212042119.0: 99, 3584729874.0: 100, 469415783.0: 101, 1662195780.0: 102, 1841860726.0: 103, 4261980312.0: 104, 1417325022.0: 105, 1592644896.0: 106, 1777389026.0: 107, 2712310530.0: 108, 328110459.0: 109, 3434189395.0: 110, 1539747680.0: 111, 1171535613.0: 112, 657550724.0: 113, 300655516.0: 114, 780607919.0: 115, 1207362657.0: 116, 3172535531.0: 117, 1132173523.0: 118, 1722512241.0: 119, 2720189389.0: 120, 2328432886.0: 121, 3883628989.0: 122, 3980996196.0: 123, 1771348263.0: 124, 1828554737.0: 125, 1227713619.0: 126, 3692883769.0: 127, 994277907.0: 128, 3288611889.0: 129, 3278501928.0: 130, 880584773.0: 131, 3144218308.0: 132, 1011244654.0: 133, 3576036187.0: 134, 1130211456.0: 135, 64262561.0: 136}, {543223747.0: 0, 1037565863.0: 1, 3463352047.0: 2, 0.0: 3}]\n",
      "target = '' \n",
      "target_column = 11\n",
      "important_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "ignore_idxs = []\n",
      "classifier_type = 'NN'\n",
      "num_attr = 11\n",
      "n_classes = 2\n",
      "model_cap = 27\n",
      "w_h = np.array([[0.04796169325709343, -0.39163467288017273, -0.2884780168533325, -1.9601606130599976, 0.9081411957740784, 1.0654219388961792, 0.042994141578674316, -0.7435160279273987, -0.03990408405661583, 0.24985501170158386, 0.6495479941368103], [-0.7982990741729736, -0.6741688251495361, -0.026307597756385803, -1.8389593362808228, -0.3968714475631714, -0.5987955927848816, 0.4834515452384949, 0.11129826307296753, -0.996185839176178, -0.1795966774225235, -0.1320621371269226]])\n",
      "b_h = np.array([0.03143518790602684, -1.212617039680481])\n",
      "w_o = np.array([[0.3499300181865692, 0.3494601249694824]])\n",
      "b_o = np.array(-0.5114585161209106)\n",
      "\n",
      "\n",
      "\n",
      "def __column_norm(column, mappings):\n",
      "    normalized_col = np.zeros(column.shape[0])\n",
      "    for i, val in enumerate(column.reshape(-1)):\n",
      "        if val not in mappings:\n",
      "            mappings[val] = int(max(mappings.values())) + 1\n",
      "        normalized_col[i] = mappings[val]\n",
      "    return normalized_col\n",
      "\n",
      "\n",
      "def __normalize(arr):\n",
      "    for i,mapping in zip(list_of_cols_to_normalize, column_mappings):\n",
      "        if i >= arr.shape[1]:\n",
      "            break\n",
      "        col = arr[:, i]\n",
      "        normcol = __column_norm(col, mapping)\n",
      "        arr[:, i] = normcol\n",
      "    return arr\n",
      "\n",
      "\n",
      "def __convert(cell):\n",
      "    value = str(cell)\n",
      "    try:\n",
      "        result = int(value)\n",
      "        return result\n",
      "    except ValueError:\n",
      "        try:\n",
      "            result=float(value)\n",
      "            if math.isnan(result):\n",
      "                print('NaN value found. Aborting.')\n",
      "                sys.exit(1)\n",
      "            return result\n",
      "        except ValueError:\n",
      "            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def __get_key(val, dictionary):\n",
      "    if dictionary == {}:\n",
      "        return val\n",
      "    for key, value in dictionary.items(): \n",
      "        if val == value:\n",
      "            return key\n",
      "    if val not in dictionary.values:\n",
      "        print(\"Label key does not exist\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __convertclassid(cell, classlist=[]):\n",
      "\n",
      "    value = str(cell)\n",
      "    \n",
      "    if value == '':\n",
      "        print('Empty value encountered for a class label. Aborting.')\n",
      "        sys.exit(1)\n",
      "    \n",
      "    if mapping != {}:\n",
      "        result = -1\n",
      "        try:\n",
      "            result = mapping[cell]\n",
      "        except KeyError:\n",
      "            print(f\"The class label {value} does not exist in the class mapping. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "        return result\n",
      "    \n",
      "    try:\n",
      "        result = float(cell)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "    except:\n",
      "        result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "        if result in classlist:\n",
      "            result = classlist.index(result)\n",
      "        else:\n",
      "            classlist.append(str(result))\n",
      "            result = classlist.index(result)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "    finally:\n",
      "        if result < 0:\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to non-negative integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "    return result\n",
      "\n",
      "\n",
      "def __clean(filename, outfile, headerless=False, testfile=False, trim=False):\n",
      "    classlist = []\n",
      "    outbuf = []\n",
      "    remove_bad_chars = lambda x: x.replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    \n",
      "    with open(filename, encoding='utf-8') as csv_file, open(outfile, \"w+\", encoding='utf-8') as f:\n",
      "        \n",
      "        reader = csv.reader(csv_file)\n",
      "        if not headerless:\n",
      "            next(reader, None)\n",
      "        \n",
      "        for i, row in enumerate(reader):\n",
      "\n",
      "            if row == []:\n",
      "                continue\n",
      "\n",
      "            \n",
      "            expected_row_length = len(important_idxs)\n",
      "            if not trim:\n",
      "                expected_row_length += len(ignorecolumns)\n",
      "            if not testfile:\n",
      "                expected_row_length += 1\n",
      "            actual_row_length = len(row)\n",
      "\n",
      "            if testfile and actual_row_length == expected_row_length + 1:\n",
      "                error_str = f\"We found {actual_row_length} columns but expected {expected_row_length} columns at row {i}. \"\n",
      "                error_str += f\"Please check that the CSV contains no target column otherwise use -validate. Aborting.\"\n",
      "                print(error_str)\n",
      "                sys.exit(1)\n",
      "            \n",
      "            if actual_row_length != expected_row_length:\n",
      "                print(f\"We found {actual_row_length} columns but expected {expected_row_length} columns.\")\n",
      "                sys.exit(1)            \n",
      "\n",
      "            if testfile:\n",
      "                if len(row) == 1:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(row[0])))]\n",
      "                else:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convert(remove_bad_chars(row[-1])))]         \n",
      "            else:\n",
      "                converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convertclassid(row[-1], classlist))]\n",
      "            outbuf.extend(converted_row)\n",
      "\n",
      "            if len(outbuf) < IOBUF:\n",
      "                outbuf.append(os.linesep)\n",
      "            else:\n",
      "                print(''.join(outbuf), file=f)\n",
      "                outbuf = []\n",
      "        \n",
      "        print(''.join(outbuf), end=\"\", file=f)\n",
      "\n",
      "    n_classes_found = len(classlist)\n",
      "    if not testfile and n_classes_found < 2:\n",
      "        print(f\"Only {n_classes_found} classes were found. Aborting.\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):\n",
      "    stats = {}\n",
      "    if labels is None:\n",
      "        labels = np.array(list(set(list(y_true.astype('int')))))\n",
      "    else:\n",
      "        labels = np.asarray(labels)\n",
      "        if np.all([l not in y_true for l in labels]):\n",
      "            raise ValueError(\"At least one label specified must be in y_true\")\n",
      "    n_labels = labels.size\n",
      "\n",
      "    for class_i in range(n_labels):\n",
      "        stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}\n",
      "        class_i_indices = np.argwhere(y_true==class_i)\n",
      "        not_class_i_indices = np.argwhere(y_true!=class_i)\n",
      "        stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices] == class_i))\n",
      "        stats[int(class_i)]['FN'] = int(np.sum(y_pred[class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['FP'] = int(np.sum(y_pred[not_class_i_indices] == class_i))\n",
      "\n",
      "    if not report_cmat:\n",
      "        if json:\n",
      "            return np.array([]), stats\n",
      "        else:\n",
      "            sys.exit(0)\n",
      "\n",
      "    if sample_weight is None:\n",
      "        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n",
      "    else:\n",
      "        sample_weight = np.asarray(sample_weight)\n",
      "    if y_true.shape[0]!=y_pred.shape[0]:\n",
      "        raise ValueError(\"y_true and y_pred must be of the same length\")\n",
      "\n",
      "    if normalize not in ['true', 'pred', 'all', None]:\n",
      "        raise ValueError(\"normalize must be one of {'true', 'pred', 'all', None}\")\n",
      "\n",
      "\n",
      "    label_to_ind = {y: x for x, y in enumerate(labels)}\n",
      "    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\n",
      "    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\n",
      "    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n",
      "    y_pred = y_pred[ind]\n",
      "    y_true = y_true[ind]\n",
      "\n",
      "    sample_weight = sample_weight[ind]\n",
      "    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\n",
      "        dtype = np.int64\n",
      "    else:\n",
      "        dtype = np.float64\n",
      "    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()\n",
      "\n",
      "    with np.errstate(all='ignore'):\n",
      "        if normalize == 'true':\n",
      "            cm = cm / cm.sum(axis=1, keepdims=True)\n",
      "        elif normalize == 'pred':\n",
      "            cm = cm / cm.sum(axis=0, keepdims=True)\n",
      "        elif normalize == 'all':\n",
      "            cm = cm / cm.sum()\n",
      "        cm = np.nan_to_num(cm)\n",
      "    return cm, stats\n",
      "\n",
      "\n",
      "def __predict(arr, headerless, csvfile, trim=False):\n",
      "    with open(csvfile, 'r', encoding='utf-8') as csvinput:\n",
      "        reader = csv.reader(csvinput)\n",
      "        if not headerless:\n",
      "            if trim:\n",
      "                header = ','.join([x for i, x in enumerate(next(reader, None)) if i in important_idxs] + ['Prediction'])\n",
      "            else:\n",
      "                header = ','.join(next(reader, None) + ['Prediction'])\n",
      "            print(header)\n",
      "        outputs = __classify(arr)\n",
      "        for i, row in enumerate(reader):\n",
      "            pred = str(__get_key(int(outputs[i]), mapping))\n",
      "            if trim:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]\n",
      "            else:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for field in row]            \n",
      "            row.append(pred)\n",
      "            print(','.join(row))\n",
      "\n",
      "\n",
      "def __preprocess_and_clean_in_memory(arr):\n",
      "    if not isinstance(arr, list) and not isinstance(arr, np.ndarray):\n",
      "        print(f'The input to \\'predict\\' must be a list or np.ndarray but an input of type {type(arr).__name__} was found.')\n",
      "        sys.exit(1)\n",
      "    clean_arr = np.zeros((len(arr), len(important_idxs)))\n",
      "    for i, row in enumerate(arr):\n",
      "        try:\n",
      "            row_used_cols_only = [row[i] for i in important_idxs]\n",
      "        except IndexError:\n",
      "            error_str = f\"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {num_attr}).\"\n",
      "            if len(arr) == num_attr and len(arr[0]) != num_attr:\n",
      "                error_str += \"\\n\\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' \"\n",
      "                error_str += \"rather than as an element of a list. Make sure that even single instances \"\n",
      "                error_str += \"are enclosed in a list. Example: predict_in_memory(0) is invalid but \"\n",
      "                error_str += \"predict_in_memory([0]) is valid.\"\n",
      "            print(error_str)\n",
      "            sys.exit(1)\n",
      "        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]\n",
      "    return clean_arr\n",
      "\n",
      "\n",
      "def __classify(arr, return_probabilities=False):\n",
      "    h = np.dot(arr, w_h.T) + b_h\n",
      "    relu = np.maximum(h, np.zeros_like(h))\n",
      "    out = np.dot(relu, w_o.T) + b_o\n",
      "    if return_probabilities:\n",
      "        exp_o = np.zeros((out.shape[0],))\n",
      "        idxs_negative = np.argwhere(out < 0.).reshape(-1)\n",
      "        if idxs_negative.shape[0] > 0:\n",
      "            exp_o[idxs_negative] = 1. - 1. / (1. + np.exp(out[idxs_negative])).reshape(-1)\n",
      "        idxs_positive = np.argwhere(out >= 0.).reshape(-1)\n",
      "        if idxs_positive.shape[0] > 0:\n",
      "            exp_o[idxs_positive] = 1. / (1. + np.exp(-out[idxs_positive])).reshape(-1)\n",
      "        exp_o = exp_o.reshape(-1, 1)\n",
      "        output = np.concatenate((1. - exp_o, exp_o), axis=1)\n",
      "    else:\n",
      "        output = (out >= 0).astype('int').reshape(-1)\n",
      "    return output\n",
      "\n",
      "\n",
      "def __validate_kwargs(kwargs):\n",
      "    for key in kwargs:\n",
      "        if key not in ['return_probabilities']:\n",
      "        \n",
      "            print(f'{key} is not a keyword argument for Brainome\\'s {classifier_type} predictor. Please see the documentation.')\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def predict(arr, remap=True, **kwargs):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : list[list]\n",
      "        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'.\n",
      "\n",
      "    remap : bool\n",
      "        If True and 'return_probs' is False, remaps the output to the original class\n",
      "        label. If 'return_probs' is True this instead adds a header indicating which\n",
      "        original class label each column of output corresponds to.\n",
      "    \n",
      "    **kwargs :\n",
      "        return_probabilities : bool\n",
      "            If true, return class membership probabilities instead of classifications.\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    output : np.ndarray\n",
      "        A numpy array of\n",
      "\n",
      "            1. Class predictions if 'return_probabilities' is False.\n",
      "            2. Class probabilities if 'return_probabilities' is True.\n",
      "        \"\"\"\n",
      "    kwargs = kwargs or {}\n",
      "    __validate_kwargs(kwargs)\n",
      "    remove_bad_chars = lambda x: str(x).replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    arr = [[remove_bad_chars(field) for field in row] for row in arr]\n",
      "    arr = __preprocess_and_clean_in_memory(arr)\n",
      "    arr = __normalize(arr)\n",
      "    output = __classify(arr, **kwargs)\n",
      "    if remap:\n",
      "        if len(output.shape) > 1: # probabilities were returned\n",
      "            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)\n",
      "            output = np.concatenate((header, output), axis=0)\n",
      "        else:\n",
      "            output = np.array([__get_key(prediction, mapping) for prediction in output])\n",
      "    return output\n",
      "\n",
      "\n",
      "def validate(cleanarr):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    cleanarr : np.ndarray\n",
      "        An array of float values that has undergone each pre-\n",
      "        prediction step.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    count : int\n",
      "        A count of the number of instances in cleanarr.\n",
      "\n",
      "    correct_count : int\n",
      "        A count of the number of correctly classified instances in\n",
      "        cleanarr.\n",
      "\n",
      "    numeachclass : dict\n",
      "        A dictionary mapping each class to its number of instances.\n",
      "\n",
      "    outputs : np.ndarray\n",
      "        The output of the predictor's '__classify' method on cleanarr.\n",
      "    \"\"\"\n",
      "    outputs = __classify(cleanarr[:, :-1])\n",
      "    count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0\n",
      "    correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))\n",
      "    count = outputs.shape[0]\n",
      "    num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))\n",
      "    num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))\n",
      "    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs\n",
      "    \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))\n",
      "    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')\n",
      "    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')\n",
      "    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')\n",
      "    parser.add_argument('-json', action=\"store_true\", default=False, help=\"report measurements as json\")\n",
      "    parser.add_argument('-trim', action=\"store_true\", help=\"If true, the prediction will not output ignored columns.\")\n",
      "    args = parser.parse_args()\n",
      "    faulthandler.enable()\n",
      "\n",
      "    if args.validate:\n",
      "        args.trim = True\n",
      "\n",
      "    is_testfile = not args.validate\n",
      "    \n",
      "    cleanfile = tempfile.NamedTemporaryFile().name\n",
      "    __clean(args.csvfile, cleanfile, args.headerless, is_testfile, trim=args.trim)\n",
      "    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')\n",
      "    if len(cleanarr.shape) == 1:\n",
      "        if args.trim and len(important_idxs) == 1:\n",
      "            cleanarr = cleanarr.reshape(-1, 1)\n",
      "        elif len(open(cleanfile, 'r').read().splitlines()) == 1:\n",
      "            cleanarr = cleanarr.reshape(1, -1)\n",
      "\n",
      "    if not args.trim and ignorecolumns != []:\n",
      "        cleanarr = cleanarr[:, important_idxs].reshape(-1, len(important_idxs))\n",
      "\n",
      "    cleanarr = __normalize(cleanarr)\n",
      "    if not args.validate:\n",
      "        __predict(cleanarr, args.headerless, args.csvfile, trim=args.trim)\n",
      "    else:\n",
      "        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(cleanarr)\n",
      "        \n",
      "        true_labels = cleanarr[:, -1]\n",
      "        classcounts = np.bincount(cleanarr[:, -1].astype('int32')).reshape(-1)\n",
      "        classbalance = (classcounts[np.argwhere(classcounts > 0)] / cleanarr.shape[0]).reshape(-1).tolist()\n",
      "        best_guess = round(100.0 * np.max(classbalance), 2)\n",
      "        H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))\n",
      "        modelacc = int(float(correct_count * 10000) / count) / 100.0\n",
      "\n",
      "        if args.json:\n",
      "            FN = float(num_FN) * 100.0 / float(count)\n",
      "            FP = float(num_FP) * 100.0 / float(count)\n",
      "            TN = float(num_TN) * 100.0 / float(count)\n",
      "            TP = float(num_TP) * 100.0 / float(count)\n",
      "\n",
      "            if int(num_TP + num_FN) != 0:\n",
      "                TPR = num_TP / (num_TP + num_FN)  # Sensitivity, Recall\n",
      "            if int(num_TN + num_FP) != 0:\n",
      "                TNR = num_TN / (num_TN + num_FP)  # Specificity\n",
      "            if int(num_TP + num_FP) != 0:\n",
      "                PPV = num_TP / (num_TP + num_FP)  # Recall\n",
      "            if int(num_FN + num_TP) != 0:\n",
      "                FNR = num_FN / (num_FN + num_TP)  # Miss rate\n",
      "            if int(2 * num_TP + num_FP + num_FN) != 0:\n",
      "                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN)  # F1 Score\n",
      "            if int(num_TP + num_FN + num_FP) != 0:\n",
      "                TS = num_TP / (num_TP + num_FN + num_FP)  # Critical Success Index\n",
      "            json_dict = {'instance_count': count,\n",
      "                         'classifier_type': classifier_type,\n",
      "                         'classes': n_classes,\n",
      "                         'number_correct': correct_count,\n",
      "                         'accuracy': {\n",
      "                             'best_guess': best_guess,\n",
      "                             'improvement': modelacc - best_guess,\n",
      "                             'model_accuracy': modelacc,\n",
      "                         },\n",
      "                         'false_negative_instances': num_FN,\n",
      "                         'false_positive_instances': num_FP,\n",
      "                         'true_positive_instances': num_TP,\n",
      "                         'true_negative_instances': num_TN,\n",
      "                         'false_negatives': FN,\n",
      "                         'false_positives': FP,\n",
      "                         'true_negatives': TN,\n",
      "                         'true_positives': TP,\n",
      "                         'model_capacity': model_cap,\n",
      "                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,\n",
      "                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,\n",
      "                         'shannon_entropy_of_labels': H,\n",
      "                         'classbalance': classbalance} \n",
      "        else:\n",
      "            print(\"Classifier Type:                    Neural Network\")\n",
      "            print(f\"System Type:                        {n_classes}-way classifier\")\n",
      "            print()\n",
      "            print(\"Accuracy:\")\n",
      "            print(\"    Best-guess accuracy:            {:.2f}%\".format(best_guess))\n",
      "            print(\"    Model accuracy:                 {:.2f}%\".format(modelacc) + \" (\" + str(int(correct_count)) + \"/\" + str(count) + \" correct)\")\n",
      "            print(\"    Improvement over best guess:    {:.2f}%\".format(modelacc - best_guess) + \" (of possible \" + str(round(100 - best_guess, 2)) + \"%)\")\n",
      "            print()\n",
      "            print(\"Model capacity (MEC):               {:.0f} bits\".format(model_cap))\n",
      "            if classifier_type == '\\'NN\\'':\n",
      "                print(\"Model Capacity Utilized:            {:.0f} bits\".format(cap_utilized))  # noqa\n",
      "            print(\"Generalization ratio:               {:.2f}\".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + \" bits/bit\")\n",
      "\n",
      "        mtrx, stats = __confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)\n",
      "\n",
      "        if args.json:\n",
      "            json_dict['confusion_matrix'] = mtrx.tolist()\n",
      "            json_dict['multiclass_stats'] = stats\n",
      "            print(json.dumps(json_dict))\n",
      "        else:\n",
      "            mtrx = mtrx.astype('str')\n",
      "            labels = np.array(list(mapping.keys())).reshape(-1, 1)\n",
      "            mtrx = np.concatenate((labels, mtrx), axis=1).astype('str')\n",
      "            max_TP_len, max_FP_len, max_TN_len, max_FN_len = 0, 0, 0, 0\n",
      "            max_class_name_len = len('target') + 2\n",
      "            for classs in mapping.keys():\n",
      "                max_class_name_len = max(max_class_name_len, len(classs))\n",
      "            for key in stats.keys():\n",
      "                class_stats = stats[key]\n",
      "                max_TP_len, max_FP_len, max_TN_len, max_FN_len = max(max_TP_len, len(str(class_stats['TP']))), max(max_FP_len, len(str(class_stats['FP']))), max(\n",
      "                    max_TN_len, len(str(class_stats['TN']))), max(max_FN_len, len(str(class_stats['FN'])))\n",
      "            print()\n",
      "            print(\"Confusion Matrix:\")\n",
      "            print()\n",
      "            max_len_value = int(np.max(np.vectorize(len)(mtrx)))\n",
      "            max_pred_len = (int(mtrx.shape[1]) - 1) * max_len_value\n",
      "\n",
      "            print(\" \" * 4 + \"{:>{}} |{:^{}}\".format(\"Actual\", max_class_name_len, \"Predicted\", max_pred_len))\n",
      "            print(\" \" * 4 + \"-\" * (max_class_name_len + max_pred_len + mtrx.shape[1] + 1))\n",
      "            for row in mtrx:\n",
      "                print(str(\" \" * 4 + \"{:>{}}\".format(row[0], max_class_name_len)) + \" |\" + \"{:^{}}\".format(\n",
      "                    (' '.join([str('{:>{}}'.format(i, max_len_value)) for i in row[1:]])), max_pred_len))\n",
      "            print()\n",
      "            print(\"Accuracy by Class:\")\n",
      "            print()\n",
      "            print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format('target',\n",
      "                                                                                                              max_class_name_len,\n",
      "                                                                                                              'TP', max_TP_len,\n",
      "                                                                                                              'FP', max_FP_len,\n",
      "                                                                                                              'TN', max_TN_len,\n",
      "                                                                                                              'FN', max_FN_len,\n",
      "                                                                                                              'TPR', 'TNR',\n",
      "                                                                                                              'PPV', 'NPV',\n",
      "                                                                                                              'F1', 'TS'))\n",
      "            print(\" \" * 4 + \"-\" * max_class_name_len + ' | ' + \"-\" * (\n",
      "                max_TP_len) + ' ' + \"-\" * max_FP_len + ' ' + \"-\" * max_TN_len + ' ' + \"-\" * max_FN_len + (' ' + 7 * \"-\") * 6)\n",
      "            for raw_class in mapping.keys():\n",
      "                class_stats = stats[int(mapping[raw_class])]\n",
      "                TPR = class_stats['TP'] / (class_stats['TP'] + class_stats['FN']) if int(\n",
      "                    class_stats['TP'] + class_stats['FN']) != 0 else 0\n",
      "                TNR = class_stats['TN'] / (class_stats['TN'] + class_stats['FP']) if int(\n",
      "                    class_stats['TN'] + class_stats['FP']) != 0 else 0\n",
      "                PPV = class_stats['TP'] / (class_stats['TP'] + class_stats['FP']) if int(\n",
      "                    class_stats['TP'] + class_stats['FP']) != 0 else 0\n",
      "                NPV = class_stats['TN'] / (class_stats['TN'] + class_stats['FN']) if int(\n",
      "                    class_stats['TN'] + class_stats['FN']) != 0 else 0\n",
      "                F1 = 2 * class_stats['TP'] / (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                TS = class_stats['TP'] / (class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format(raw_class,\n",
      "                                                                                                                  max_class_name_len,\n",
      "                                                                                                                  class_stats['TP'],\n",
      "                                                                                                                  max_TP_len,\n",
      "                                                                                                                  class_stats['FP'],\n",
      "                                                                                                                  max_FP_len,\n",
      "                                                                                                                  class_stats['TN'],\n",
      "                                                                                                                  max_TN_len,\n",
      "                                                                                                                  class_stats['FN'],\n",
      "                                                                                                                  max_FN_len,\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TPR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TNR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * PPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * NPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * F1, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TS, 2))))\n",
      "            \n",
      "    os.remove(cleanfile)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('NN_predictor.py', 'r') as data:\n",
    "    print(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fee81",
   "metadata": {},
   "source": [
    "## 4. Decision Tree\n",
    "Force the selection of Decision Tree by using the **-f DT** parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a727df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "\u001b[01;1mBrainome Table Compiler v1.006-14-prod\u001b[0m\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:                 y Demo User  (Evaluation)\n",
      "Expiration Date:             2021-12-12   129 days left\n",
      "Maximum File Size:           100 MB\n",
      "Maximum Instances:           20000\n",
      "Maximum Attributes:          100\n",
      "Maximum Classes:             unlimited\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\u001b[01;1mCommand:\u001b[0m\n",
      "    btc data/titanic_train.csv -f DT -y -o DT_predictor.py\n",
      "\n",
      "Start Time:                 08/05/2021, 18:55 UTC\n",
      "\n",
      "Cleaning...done. \n",
      "Splitting into training and validation...done. \n",
      "Pre-training measurements...done. \n",
      "\n",
      "\n",
      "\u001b[01;1mPre-training Measurements\u001b[0m\n",
      "Data:\n",
      "    Input:                      data/titanic_train.csv\n",
      "    Target Column:              Survived\n",
      "    Number of instances:        800\n",
      "    Number of attributes:        11 out of 11\n",
      "    Number of classes:            2\n",
      "\n",
      "Class Balance:                \n",
      "                            died: 61.50%\n",
      "                        survived: 38.50%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          61.50%\n",
      "    Data Sufficiency:             Maybe enough data to generalize. [yellow]\n",
      "\n",
      "Capacity Progression:             at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Ideal Machine Learner:              6,   7,   8,   8,   9,   9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expected Generalization:\n",
      "    Decision Tree:                 2.02 bits/bit\n",
      "    Neural Network:                6.52 bits/bit\n",
      "    Random Forest:                10.13 bits/bit\n",
      "\n",
      "Expected Accuracy:              Training            Validation\n",
      "    Decision Tree:               100.00%                52.50%\n",
      "    Neural Network:                 ----                  ----\n",
      "    Random Forest:               100.00%                80.25%\n",
      "\n",
      "Recommendations:\n",
      "    Warning: Data has high information density. Using effort 5 and larger ( -e 5 ) can improve results.\n",
      "    We recommend using Random Forest -f RF.\n",
      "    If predictor accuracy is insufficient, try using the option -rank to automatically select the important attributes.\n",
      "    If predictor accuracy is insufficient, try using the effort option -e with a value of 5 or more to increase training time.\n",
      "    Model type DT given by user. \n",
      "\n",
      "Time to Build Estimates:\n",
      "    Decision Tree:                a few seconds\n",
      "\n",
      "\n",
      "Building classifier...done. \n",
      "Compiling predictor...done. \n",
      "Validating predictor...done. \n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        DT_predictor.py\n",
      "    Classifier Type:              Decision Tree\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:         100.00% (479/479 correct)\n",
      "      Validation Accuracy:        54.82% (176/321 correct)\n",
      "      Combined Model Accuracy:    81.87% (655/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):        236    bits\n",
      "\n",
      "    Generalization Ratio:          1.94 bits/bit\n",
      "    Percent of Data Memorized:   104.60%\n",
      "    Resilience to Noise:          -0.31 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  295    0 \n",
      "            survived |    0  184 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  124   73 \n",
      "            survived |   72   52 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  295    0  184    0  100.00%  100.00%  100.00%  100.00%  100.00%  100.00%\n",
      "            survived |  184    0  295    0  100.00%  100.00%  100.00%  100.00%  100.00%  100.00%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  124   72   52   73   62.94%   41.94%   63.27%   41.60%   63.10%   46.10%\n",
      "            survived |   52   73  124   72   41.94%   62.94%   41.60%   63.27%   41.77%   26.40%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "End Time:           08/05/2021, 18:55 UTC\n",
      "Runtime Duration:   9s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!brainome data/titanic_train.csv -f DT -y -o DT_predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c326bb",
   "metadata": {},
   "source": [
    "### View Decision Tree Predictor Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "398a4312",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "#\n",
      "# This code has been produced by a free evaluation version of Brainome(tm).\n",
      "# Portions of this code copyright (c) 2019-2021 by Brainome, Inc. All Rights Reserved.\n",
      "# Brainome, Inc grants an exclusive (subject to our continuing rights to use and modify models),\n",
      "# worldwide, non-sublicensable, and non-transferable limited license to use and modify this\n",
      "# predictor produced through the input of your data:\n",
      "# (i) for users accessing the service through a free evaluation account, solely for your\n",
      "# own non-commercial purposes, including for the purpose of evaluating this service, and\n",
      "# (ii) for users accessing the service through a paid, commercial use account, for your\n",
      "# own internal  and commercial purposes.\n",
      "# Please contact support@brainome.ai with any questions.\n",
      "# Use of predictions results at your own risk.\n",
      "#\n",
      "# Output of Brainome v1.006-14-prod.\n",
      "# Invocation: brainome data/titanic_train.csv -f DT -y -o DT_predictor.py\n",
      "# Total compiler execution time: 0:00:08.31. Finished on: Aug-05-2021 18:55:09.\n",
      "# This source code requires Python 3.\n",
      "#\n",
      "\"\"\"\n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        DT_predictor.py\n",
      "    Classifier Type:              Decision Tree\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:         100.00% (479/479 correct)\n",
      "      Validation Accuracy:        54.82% (176/321 correct)\n",
      "      Combined Model Accuracy:    81.87% (655/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):        236    bits\n",
      "\n",
      "    Generalization Ratio:          1.94 bits/bit\n",
      "    Percent of Data Memorized:   104.60%\n",
      "    Resilience to Noise:          -0.31 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  295    0 \n",
      "            survived |    0  184 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  124   73 \n",
      "            survived |   72   52 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  295    0  184    0  100.00%  100.00%  100.00%  100.00%  100.00%  100.00%\n",
      "            survived |  184    0  295    0  100.00%  100.00%  100.00%  100.00%  100.00%  100.00%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  124   72   52   73   62.94%   41.94%   63.27%   41.60%   63.10%   46.10%\n",
      "            survived |   52   73  124   72   41.94%   62.94%   41.60%   63.27%   41.77%   26.40%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "import math\n",
      "import os\n",
      "import argparse\n",
      "import tempfile\n",
      "import csv\n",
      "import binascii\n",
      "import faulthandler\n",
      "import json\n",
      "from io import StringIO\n",
      "try:\n",
      "    import numpy as np # For numpy see: http://numpy.org\n",
      "    from numpy import array\n",
      "except:\n",
      "    print(\"This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.\")\n",
      "    sys.exit(1)\n",
      "try:\n",
      "    from scipy.sparse import coo_matrix\n",
      "    report_cmat = True\n",
      "except:\n",
      "    print(\"Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.\")\n",
      "    report_cmat = False\n",
      "\n",
      "IOBUF = 100000000\n",
      "sys.setrecursionlimit(1000000)\n",
      "TRAINFILE = ['data/titanic_train.csv']\n",
      "mapping = {'died': 0, 'survived': 1}\n",
      "ignorelabels = []\n",
      "ignorecolumns = []\n",
      "target = '' \n",
      "target_column = 11\n",
      "important_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "ignore_idxs = []\n",
      "classifier_type = 'DT'\n",
      "num_attr = 11\n",
      "n_classes = 2\n",
      "model_cap = 236\n",
      "energy_thresholds = [1954209900.8771, 1966898446.45, 1984687310.9479, 2048340405.7896, 2056979596.4625, 2107370388.5, 2145455080.19375, 2234551890.9271, 2274864027.16045, 2297909074.8875, 2318640756.47915, 2322555857.89165, 2326281326.92705, 2353978967.9479, 2409434516.0, 2493169439.17085, 2502538436.2271004, 2514902969.64375, 2545776911.6979, 2575495144.09165, 2578804290.57915, 2795911355.6979, 2811464062.35625, 2826506978.03125, 2844147297.5, 2851749332.9, 2856953181.9625, 2980083163.025, 3042487605.55, 3302692341.93335, 3317601512.5708, 3323619280.7729, 3385208653.8812504, 3399670587.6625004, 3468349520.8500004, 3624279345.44375, 3650404669.0375, 3689560160.45, 3716234801.1854, 3735010317.4479, 3750958546.0, 3763780575.6375, 3767135098.8875, 3822342719.72085, 3856033956.75415, 3857859285.89375, 3911598343.75, 3935232820.0, 3986733203.0, 4029514084.7375, 4089905774.7521, 4140186056.5, 4153805793.75205, 4186007605.5, 4210270211.45, 4221901496.49585, 4244157280.3875, 4259857635.0521, 4281344895.61, 4290748065.475, 4296340386.17915, 4349006031.6146, 4379925147.125, 4385497434.0021, 4398144900.389601, 4445071852.8104, 4450348446.1229, 4469997972.71665, 4481409165.891649, 4529057880.6375, 4532391556.0979, 4588070535.4271, 4595868264.5, 4616711222.8604, 4643099864.3021, 4658452126.4271, 4674212511.9479, 4765856621.69585, 4781167103.608351, 4805454584.82085, 4819885874.6021, 4837386631.8646, 4843647162.3, 4860690905.33125, 4862445480.9771, 4901366805.72915, 4909726762.47915, 4922137355.64835, 4951820595.15, 4984556408.0, 4999559285.09375, 5010578725.608351, 5015112806.391701, 5033951434.25, 5084281947.30625, 5116305332.35625, 5123610985.25, 5158807534.36875, 5172396555.11875, 5185912949.025, 5190711884.5, 5192349379.14585, 5199028594.70835, 5227058073.1958, 5234919899.6854, 5329033860.3646, 5343647870.512501, 5357663076.31045, 5378420196.62085, 5417870884.375, 5439386095.25, 5468903044.5, 5499731215.875, 5502753107.25, 5510576795.75, 5554968013.6229, 5574584663.0625, 5583352990.5625, 5587254047.025, 5607451274.39375, 5622928118.80625, 5637811791.125, 5650245206.6771, 5699674567.36875, 5729367282.325, 5784229181.1375, 5790703502.2396, 5843012629.125, 5848562202.0, 5864174942.3708, 5870822842.6229, 5880771609.7125, 5889370258.2354, 5931098954.97705, 5950585855.4979, 5963497803.3875, 5974614295.6625, 5995467186.27085, 6007707354.6667, 6011527670.0271, 6015088077.3625, 6019225072.45, 6028598468.6479, 6033388534.5, 6040021424.1104, 6045252552.85, 6052538892.2396, 6081882252.9, 6120754780.25, 6121963144.2146, 6136222423.4146, 6169774668.70625, 6192631177.08125, 6202180107.0625, 6212014482.1479, 6230711546.9771, 6233262960.4646, 6268215191.0625, 6298922277.025, 6324071306.1979, 6368123334.075001, 6382195352.3646, 6424368966.0, 6433637134.625, 6455963376.8521, 6469500005.1021, 6472084137.5, 6529033898.61665, 6541877226.17915, 6584236843.2, 6597976446.35, 6614488786.525, 6624425763.525, 6626978597.1896, 6663646240.23125, 6682881575.925, 6689988148.175, 6726411924.125, 6766649404.5625, 6782019230.775, 6805360199.625, 6873122026.51875, 6894233205.2125, 6926920761.25, 6968787745.5146, 6985867400.7229, 6993243714.5354, 6994927518.15, 7001519513.75, 7046520885.75, 7049087584.425, 7071633639.25, 7101645922.525, 7169414373.0, 7202912572.8, 7290563941.15, 7299909109.75, 7414381449.55, 7491442229.4375, 7534498748.6375, 7542113909.6, 7601144941.7875, 7613316099.6375, 7631758009.275, 7648174372.525, 7668405429.3146, 7707278400.0396, 7887171730.68335, 7956991389.5625, 7983444837.25, 8024897991.4625, 8074177817.925, 8119487168.98125, 8135137325.03125, 8146811049.731251, 8233197020.14375, 8311605456.875, 8343935520.4646, 8466589564.5625, 8675188562.0625, 8686805156.75, 8748252215.5125, 8805153271.0, 8812221864.42915, 8908803595.5125, 9031143140.75, 9143531085.575, 9246739039.25, 9275650782.3, 9306656233.825, 9681523874.375, 9792453097.2, 9921711462.7, 10421083251.11875, 10797812860.5125, 11170140893.510399]\n",
      "start_label = 0\n",
      "\n",
      "\n",
      "def __convert(cell):\n",
      "    value = str(cell)\n",
      "    try:\n",
      "        result = int(value)\n",
      "        return result\n",
      "    except ValueError:\n",
      "        try:\n",
      "            result=float(value)\n",
      "            if math.isnan(result):\n",
      "                print('NaN value found. Aborting.')\n",
      "                sys.exit(1)\n",
      "            return result\n",
      "        except ValueError:\n",
      "            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def __get_key(val, dictionary):\n",
      "    if dictionary == {}:\n",
      "        return val\n",
      "    for key, value in dictionary.items(): \n",
      "        if val == value:\n",
      "            return key\n",
      "    if val not in dictionary.values:\n",
      "        print(\"Label key does not exist\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __convertclassid(cell, classlist=[]):\n",
      "\n",
      "    value = str(cell)\n",
      "    \n",
      "    if value == '':\n",
      "        print('Empty value encountered for a class label. Aborting.')\n",
      "        sys.exit(1)\n",
      "    \n",
      "    if mapping != {}:\n",
      "        result = -1\n",
      "        try:\n",
      "            result = mapping[cell]\n",
      "        except KeyError:\n",
      "            print(f\"The class label {value} does not exist in the class mapping. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "        return result\n",
      "    \n",
      "    try:\n",
      "        result = float(cell)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "    except:\n",
      "        result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "        if result in classlist:\n",
      "            result = classlist.index(result)\n",
      "        else:\n",
      "            classlist.append(str(result))\n",
      "            result = classlist.index(result)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "    finally:\n",
      "        if result < 0:\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to non-negative integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "    return result\n",
      "\n",
      "\n",
      "def __clean(filename, outfile, headerless=False, testfile=False, trim=False):\n",
      "    classlist = []\n",
      "    outbuf = []\n",
      "    remove_bad_chars = lambda x: x.replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    \n",
      "    with open(filename, encoding='utf-8') as csv_file, open(outfile, \"w+\", encoding='utf-8') as f:\n",
      "        \n",
      "        reader = csv.reader(csv_file)\n",
      "        if not headerless:\n",
      "            next(reader, None)\n",
      "        \n",
      "        for i, row in enumerate(reader):\n",
      "\n",
      "            if row == []:\n",
      "                continue\n",
      "\n",
      "            \n",
      "            expected_row_length = len(important_idxs)\n",
      "            if not trim:\n",
      "                expected_row_length += len(ignorecolumns)\n",
      "            if not testfile:\n",
      "                expected_row_length += 1\n",
      "            actual_row_length = len(row)\n",
      "\n",
      "            if testfile and actual_row_length == expected_row_length + 1:\n",
      "                error_str = f\"We found {actual_row_length} columns but expected {expected_row_length} columns at row {i}. \"\n",
      "                error_str += f\"Please check that the CSV contains no target column otherwise use -validate. Aborting.\"\n",
      "                print(error_str)\n",
      "                sys.exit(1)\n",
      "            \n",
      "            if actual_row_length != expected_row_length:\n",
      "                print(f\"We found {actual_row_length} columns but expected {expected_row_length} columns.\")\n",
      "                sys.exit(1)            \n",
      "\n",
      "            if testfile:\n",
      "                if len(row) == 1:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(row[0])))]\n",
      "                else:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convert(remove_bad_chars(row[-1])))]         \n",
      "            else:\n",
      "                converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convertclassid(row[-1], classlist))]\n",
      "            outbuf.extend(converted_row)\n",
      "\n",
      "            if len(outbuf) < IOBUF:\n",
      "                outbuf.append(os.linesep)\n",
      "            else:\n",
      "                print(''.join(outbuf), file=f)\n",
      "                outbuf = []\n",
      "        \n",
      "        print(''.join(outbuf), end=\"\", file=f)\n",
      "\n",
      "    n_classes_found = len(classlist)\n",
      "    if not testfile and n_classes_found < 2:\n",
      "        print(f\"Only {n_classes_found} classes were found. Aborting.\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):\n",
      "    stats = {}\n",
      "    if labels is None:\n",
      "        labels = np.array(list(set(list(y_true.astype('int')))))\n",
      "    else:\n",
      "        labels = np.asarray(labels)\n",
      "        if np.all([l not in y_true for l in labels]):\n",
      "            raise ValueError(\"At least one label specified must be in y_true\")\n",
      "    n_labels = labels.size\n",
      "\n",
      "    for class_i in range(n_labels):\n",
      "        stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}\n",
      "        class_i_indices = np.argwhere(y_true==class_i)\n",
      "        not_class_i_indices = np.argwhere(y_true!=class_i)\n",
      "        stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices] == class_i))\n",
      "        stats[int(class_i)]['FN'] = int(np.sum(y_pred[class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['FP'] = int(np.sum(y_pred[not_class_i_indices] == class_i))\n",
      "\n",
      "    if not report_cmat:\n",
      "        if json:\n",
      "            return np.array([]), stats\n",
      "        else:\n",
      "            sys.exit(0)\n",
      "\n",
      "    if sample_weight is None:\n",
      "        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n",
      "    else:\n",
      "        sample_weight = np.asarray(sample_weight)\n",
      "    if y_true.shape[0]!=y_pred.shape[0]:\n",
      "        raise ValueError(\"y_true and y_pred must be of the same length\")\n",
      "\n",
      "    if normalize not in ['true', 'pred', 'all', None]:\n",
      "        raise ValueError(\"normalize must be one of {'true', 'pred', 'all', None}\")\n",
      "\n",
      "\n",
      "    label_to_ind = {y: x for x, y in enumerate(labels)}\n",
      "    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\n",
      "    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\n",
      "    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n",
      "    y_pred = y_pred[ind]\n",
      "    y_true = y_true[ind]\n",
      "\n",
      "    sample_weight = sample_weight[ind]\n",
      "    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\n",
      "        dtype = np.int64\n",
      "    else:\n",
      "        dtype = np.float64\n",
      "    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()\n",
      "\n",
      "    with np.errstate(all='ignore'):\n",
      "        if normalize == 'true':\n",
      "            cm = cm / cm.sum(axis=1, keepdims=True)\n",
      "        elif normalize == 'pred':\n",
      "            cm = cm / cm.sum(axis=0, keepdims=True)\n",
      "        elif normalize == 'all':\n",
      "            cm = cm / cm.sum()\n",
      "        cm = np.nan_to_num(cm)\n",
      "    return cm, stats\n",
      "\n",
      "\n",
      "def __predict(arr, headerless, csvfile, trim=False):\n",
      "    with open(csvfile, 'r', encoding='utf-8') as csvinput:\n",
      "        reader = csv.reader(csvinput)\n",
      "        if not headerless:\n",
      "            if trim:\n",
      "                header = ','.join([x for i, x in enumerate(next(reader, None)) if i in important_idxs] + ['Prediction'])\n",
      "            else:\n",
      "                header = ','.join(next(reader, None) + ['Prediction'])\n",
      "            print(header)\n",
      "        outputs = __classify(arr)\n",
      "        for i, row in enumerate(reader):\n",
      "            pred = str(__get_key(int(outputs[i]), mapping))\n",
      "            if trim:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]\n",
      "            else:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for field in row]            \n",
      "            row.append(pred)\n",
      "            print(','.join(row))\n",
      "\n",
      "\n",
      "def __preprocess_and_clean_in_memory(arr):\n",
      "    if not isinstance(arr, list) and not isinstance(arr, np.ndarray):\n",
      "        print(f'The input to \\'predict\\' must be a list or np.ndarray but an input of type {type(arr).__name__} was found.')\n",
      "        sys.exit(1)\n",
      "    clean_arr = np.zeros((len(arr), len(important_idxs)))\n",
      "    for i, row in enumerate(arr):\n",
      "        try:\n",
      "            row_used_cols_only = [row[i] for i in important_idxs]\n",
      "        except IndexError:\n",
      "            error_str = f\"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {num_attr}).\"\n",
      "            if len(arr) == num_attr and len(arr[0]) != num_attr:\n",
      "                error_str += \"\\n\\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' \"\n",
      "                error_str += \"rather than as an element of a list. Make sure that even single instances \"\n",
      "                error_str += \"are enclosed in a list. Example: predict_in_memory(0) is invalid but \"\n",
      "                error_str += \"predict_in_memory([0]) is valid.\"\n",
      "            print(error_str)\n",
      "            sys.exit(1)\n",
      "        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]\n",
      "    return clean_arr\n",
      "\n",
      "\n",
      "def __classify(rows):\n",
      "    try:\n",
      "        energys = np.sum(rows, axis=1, dtype=np.float128)\n",
      "    except:\n",
      "        energys = np.sum(rows, axis=1, dtype=np.longdouble)\n",
      "    numers = np.searchsorted(energy_thresholds, energys, side='left') - 1\n",
      "    indys = np.argwhere(np.logical_and(numers <= len(energy_thresholds), numers >= 0)).reshape(-1)\n",
      "    defaultindys = np.argwhere(np.logical_not(np.logical_and(numers <= len(energy_thresholds), numers >= 0))).reshape(-1)\n",
      "    output = np.zeros(energys.shape[0])\n",
      "    output[indys] = (numers[indys] + 0) % 2\n",
      "    if list(defaultindys):\n",
      "        output[defaultindys] = 1\n",
      "    return output\n",
      "\n",
      "\n",
      "def __validate_kwargs(kwargs):\n",
      "    for key in kwargs:\n",
      "        if key not in []: \n",
      "            print(f'{key} is not a keyword argument for Brainome\\'s {classifier_type} predictor. Please see the documentation.')\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def predict(arr, remap=True, **kwargs):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : list[list]\n",
      "        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'.\n",
      "\n",
      "    remap : bool\n",
      "        If True, remaps the output to the original class label.\n",
      "    \n",
      "    **kwargs :\n",
      "        None\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    output : np.ndarray\n",
      "        A numpy array of predictions.\n",
      "    \"\"\"\n",
      "    kwargs = kwargs or {}\n",
      "    __validate_kwargs(kwargs)\n",
      "    remove_bad_chars = lambda x: str(x).replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    arr = [[remove_bad_chars(field) for field in row] for row in arr]\n",
      "    arr = __preprocess_and_clean_in_memory(arr)\n",
      "    output = __classify(arr, **kwargs)\n",
      "    if remap:\n",
      "        if len(output.shape) > 1: # probabilities were returned\n",
      "            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)\n",
      "            output = np.concatenate((header, output), axis=0)\n",
      "        else:\n",
      "            output = np.array([__get_key(prediction, mapping) for prediction in output])\n",
      "    return output\n",
      "\n",
      "\n",
      "def validate(cleanarr):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    cleanarr : np.ndarray\n",
      "        An array of float values that has undergone each pre-\n",
      "        prediction step.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    count : int\n",
      "        A count of the number of instances in cleanarr.\n",
      "\n",
      "    correct_count : int\n",
      "        A count of the number of correctly classified instances in\n",
      "        cleanarr.\n",
      "\n",
      "    numeachclass : dict\n",
      "        A dictionary mapping each class to its number of instances.\n",
      "\n",
      "    outputs : np.ndarray\n",
      "        The output of the predictor's '__classify' method on cleanarr.\n",
      "    \"\"\"\n",
      "    outputs = __classify(cleanarr[:, :-1])\n",
      "    count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0\n",
      "    correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))\n",
      "    count = outputs.shape[0]\n",
      "    num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))\n",
      "    num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))\n",
      "    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs\n",
      "    \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))\n",
      "    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')\n",
      "    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')\n",
      "    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')\n",
      "    parser.add_argument('-json', action=\"store_true\", default=False, help=\"report measurements as json\")\n",
      "    parser.add_argument('-trim', action=\"store_true\", help=\"If true, the prediction will not output ignored columns.\")\n",
      "    args = parser.parse_args()\n",
      "    faulthandler.enable()\n",
      "\n",
      "    if args.validate:\n",
      "        args.trim = True\n",
      "\n",
      "    is_testfile = not args.validate\n",
      "    \n",
      "    cleanfile = tempfile.NamedTemporaryFile().name\n",
      "    __clean(args.csvfile, cleanfile, args.headerless, is_testfile, trim=args.trim)\n",
      "    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')\n",
      "    if len(cleanarr.shape) == 1:\n",
      "        if args.trim and len(important_idxs) == 1:\n",
      "            cleanarr = cleanarr.reshape(-1, 1)\n",
      "        elif len(open(cleanfile, 'r').read().splitlines()) == 1:\n",
      "            cleanarr = cleanarr.reshape(1, -1)\n",
      "\n",
      "    if not args.trim and ignorecolumns != []:\n",
      "        cleanarr = cleanarr[:, important_idxs].reshape(-1, len(important_idxs))\n",
      "\n",
      "    if not args.validate:\n",
      "        __predict(cleanarr, args.headerless, args.csvfile, trim=args.trim)\n",
      "    else:\n",
      "        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(cleanarr)\n",
      "        \n",
      "        true_labels = cleanarr[:, -1]\n",
      "        classcounts = np.bincount(cleanarr[:, -1].astype('int32')).reshape(-1)\n",
      "        classbalance = (classcounts[np.argwhere(classcounts > 0)] / cleanarr.shape[0]).reshape(-1).tolist()\n",
      "        best_guess = round(100.0 * np.max(classbalance), 2)\n",
      "        H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))\n",
      "        modelacc = int(float(correct_count * 10000) / count) / 100.0\n",
      "\n",
      "        if args.json:\n",
      "            FN = float(num_FN) * 100.0 / float(count)\n",
      "            FP = float(num_FP) * 100.0 / float(count)\n",
      "            TN = float(num_TN) * 100.0 / float(count)\n",
      "            TP = float(num_TP) * 100.0 / float(count)\n",
      "\n",
      "            if int(num_TP + num_FN) != 0:\n",
      "                TPR = num_TP / (num_TP + num_FN)  # Sensitivity, Recall\n",
      "            if int(num_TN + num_FP) != 0:\n",
      "                TNR = num_TN / (num_TN + num_FP)  # Specificity\n",
      "            if int(num_TP + num_FP) != 0:\n",
      "                PPV = num_TP / (num_TP + num_FP)  # Recall\n",
      "            if int(num_FN + num_TP) != 0:\n",
      "                FNR = num_FN / (num_FN + num_TP)  # Miss rate\n",
      "            if int(2 * num_TP + num_FP + num_FN) != 0:\n",
      "                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN)  # F1 Score\n",
      "            if int(num_TP + num_FN + num_FP) != 0:\n",
      "                TS = num_TP / (num_TP + num_FN + num_FP)  # Critical Success Index\n",
      "            json_dict = {'instance_count': count,\n",
      "                         'classifier_type': classifier_type,\n",
      "                         'classes': n_classes,\n",
      "                         'number_correct': correct_count,\n",
      "                         'accuracy': {\n",
      "                             'best_guess': best_guess,\n",
      "                             'improvement': modelacc - best_guess,\n",
      "                             'model_accuracy': modelacc,\n",
      "                         },\n",
      "                         'false_negative_instances': num_FN,\n",
      "                         'false_positive_instances': num_FP,\n",
      "                         'true_positive_instances': num_TP,\n",
      "                         'true_negative_instances': num_TN,\n",
      "                         'false_negatives': FN,\n",
      "                         'false_positives': FP,\n",
      "                         'true_negatives': TN,\n",
      "                         'true_positives': TP,\n",
      "                         'model_capacity': model_cap,\n",
      "                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,\n",
      "                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,\n",
      "                         'shannon_entropy_of_labels': H,\n",
      "                         'classbalance': classbalance} \n",
      "        else:\n",
      "            print(\"Classifier Type:                    Decision Tree\")\n",
      "            print(f\"System Type:                        {n_classes}-way classifier\")\n",
      "            print()\n",
      "            print(\"Accuracy:\")\n",
      "            print(\"    Best-guess accuracy:            {:.2f}%\".format(best_guess))\n",
      "            print(\"    Model accuracy:                 {:.2f}%\".format(modelacc) + \" (\" + str(int(correct_count)) + \"/\" + str(count) + \" correct)\")\n",
      "            print(\"    Improvement over best guess:    {:.2f}%\".format(modelacc - best_guess) + \" (of possible \" + str(round(100 - best_guess, 2)) + \"%)\")\n",
      "            print()\n",
      "            print(\"Model capacity (MEC):               {:.0f} bits\".format(model_cap))\n",
      "            if classifier_type == '\\'NN\\'':\n",
      "                print(\"Model Capacity Utilized:            {:.0f} bits\".format(cap_utilized))  # noqa\n",
      "            print(\"Generalization ratio:               {:.2f}\".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + \" bits/bit\")\n",
      "\n",
      "        mtrx, stats = __confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)\n",
      "\n",
      "        if args.json:\n",
      "            json_dict['confusion_matrix'] = mtrx.tolist()\n",
      "            json_dict['multiclass_stats'] = stats\n",
      "            print(json.dumps(json_dict))\n",
      "        else:\n",
      "            mtrx = mtrx.astype('str')\n",
      "            labels = np.array(list(mapping.keys())).reshape(-1, 1)\n",
      "            mtrx = np.concatenate((labels, mtrx), axis=1).astype('str')\n",
      "            max_TP_len, max_FP_len, max_TN_len, max_FN_len = 0, 0, 0, 0\n",
      "            max_class_name_len = len('target') + 2\n",
      "            for classs in mapping.keys():\n",
      "                max_class_name_len = max(max_class_name_len, len(classs))\n",
      "            for key in stats.keys():\n",
      "                class_stats = stats[key]\n",
      "                max_TP_len, max_FP_len, max_TN_len, max_FN_len = max(max_TP_len, len(str(class_stats['TP']))), max(max_FP_len, len(str(class_stats['FP']))), max(\n",
      "                    max_TN_len, len(str(class_stats['TN']))), max(max_FN_len, len(str(class_stats['FN'])))\n",
      "            print()\n",
      "            print(\"Confusion Matrix:\")\n",
      "            print()\n",
      "            max_len_value = int(np.max(np.vectorize(len)(mtrx)))\n",
      "            max_pred_len = (int(mtrx.shape[1]) - 1) * max_len_value\n",
      "\n",
      "            print(\" \" * 4 + \"{:>{}} |{:^{}}\".format(\"Actual\", max_class_name_len, \"Predicted\", max_pred_len))\n",
      "            print(\" \" * 4 + \"-\" * (max_class_name_len + max_pred_len + mtrx.shape[1] + 1))\n",
      "            for row in mtrx:\n",
      "                print(str(\" \" * 4 + \"{:>{}}\".format(row[0], max_class_name_len)) + \" |\" + \"{:^{}}\".format(\n",
      "                    (' '.join([str('{:>{}}'.format(i, max_len_value)) for i in row[1:]])), max_pred_len))\n",
      "            print()\n",
      "            print(\"Accuracy by Class:\")\n",
      "            print()\n",
      "            print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format('target',\n",
      "                                                                                                              max_class_name_len,\n",
      "                                                                                                              'TP', max_TP_len,\n",
      "                                                                                                              'FP', max_FP_len,\n",
      "                                                                                                              'TN', max_TN_len,\n",
      "                                                                                                              'FN', max_FN_len,\n",
      "                                                                                                              'TPR', 'TNR',\n",
      "                                                                                                              'PPV', 'NPV',\n",
      "                                                                                                              'F1', 'TS'))\n",
      "            print(\" \" * 4 + \"-\" * max_class_name_len + ' | ' + \"-\" * (\n",
      "                max_TP_len) + ' ' + \"-\" * max_FP_len + ' ' + \"-\" * max_TN_len + ' ' + \"-\" * max_FN_len + (' ' + 7 * \"-\") * 6)\n",
      "            for raw_class in mapping.keys():\n",
      "                class_stats = stats[int(mapping[raw_class])]\n",
      "                TPR = class_stats['TP'] / (class_stats['TP'] + class_stats['FN']) if int(\n",
      "                    class_stats['TP'] + class_stats['FN']) != 0 else 0\n",
      "                TNR = class_stats['TN'] / (class_stats['TN'] + class_stats['FP']) if int(\n",
      "                    class_stats['TN'] + class_stats['FP']) != 0 else 0\n",
      "                PPV = class_stats['TP'] / (class_stats['TP'] + class_stats['FP']) if int(\n",
      "                    class_stats['TP'] + class_stats['FP']) != 0 else 0\n",
      "                NPV = class_stats['TN'] / (class_stats['TN'] + class_stats['FN']) if int(\n",
      "                    class_stats['TN'] + class_stats['FN']) != 0 else 0\n",
      "                F1 = 2 * class_stats['TP'] / (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                TS = class_stats['TP'] / (class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format(raw_class,\n",
      "                                                                                                                  max_class_name_len,\n",
      "                                                                                                                  class_stats['TP'],\n",
      "                                                                                                                  max_TP_len,\n",
      "                                                                                                                  class_stats['FP'],\n",
      "                                                                                                                  max_FP_len,\n",
      "                                                                                                                  class_stats['TN'],\n",
      "                                                                                                                  max_TN_len,\n",
      "                                                                                                                  class_stats['FN'],\n",
      "                                                                                                                  max_FN_len,\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TPR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TNR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * PPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * NPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * F1, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TS, 2))))\n",
      "            \n",
      "    os.remove(cleanfile)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('DT_predictor.py', 'r') as data:\n",
    "    print(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36fb88",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Check out [Brainome 104_Using_Predictor](./brainome_104_Using_Predictor.ipynb)\n",
    "- Check out [Brainome 201 Measurements](./brainome_201_Measurements.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
