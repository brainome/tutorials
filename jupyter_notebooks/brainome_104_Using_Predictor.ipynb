{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895a5bb2",
   "metadata": {},
   "source": [
    "![](https://www.brainome.ai/wp-content/uploads/2020/08/brainome_logo.png)\n",
    "# 104 Using Brainome's Predictors\n",
    "The python predictor generated by Brainome has three modes.\n",
    "1. Validate\n",
    "2. Classify\n",
    "3. Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b880e",
   "metadata": {},
   "source": [
    "## Install brainome and generate a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57882e55",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: brainome in /usr/local/lib/python3.9/site-packages (1.6.68)\n",
      "Requirement already satisfied: brainome-mac-python3.9==1.6.* in /usr/local/lib/python3.9/site-packages (from brainome) (1.6.14)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (0.24.2)\n",
      "Requirement already satisfied: Jinja2>=3.0.0 in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (3.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (2.5.4.1)\n",
      "Requirement already satisfied: xgboost==1.4.2 in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (1.4.2)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (1.20.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/site-packages (from xgboost==1.4.2->brainome-mac-python3.9==1.6.*->brainome) (1.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from Jinja2>=3.0.0->brainome-mac-python3.9==1.6.*->brainome) (2.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-mac-python3.9==1.6.*->brainome) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-mac-python3.9==1.6.*->brainome) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torch>=1.4.0->brainome-mac-python3.9==1.6.*->brainome) (3.10.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "\u001b[01;1mBrainome Table Compiler v1.006-14-prod\u001b[0m\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:                 y Demo User  (Evaluation)\n",
      "Expiration Date:             2021-12-12   131 days left\n",
      "Maximum File Size:           100 MB\n",
      "Maximum Instances:           20000\n",
      "Maximum Attributes:          100\n",
      "Maximum Classes:             unlimited\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\u001b[01;1mCommand:\u001b[0m\n",
      "    btc data/titanic_train.csv -y -o predictor_104.py\n",
      "\n",
      "Start Time:                 08/03/2021, 15:55 PDT\n",
      "\n",
      "Cleaning...done. \n",
      "Splitting into training and validation...done. \n",
      "Pre-training measurements...done. \n",
      "\n",
      "\n",
      "\u001b[01;1mPre-training Measurements\u001b[0m\n",
      "Data:\n",
      "    Input:                      data/titanic_train.csv\n",
      "    Target Column:              Survived\n",
      "    Number of instances:        800\n",
      "    Number of attributes:        11 out of 11\n",
      "    Number of classes:            2\n",
      "\n",
      "Class Balance:                \n",
      "                            died: 61.50%\n",
      "                        survived: 38.50%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          61.50%\n",
      "    Data Sufficiency:             Maybe enough data to generalize. [yellow]\n",
      "\n",
      "Capacity Progression:             at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Ideal Machine Learner:              6,   7,   8,   8,   9,   9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expected Generalization:\n",
      "    Decision Tree:                 2.02 bits/bit\n",
      "    Neural Network:                6.52 bits/bit\n",
      "    Random Forest:                10.13 bits/bit\n",
      "\n",
      "Expected Accuracy:              Training            Validation\n",
      "    Decision Tree:               100.00%                52.50%\n",
      "    Neural Network:                 ----                  ----\n",
      "    Random Forest:               100.00%                80.25%\n",
      "\n",
      "Recommendations:\n",
      "    Warning: Data has high information density. Using effort 5 and larger ( -e 5 ) can improve results.\n",
      "    We recommend using Random Forest -f RF.\n",
      "    If predictor accuracy is insufficient, try using the option -rank to automatically select the important attributes.\n",
      "    If predictor accuracy is insufficient, try using the effort option -e with a value of 5 or more to increase training time.\n",
      "    Defaulting to RF model. Model can be forced with -f parameter. \n",
      "\n",
      "\n",
      "Building classifier...done. \n",
      "Compiling predictor...done. \n",
      "Validating predictor...done. \n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        predictor_104.py\n",
      "    Classifier Type:              Random Forest\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          86.84% (416/479 correct)\n",
      "      Validation Accuracy:        80.99% (260/321 correct)\n",
      "      Combined Model Accuracy:    84.50% (676/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         41    bits\n",
      "\n",
      "    Generalization Ratio:          9.74 bits/bit\n",
      "    Percent of Data Memorized:    20.84%\n",
      "    Resilience to Noise:          -1.01 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  279   16 \n",
      "            survived |   47  137 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  175   22 \n",
      "            survived |   39   85 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  279   47  137   16   94.58%   74.46%   85.58%   89.54%   89.86%   81.58%\n",
      "            survived |  137   16  279   47   74.46%   94.58%   89.54%   85.58%   81.31%   68.50%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  175   39   85   22   88.83%   68.55%   81.78%   79.44%   85.16%   74.15%\n",
      "            survived |   85   22  175   39   68.55%   88.83%   79.44%   81.78%   73.59%   58.22%\n",
      "\n",
      "\n",
      "    Attribute Ranking:\n",
      "                                      Feature | Relative Importance\n",
      "                                          Sex :   0.4912\n",
      "                                  Cabin_Class :   0.1242\n",
      "                                 Cabin_Number :   0.0664\n",
      "                              Parent_Children :   0.0599\n",
      "                                          Age :   0.0599\n",
      "                                Ticket_Number :   0.0414\n",
      "                                         Fare :   0.0379\n",
      "                                  PassengerId :   0.0332\n",
      "                               Sibling_Spouse :   0.0298\n",
      "                                         Name :   0.0288\n",
      "                          Port_of_Embarkation :   0.0273\n",
      "         \n",
      "\n",
      "\n",
      "\n",
      "End Time:           08/03/2021, 15:55 PDT\n",
      "Runtime Duration:   11s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install brainome \n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade brainome\n",
    "!brainome data/titanic_train.csv -y -o predictor_104.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d5739",
   "metadata": {},
   "source": [
    "## 1. Validation\n",
    "The predictor can take an identically formatted to the training data set and compare it's outcomes with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781a9407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Type:                    Decision Tree\r\n",
      "System Type:                        2-way classifier\r\n",
      "\r\n",
      "Accuracy:\r\n",
      "    Best-guess accuracy:            61.25%\r\n",
      "    Model accuracy:                 81.25% (65/80 correct)\r\n",
      "    Improvement over best guess:    20.00% (of possible 38.75%)\r\n",
      "\r\n",
      "Model capacity (MEC):               1 bits\r\n",
      "Generalization ratio:               62.61 bits/bit\r\n",
      "\r\n",
      "Confusion Matrix:\r\n",
      "\r\n",
      "      Actual |   Predicted    \r\n",
      "    ----------------------------\r\n",
      "        died |      45        4\r\n",
      "    survived |      11       20\r\n",
      "\r\n",
      "Accuracy by Class:\r\n",
      "\r\n",
      "      target | TP FP TN FN     TPR     TNR     PPV     NPV      F1      TS\r\n",
      "    -------- | -- -- -- -- ------- ------- ------- ------- ------- -------\r\n",
      "        died | 45 11 20  4  91.84%  64.52%  80.36%  83.33%  85.71%  75.00%\r\n",
      "    survived | 20  4 45 11  64.52%  91.84%  83.33%  80.36%  72.73%  57.14%\r\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} predictor.py -validate data/titanic_validate.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05028412",
   "metadata": {},
   "source": [
    "## 2. Classify\n",
    "The predictor can classify a similar data set but without the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478ea137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "\u001b[01;1mBrainome Table Compiler v1.006-14-prod\u001b[0m\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:                 y Demo User  (Evaluation)\n",
      "Expiration Date:             2021-12-12   131 days left\n",
      "Maximum File Size:           100 MB\n",
      "Maximum Instances:           20000\n",
      "Maximum Attributes:          100\n",
      "Maximum Classes:             unlimited\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\u001b[01;1mCommand:\u001b[0m\n",
      "    btc data/titanic_train.csv -f RF -y -o RF_predictor.py\n",
      "\n",
      "Start Time:                 08/03/2021, 15:49 PDT\n",
      "\n",
      "Cleaning...done. \n",
      "Splitting into training and validation...done. \n",
      "Pre-training measurements...done. \n",
      "\n",
      "\n",
      "\u001b[01;1mPre-training Measurements\u001b[0m\n",
      "Data:\n",
      "    Input:                      data/titanic_train.csv\n",
      "    Target Column:              Survived\n",
      "    Number of instances:        800\n",
      "    Number of attributes:        11 out of 11\n",
      "    Number of classes:            2\n",
      "\n",
      "Class Balance:                \n",
      "                            died: 61.50%\n",
      "                        survived: 38.50%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          61.50%\n",
      "    Data Sufficiency:             Maybe enough data to generalize. [yellow]\n",
      "\n",
      "Capacity Progression:             at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Ideal Machine Learner:              6,   7,   8,   8,   9,   9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expected Generalization:\n",
      "    Decision Tree:                 2.02 bits/bit\n",
      "    Neural Network:                6.52 bits/bit\n",
      "    Random Forest:                10.13 bits/bit\n",
      "\n",
      "Expected Accuracy:              Training            Validation\n",
      "    Decision Tree:               100.00%                52.50%\n",
      "    Neural Network:                 ----                  ----\n",
      "    Random Forest:               100.00%                80.25%\n",
      "\n",
      "Recommendations:\n",
      "    Warning: Data has high information density. Using effort 5 and larger ( -e 5 ) can improve results.\n",
      "    We recommend using Random Forest -f RF.\n",
      "    If predictor accuracy is insufficient, try using the option -rank to automatically select the important attributes.\n",
      "    If predictor accuracy is insufficient, try using the effort option -e with a value of 5 or more to increase training time.\n",
      "    Model type RF given by user. \n",
      "\n",
      "\n",
      "Building classifier...done. \n",
      "Compiling predictor...done. \n",
      "Validating predictor...done. \n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        RF_predictor.py\n",
      "    Classifier Type:              Random Forest\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          86.84% (416/479 correct)\n",
      "      Validation Accuracy:        80.99% (260/321 correct)\n",
      "      Combined Model Accuracy:    84.50% (676/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         41    bits\n",
      "\n",
      "    Generalization Ratio:          9.74 bits/bit\n",
      "    Percent of Data Memorized:    20.84%\n",
      "    Resilience to Noise:          -1.01 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  279   16 \n",
      "            survived |   47  137 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  175   22 \n",
      "            survived |   39   85 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  279   47  137   16   94.58%   74.46%   85.58%   89.54%   89.86%   81.58%\n",
      "            survived |  137   16  279   47   74.46%   94.58%   89.54%   85.58%   81.31%   68.50%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  175   39   85   22   88.83%   68.55%   81.78%   79.44%   85.16%   74.15%\n",
      "            survived |   85   22  175   39   68.55%   88.83%   79.44%   81.78%   73.59%   58.22%\n",
      "\n",
      "\n",
      "    Attribute Ranking:\n",
      "                                      Feature | Relative Importance\n",
      "                                          Sex :   0.4912\n",
      "                                  Cabin_Class :   0.1242\n",
      "                                 Cabin_Number :   0.0664\n",
      "                              Parent_Children :   0.0599\n",
      "                                          Age :   0.0599\n",
      "                                Ticket_Number :   0.0414\n",
      "                                         Fare :   0.0379\n",
      "                                  PassengerId :   0.0332\n",
      "                               Sibling_Spouse :   0.0298\n",
      "                                         Name :   0.0288\n",
      "                          Port_of_Embarkation :   0.0273\n",
      "         \n",
      "\n",
      "\n",
      "\n",
      "End Time:           08/03/2021, 15:49 PDT\n",
      "Runtime Duration:   10s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!brainome data/titanic_train.csv -f RF -y -o RF_predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a2bf9",
   "metadata": {},
   "source": [
    "View Random Forest Predictor Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c857ef2e",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "#\n",
      "# This code has been produced by a free evaluation version of Brainome(tm).\n",
      "# Portions of this code copyright (c) 2019-2021 by Brainome, Inc. All Rights Reserved.\n",
      "# Brainome, Inc grants an exclusive (subject to our continuing rights to use and modify models),\n",
      "# worldwide, non-sublicensable, and non-transferable limited license to use and modify this\n",
      "# predictor produced through the input of your data:\n",
      "# (i) for users accessing the service through a free evaluation account, solely for your\n",
      "# own non-commercial purposes, including for the purpose of evaluating this service, and\n",
      "# (ii) for users accessing the service through a paid, commercial use account, for your\n",
      "# own internal  and commercial purposes.\n",
      "# Please contact support@brainome.ai with any questions.\n",
      "# Use of predictions results at your own risk.\n",
      "#\n",
      "# Output of Brainome v1.006-14-prod.\n",
      "# Invocation: brainome data/titanic_train.csv -f RF -y -o RF_predictor.py\n",
      "# Total compiler execution time: 0:00:09.50. Finished on: Aug-03-2021 15:49:35.\n",
      "# This source code requires Python 3.\n",
      "#\n",
      "\"\"\"\n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        RF_predictor.py\n",
      "    Classifier Type:              Random Forest\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          86.84% (416/479 correct)\n",
      "      Validation Accuracy:        80.99% (260/321 correct)\n",
      "      Combined Model Accuracy:    84.50% (676/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         41    bits\n",
      "\n",
      "    Generalization Ratio:          9.74 bits/bit\n",
      "    Percent of Data Memorized:    20.84%\n",
      "    Resilience to Noise:          -1.01 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  279   16 \n",
      "            survived |   47  137 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  175   22 \n",
      "            survived |   39   85 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  279   47  137   16   94.58%   74.46%   85.58%   89.54%   89.86%   81.58%\n",
      "            survived |  137   16  279   47   74.46%   94.58%   89.54%   85.58%   81.31%   68.50%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  175   39   85   22   88.83%   68.55%   81.78%   79.44%   85.16%   74.15%\n",
      "            survived |   85   22  175   39   68.55%   88.83%   79.44%   81.78%   73.59%   58.22%\n",
      "\n",
      "\n",
      "    Attribute Ranking:\n",
      "                                      Feature | Relative Importance\n",
      "                                          Sex :   0.4912\n",
      "                                  Cabin_Class :   0.1242\n",
      "                                 Cabin_Number :   0.0664\n",
      "                              Parent_Children :   0.0599\n",
      "                                          Age :   0.0599\n",
      "                                Ticket_Number :   0.0414\n",
      "                                         Fare :   0.0379\n",
      "                                  PassengerId :   0.0332\n",
      "                               Sibling_Spouse :   0.0298\n",
      "                                         Name :   0.0288\n",
      "                          Port_of_Embarkation :   0.0273\n",
      "         \n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "import math\n",
      "import os\n",
      "import argparse\n",
      "import tempfile\n",
      "import csv\n",
      "import binascii\n",
      "import faulthandler\n",
      "import json\n",
      "from io import StringIO\n",
      "try:\n",
      "    import numpy as np # For numpy see: http://numpy.org\n",
      "    from numpy import array\n",
      "except:\n",
      "    print(\"This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.\")\n",
      "    sys.exit(1)\n",
      "try:\n",
      "    from scipy.sparse import coo_matrix\n",
      "    report_cmat = True\n",
      "except:\n",
      "    print(\"Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.\")\n",
      "    report_cmat = False\n",
      "try:\n",
      "    import multiprocessing\n",
      "    var_dict = {}\n",
      "    default_to_serial = False\n",
      "except:\n",
      "    default_to_serial = True\n",
      "\n",
      "IOBUF = 100000000\n",
      "sys.setrecursionlimit(1000000)\n",
      "TRAINFILE = ['data/titanic_train.csv']\n",
      "mapping = {'died': 0, 'survived': 1}\n",
      "ignorelabels = []\n",
      "ignorecolumns = []\n",
      "target = '' \n",
      "target_column = 11\n",
      "important_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "ignore_idxs = []\n",
      "classifier_type = 'RF'\n",
      "num_attr = 11\n",
      "n_classes = 2\n",
      "model_cap = 41\n",
      "logits_dict = {0: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.36425662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.243536353, 0.191349998, 0.0098128207, 0.331673324, -0.0, -0.255133331, 0.0883153826, 0.298732072, -0.287025005, 0.0765400007, 0.185177416, 0.0153079992]), 1: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.36425662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.243536353, -0.191349998, -0.0098128207, -0.331673324, -0.0, 0.255133331, -0.0883153826, -0.298732072, 0.287025005, -0.0765400007, -0.185177416, -0.0153079992]), 2: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.265477031, 0.0, 0.270277321, 0.0, 0.0, 0.0, 0.0, -0.150360435, 0.00850328244, 0.185843408, -0.0649351925, -0.116907045, 0.0839446634, -0.0411008634, 0.232498676, 0.0817588419, 0.235827729]), 3: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.265477002, 0.0, -0.270277321, 0.0, 0.0, 0.0, 0.0, 0.15036045, -0.00850327779, -0.185843378, 0.0649352148, 0.116907068, -0.0839446336, 0.041100882, -0.232498676, -0.0817588121, -0.235827684]), 4: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.254825503, 0.0, 0.0, -0.123233855, -0.235735834, 0.039184168, -0.188743472, -0.153517619, 0.112402298, 0.196248844, -0.115611948, -0.191678584, 0.120147459, 0.160565287, 0.0143993665]), 5: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.254825503, 0.0, 0.0, 0.123233832, 0.235735834, -0.039184168, 0.188743442, 0.153517663, -0.112402275, -0.196248844, 0.11561197, 0.191678569, -0.120147429, -0.160565287, -0.0143993739]), 6: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.214141831, 0.0, 0.0, 0.0, 0.222953677, 0.0, 0.0, 0.0810240507, -0.186694652, 0.0149209229, -0.165880695, 0.0454831272, 0.233664706, 0.164316222, -0.0880754292, 0.105792671, -0.078477487, 0.0124567663, 0.118342534]), 7: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.214141831, 0.0, 0.0, 0.0, -0.222953692, 0.0, 0.0, -0.0810240358, 0.186694652, -0.0149209267, 0.16588068, -0.0454831496, -0.233664706, -0.164316207, 0.0880754441, -0.105792664, 0.078477487, -0.0124568064, -0.118342534])}\n",
      "right_children_dict = {0: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 1: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 2: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, -1, 17, 19, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 3: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, -1, 17, 19, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 4: array([1, 3, 5, 7, 9, 11, 13, -1, 15, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 5: array([1, 3, 5, 7, 9, 11, 13, -1, 15, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 6: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 7: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])}\n",
      "split_feats_dict = {0: array([3, 1, 9, 7, 4, 7, 7, 0, 0, 7, 8, 0, 7, 9, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 1: array([3, 1, 9, 7, 4, 7, 7, 0, 0, 7, 8, 0, 7, 9, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 2: array([3, 1, 7, 7, 4, 8, 1, 0, 0, 8, 0, 6, 7, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 3: array([3, 1, 7, 7, 4, 8, 1, 0, 0, 8, 0, 6, 7, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 4: array([3, 1, 8, 7, 4, 7, 4, 0, 8, 7, 8, 0, 0, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 5: array([3, 1, 8, 7, 4, 7, 4, 0, 8, 7, 8, 0, 0, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 6: array([3, 1, 8, 0, 8, 7, 7, 8, 0, 7, 2, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 7: array([3, 1, 8, 0, 8, 7, 7, 8, 0, 7, 2, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "split_vals_dict = {0: array([1342256510.0, 2.5, 99463000.0, 11753.5, 6.5, 2621.5, 17458.0, 0.0, 0.0, 1855777020.0, 17.5999985, 340.0, 2666.0, 1777625860.0, 113785.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 1: array([1342256510.0, 2.5, 99463000.0, 11753.5, 6.5, 2621.5, 17458.0, 0.0, 0.0, 1855777020.0, 17.5999985, 340.0, 2666.0, 1777625860.0, 113785.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 2: array([1342256510.0, 2.5, 314014.0, 11753.5, 36.5, 26.2749996, 1.5, 0.0, 0.0, 8.03960037, 0.0, 0.5, 28396.0, 2686373380.0, 299527200.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 3: array([1342256510.0, 2.5, 314014.0, 11753.5, 36.5, 26.2749996, 1.5, 0.0, 0.0, 8.03960037, 0.0, 0.5, 28396.0, 2686373380.0, 299527200.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 4: array([1342256510.0, 2.5, 7.76249981, 11753.5, 6.5, 2666.0, 13.0, 0.0, 13.25, 6683.5, 7.69999981, 543.0, 0.0, 314014.0, 669928000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 5: array([1342256510.0, 2.5, 7.76249981, 11753.5, 6.5, 2666.0, 13.0, 0.0, 13.25, 6683.5, 7.69999981, 543.0, 0.0, 314014.0, 669928000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 6: array([1342256510.0, 2.5, 7.76249981, 208.0, 24.8083496, 2666.0, 27627.5, 22.0, 0.0, 366226.0, 2345232900.0, 543.0, 0.0, 14.8520498, 11.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 7: array([1342256510.0, 2.5, 7.76249981, 208.0, 24.8083496, 2666.0, 27627.5, 22.0, 0.0, 366226.0, 2345232900.0, 543.0, 0.0, 14.8520498, 11.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])}\n",
      "\n",
      "\n",
      "def __convert(cell):\n",
      "    value = str(cell)\n",
      "    try:\n",
      "        result = int(value)\n",
      "        return result\n",
      "    except ValueError:\n",
      "        try:\n",
      "            result=float(value)\n",
      "            if math.isnan(result):\n",
      "                print('NaN value found. Aborting.')\n",
      "                sys.exit(1)\n",
      "            return result\n",
      "        except ValueError:\n",
      "            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def __get_key(val, dictionary):\n",
      "    if dictionary == {}:\n",
      "        return val\n",
      "    for key, value in dictionary.items(): \n",
      "        if val == value:\n",
      "            return key\n",
      "    if val not in dictionary.values:\n",
      "        print(\"Label key does not exist\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __convertclassid(cell, classlist=[]):\n",
      "\n",
      "    value = str(cell)\n",
      "    \n",
      "    if value == '':\n",
      "        print('Empty value encountered for a class label. Aborting.')\n",
      "        sys.exit(1)\n",
      "    \n",
      "    if mapping != {}:\n",
      "        result = -1\n",
      "        try:\n",
      "            result = mapping[cell]\n",
      "        except KeyError:\n",
      "            print(f\"The class label {value} does not exist in the class mapping. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "        return result\n",
      "    \n",
      "    try:\n",
      "        result = float(cell)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "    except:\n",
      "        result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "        if result in classlist:\n",
      "            result = classlist.index(result)\n",
      "        else:\n",
      "            classlist.append(str(result))\n",
      "            result = classlist.index(result)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "    finally:\n",
      "        if result < 0:\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to non-negative integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "    return result\n",
      "\n",
      "\n",
      "def __clean(filename, outfile, headerless=False, testfile=False, trim=False):\n",
      "    classlist = []\n",
      "    outbuf = []\n",
      "    remove_bad_chars = lambda x: x.replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    \n",
      "    with open(filename, encoding='utf-8') as csv_file, open(outfile, \"w+\", encoding='utf-8') as f:\n",
      "        \n",
      "        reader = csv.reader(csv_file)\n",
      "        if not headerless:\n",
      "            next(reader, None)\n",
      "        \n",
      "        for i, row in enumerate(reader):\n",
      "\n",
      "            if row == []:\n",
      "                continue\n",
      "\n",
      "            \n",
      "            expected_row_length = len(important_idxs)\n",
      "            if not trim:\n",
      "                expected_row_length += len(ignorecolumns)\n",
      "            if not testfile:\n",
      "                expected_row_length += 1\n",
      "            actual_row_length = len(row)\n",
      "\n",
      "            if testfile and actual_row_length == expected_row_length + 1:\n",
      "                error_str = f\"We found {actual_row_length} columns but expected {expected_row_length} columns at row {i}. \"\n",
      "                error_str += f\"Please check that the CSV contains no target column otherwise use -validate. Aborting.\"\n",
      "                print(error_str)\n",
      "                sys.exit(1)\n",
      "            \n",
      "            if actual_row_length != expected_row_length:\n",
      "                print(f\"We found {actual_row_length} columns but expected {expected_row_length} columns.\")\n",
      "                sys.exit(1)            \n",
      "\n",
      "            if testfile:\n",
      "                if len(row) == 1:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(row[0])))]\n",
      "                else:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convert(remove_bad_chars(row[-1])))]         \n",
      "            else:\n",
      "                converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convertclassid(row[-1], classlist))]\n",
      "            outbuf.extend(converted_row)\n",
      "\n",
      "            if len(outbuf) < IOBUF:\n",
      "                outbuf.append(os.linesep)\n",
      "            else:\n",
      "                print(''.join(outbuf), file=f)\n",
      "                outbuf = []\n",
      "        \n",
      "        print(''.join(outbuf), end=\"\", file=f)\n",
      "\n",
      "    n_classes_found = len(classlist)\n",
      "    if not testfile and n_classes_found < 2:\n",
      "        print(f\"Only {n_classes_found} classes were found. Aborting.\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):\n",
      "    stats = {}\n",
      "    if labels is None:\n",
      "        labels = np.array(list(set(list(y_true.astype('int')))))\n",
      "    else:\n",
      "        labels = np.asarray(labels)\n",
      "        if np.all([l not in y_true for l in labels]):\n",
      "            raise ValueError(\"At least one label specified must be in y_true\")\n",
      "    n_labels = labels.size\n",
      "\n",
      "    for class_i in range(n_labels):\n",
      "        stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}\n",
      "        class_i_indices = np.argwhere(y_true==class_i)\n",
      "        not_class_i_indices = np.argwhere(y_true!=class_i)\n",
      "        stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices] == class_i))\n",
      "        stats[int(class_i)]['FN'] = int(np.sum(y_pred[class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['FP'] = int(np.sum(y_pred[not_class_i_indices] == class_i))\n",
      "\n",
      "    if not report_cmat:\n",
      "        if json:\n",
      "            return np.array([]), stats\n",
      "        else:\n",
      "            sys.exit(0)\n",
      "\n",
      "    if sample_weight is None:\n",
      "        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n",
      "    else:\n",
      "        sample_weight = np.asarray(sample_weight)\n",
      "    if y_true.shape[0]!=y_pred.shape[0]:\n",
      "        raise ValueError(\"y_true and y_pred must be of the same length\")\n",
      "\n",
      "    if normalize not in ['true', 'pred', 'all', None]:\n",
      "        raise ValueError(\"normalize must be one of {'true', 'pred', 'all', None}\")\n",
      "\n",
      "\n",
      "    label_to_ind = {y: x for x, y in enumerate(labels)}\n",
      "    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\n",
      "    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\n",
      "    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n",
      "    y_pred = y_pred[ind]\n",
      "    y_true = y_true[ind]\n",
      "\n",
      "    sample_weight = sample_weight[ind]\n",
      "    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\n",
      "        dtype = np.int64\n",
      "    else:\n",
      "        dtype = np.float64\n",
      "    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()\n",
      "\n",
      "    with np.errstate(all='ignore'):\n",
      "        if normalize == 'true':\n",
      "            cm = cm / cm.sum(axis=1, keepdims=True)\n",
      "        elif normalize == 'pred':\n",
      "            cm = cm / cm.sum(axis=0, keepdims=True)\n",
      "        elif normalize == 'all':\n",
      "            cm = cm / cm.sum()\n",
      "        cm = np.nan_to_num(cm)\n",
      "    return cm, stats\n",
      "\n",
      "\n",
      "def __predict(arr, headerless, csvfile, trim=False):\n",
      "    with open(csvfile, 'r', encoding='utf-8') as csvinput:\n",
      "        reader = csv.reader(csvinput)\n",
      "        if not headerless:\n",
      "            if trim:\n",
      "                header = ','.join([x for i, x in enumerate(next(reader, None)) if i in important_idxs] + ['Prediction'])\n",
      "            else:\n",
      "                header = ','.join(next(reader, None) + ['Prediction'])\n",
      "            print(header)\n",
      "        outputs = __classify(arr)\n",
      "        for i, row in enumerate(reader):\n",
      "            pred = str(__get_key(int(outputs[i]), mapping))\n",
      "            if trim:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]\n",
      "            else:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for field in row]            \n",
      "            row.append(pred)\n",
      "            print(','.join(row))\n",
      "\n",
      "\n",
      "def __preprocess_and_clean_in_memory(arr):\n",
      "    if not isinstance(arr, list) and not isinstance(arr, np.ndarray):\n",
      "        print(f'The input to \\'predict\\' must be a list or np.ndarray but an input of type {type(arr).__name__} was found.')\n",
      "        sys.exit(1)\n",
      "    clean_arr = np.zeros((len(arr), len(important_idxs)))\n",
      "    for i, row in enumerate(arr):\n",
      "        try:\n",
      "            row_used_cols_only = [row[i] for i in important_idxs]\n",
      "        except IndexError:\n",
      "            error_str = f\"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {num_attr}).\"\n",
      "            if len(arr) == num_attr and len(arr[0]) != num_attr:\n",
      "                error_str += \"\\n\\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' \"\n",
      "                error_str += \"rather than as an element of a list. Make sure that even single instances \"\n",
      "                error_str += \"are enclosed in a list. Example: predict_in_memory(0) is invalid but \"\n",
      "                error_str += \"predict_in_memory([0]) is valid.\"\n",
      "            print(error_str)\n",
      "            sys.exit(1)\n",
      "        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]\n",
      "    return clean_arr\n",
      "\n",
      "\n",
      "def __evaluate_tree(xs, split_vals, split_feats, right_children, logits):\n",
      "    if xs is None:\n",
      "        xs = np.frombuffer(var_dict['X']).reshape(var_dict['X_shape'])\n",
      "\n",
      "    current_node_per_row = np.zeros(xs.shape[0]).astype('int')\n",
      "    values = np.empty(xs.shape[0])\n",
      "    values.fill(np.nan)\n",
      "\n",
      "    while np.isnan(values).any():\n",
      "\n",
      "        row_idxs_at_leaf = np.argwhere(np.logical_and(right_children[current_node_per_row] == -1, np.isnan(values))).reshape(-1)\n",
      "        row_idxs_at_branch = np.argwhere(right_children[current_node_per_row] != -1).reshape(-1)\n",
      "\n",
      "        if row_idxs_at_leaf.shape[0] > 0:\n",
      "\n",
      "            values[row_idxs_at_leaf] = logits[current_node_per_row[row_idxs_at_leaf]].reshape(-1)\n",
      "            current_node_per_row[row_idxs_at_leaf] = -1\n",
      "\n",
      "        if row_idxs_at_branch.shape[0] > 0:\n",
      "\n",
      "            split_values_per_row = split_vals[current_node_per_row[row_idxs_at_branch]].astype('float64')\n",
      "            split_features_per_row = split_feats[current_node_per_row[row_idxs_at_branch]].astype('int')\n",
      "            feature_val_per_row = xs[row_idxs_at_branch, split_features_per_row].reshape(-1)\n",
      "\n",
      "            branch_nodes = current_node_per_row[row_idxs_at_branch]\n",
      "            current_node_per_row[row_idxs_at_branch] = np.where(feature_val_per_row < split_values_per_row,\n",
      "                                                                right_children[branch_nodes].astype('int'),\n",
      "                                                                (right_children[branch_nodes] + 1).astype('int'))\n",
      "\n",
      "    return values\n",
      "\n",
      "\n",
      "def __build_logit_func(n_trees, clss):\n",
      "\n",
      "    def __logit_func(xs, serial, data_shape, pool=None):\n",
      "        if serial:\n",
      "            sum_of_leaf_values = np.zeros(xs.shape[0])\n",
      "            for booster_index in range(clss, n_trees, n_classes):\n",
      "                sum_of_leaf_values += __evaluate_tree(xs, split_vals_dict[booster_index], split_feats_dict[booster_index],\n",
      "                                                right_children_dict[booster_index], logits_dict[booster_index])\n",
      "        else:\n",
      "            sum_of_leaf_values = np.sum(list(pool.starmap(__evaluate_tree,\n",
      "                                            [(None, split_vals_dict[booster_index], split_feats_dict[booster_index],\n",
      "                                              right_children_dict[booster_index], logits_dict[booster_index])\n",
      "                                    for booster_index in range(clss, n_trees, n_classes)])), axis=0)\n",
      "        return sum_of_leaf_values\n",
      "\n",
      "    return __logit_func\n",
      "\n",
      "\n",
      "def __init_worker(X, X_shape):\n",
      "    var_dict['X'] = X\n",
      "    var_dict['X_shape'] = X_shape\n",
      "\n",
      "\n",
      "def __classify(rows, return_probabilities=False, force_serial=False):\n",
      "    if force_serial:\n",
      "        serial = True\n",
      "    else:\n",
      "        serial = default_to_serial\n",
      "    if isinstance(rows, list):\n",
      "        rows = np.array(rows)\n",
      "\n",
      "    logits = [__build_logit_func(8, clss) for clss in range(n_classes)]\n",
      "\n",
      "    if serial:\n",
      "        o = np.array([logits[class_index](rows, True, rows.shape) for class_index in range(n_classes)]).T\n",
      "    else:\n",
      "        shared_arr = multiprocessing.RawArray('d', rows.shape[0] * rows.shape[1])\n",
      "        shared_arr_np = np.frombuffer(shared_arr, dtype=rows.dtype).reshape(rows.shape)\n",
      "        np.copyto(shared_arr_np, rows)\n",
      "\n",
      "        procs = multiprocessing.cpu_count()\n",
      "        pool = multiprocessing.Pool(processes=procs, initializer=__init_worker, initargs=(shared_arr, rows.shape))\n",
      "        o = np.array([logits[class_index](None, False, rows.shape, pool) for class_index in range(n_classes)]).T\n",
      "\n",
      "    if return_probabilities:\n",
      "        \n",
      "        argument = o[:, 0] - o[:, 1]\n",
      "        p0 = 1.0 / (1.0 + np.exp(-argument)).reshape(-1, 1)\n",
      "        p1 = 1.0 - p0\n",
      "        output = np.concatenate((p0, p1), axis=1)\n",
      "        \n",
      "    else:\n",
      "        output = np.argmax(o,axis=1)\n",
      "    return output\n",
      "\n",
      "\n",
      "def __validate_kwargs(kwargs):\n",
      "    for key in kwargs:\n",
      "        if key not in ['return_probabilities', 'force_serial']:\n",
      "        \n",
      "            print(f'{key} is not a keyword argument for Brainome\\'s {classifier_type} predictor. Please see the documentation.')\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def predict(arr, remap=True, **kwargs):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : list[list]\n",
      "        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'.\n",
      "\n",
      "    remap : bool\n",
      "        If True and 'return_probs' is False, remaps the output to the original class\n",
      "        label. If 'return_probs' is True this instead adds a header indicating which\n",
      "        original class label each column of output corresponds to.\n",
      "    \n",
      "    **kwargs :\n",
      "        return_probabilities : bool\n",
      "            If true, return class membership probabilities instead of classifications.\n",
      "        force_serial : bool\n",
      "            If true, model inference is done in serial rather than in parallel. This is\n",
      "            useful if calling \"predict\" repeatedly inside a for-loop.\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    output : np.ndarray\n",
      "        A numpy array of\n",
      "\n",
      "            1. Class predictions if 'return_probabilities' is False.\n",
      "            2. Class probabilities if 'return_probabilities' is True.\n",
      "        \"\"\"\n",
      "    kwargs = kwargs or {}\n",
      "    __validate_kwargs(kwargs)\n",
      "    remove_bad_chars = lambda x: str(x).replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    arr = [[remove_bad_chars(field) for field in row] for row in arr]\n",
      "    arr = __preprocess_and_clean_in_memory(arr)\n",
      "    output = __classify(arr, **kwargs)\n",
      "    if remap:\n",
      "        if len(output.shape) > 1: # probabilities were returned\n",
      "            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)\n",
      "            output = np.concatenate((header, output), axis=0)\n",
      "        else:\n",
      "            output = np.array([__get_key(prediction, mapping) for prediction in output])\n",
      "    return output\n",
      "\n",
      "\n",
      "def validate(cleanarr):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    cleanarr : np.ndarray\n",
      "        An array of float values that has undergone each pre-\n",
      "        prediction step.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    count : int\n",
      "        A count of the number of instances in cleanarr.\n",
      "\n",
      "    correct_count : int\n",
      "        A count of the number of correctly classified instances in\n",
      "        cleanarr.\n",
      "\n",
      "    numeachclass : dict\n",
      "        A dictionary mapping each class to its number of instances.\n",
      "\n",
      "    outputs : np.ndarray\n",
      "        The output of the predictor's '__classify' method on cleanarr.\n",
      "    \"\"\"\n",
      "    outputs = __classify(cleanarr[:, :-1])\n",
      "    count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0\n",
      "    correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))\n",
      "    count = outputs.shape[0]\n",
      "    num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))\n",
      "    num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))\n",
      "    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs\n",
      "    \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))\n",
      "    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')\n",
      "    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')\n",
      "    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')\n",
      "    parser.add_argument('-json', action=\"store_true\", default=False, help=\"report measurements as json\")\n",
      "    parser.add_argument('-trim', action=\"store_true\", help=\"If true, the prediction will not output ignored columns.\")\n",
      "    args = parser.parse_args()\n",
      "    faulthandler.enable()\n",
      "\n",
      "    if args.validate:\n",
      "        args.trim = True\n",
      "\n",
      "    is_testfile = not args.validate\n",
      "    \n",
      "    cleanfile = tempfile.NamedTemporaryFile().name\n",
      "    __clean(args.csvfile, cleanfile, args.headerless, is_testfile, trim=args.trim)\n",
      "    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')\n",
      "    if len(cleanarr.shape) == 1:\n",
      "        if args.trim and len(important_idxs) == 1:\n",
      "            cleanarr = cleanarr.reshape(-1, 1)\n",
      "        elif len(open(cleanfile, 'r').read().splitlines()) == 1:\n",
      "            cleanarr = cleanarr.reshape(1, -1)\n",
      "\n",
      "    if not args.trim and ignorecolumns != []:\n",
      "        cleanarr = cleanarr[:, important_idxs].reshape(-1, len(important_idxs))\n",
      "\n",
      "    if not args.validate:\n",
      "        __predict(cleanarr, args.headerless, args.csvfile, trim=args.trim)\n",
      "    else:\n",
      "        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(cleanarr)\n",
      "        \n",
      "        true_labels = cleanarr[:, -1]\n",
      "        classcounts = np.bincount(cleanarr[:, -1].astype('int32')).reshape(-1)\n",
      "        classbalance = (classcounts[np.argwhere(classcounts > 0)] / cleanarr.shape[0]).reshape(-1).tolist()\n",
      "        best_guess = round(100.0 * np.max(classbalance), 2)\n",
      "        H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))\n",
      "        modelacc = int(float(correct_count * 10000) / count) / 100.0\n",
      "\n",
      "        if args.json:\n",
      "            FN = float(num_FN) * 100.0 / float(count)\n",
      "            FP = float(num_FP) * 100.0 / float(count)\n",
      "            TN = float(num_TN) * 100.0 / float(count)\n",
      "            TP = float(num_TP) * 100.0 / float(count)\n",
      "\n",
      "            if int(num_TP + num_FN) != 0:\n",
      "                TPR = num_TP / (num_TP + num_FN)  # Sensitivity, Recall\n",
      "            if int(num_TN + num_FP) != 0:\n",
      "                TNR = num_TN / (num_TN + num_FP)  # Specificity\n",
      "            if int(num_TP + num_FP) != 0:\n",
      "                PPV = num_TP / (num_TP + num_FP)  # Recall\n",
      "            if int(num_FN + num_TP) != 0:\n",
      "                FNR = num_FN / (num_FN + num_TP)  # Miss rate\n",
      "            if int(2 * num_TP + num_FP + num_FN) != 0:\n",
      "                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN)  # F1 Score\n",
      "            if int(num_TP + num_FN + num_FP) != 0:\n",
      "                TS = num_TP / (num_TP + num_FN + num_FP)  # Critical Success Index\n",
      "            json_dict = {'instance_count': count,\n",
      "                         'classifier_type': classifier_type,\n",
      "                         'classes': n_classes,\n",
      "                         'number_correct': correct_count,\n",
      "                         'accuracy': {\n",
      "                             'best_guess': best_guess,\n",
      "                             'improvement': modelacc - best_guess,\n",
      "                             'model_accuracy': modelacc,\n",
      "                         },\n",
      "                         'false_negative_instances': num_FN,\n",
      "                         'false_positive_instances': num_FP,\n",
      "                         'true_positive_instances': num_TP,\n",
      "                         'true_negative_instances': num_TN,\n",
      "                         'false_negatives': FN,\n",
      "                         'false_positives': FP,\n",
      "                         'true_negatives': TN,\n",
      "                         'true_positives': TP,\n",
      "                         'model_capacity': model_cap,\n",
      "                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,\n",
      "                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,\n",
      "                         'shannon_entropy_of_labels': H,\n",
      "                         'classbalance': classbalance} \n",
      "        else:\n",
      "            print(\"Classifier Type:                    Random Forest\")\n",
      "            print(f\"System Type:                        {n_classes}-way classifier\")\n",
      "            print()\n",
      "            print(\"Accuracy:\")\n",
      "            print(\"    Best-guess accuracy:            {:.2f}%\".format(best_guess))\n",
      "            print(\"    Model accuracy:                 {:.2f}%\".format(modelacc) + \" (\" + str(int(correct_count)) + \"/\" + str(count) + \" correct)\")\n",
      "            print(\"    Improvement over best guess:    {:.2f}%\".format(modelacc - best_guess) + \" (of possible \" + str(round(100 - best_guess, 2)) + \"%)\")\n",
      "            print()\n",
      "            print(\"Model capacity (MEC):               {:.0f} bits\".format(model_cap))\n",
      "            if classifier_type == '\\'NN\\'':\n",
      "                print(\"Model Capacity Utilized:            {:.0f} bits\".format(cap_utilized))  # noqa\n",
      "            print(\"Generalization ratio:               {:.2f}\".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + \" bits/bit\")\n",
      "\n",
      "        mtrx, stats = __confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)\n",
      "\n",
      "        if args.json:\n",
      "            json_dict['confusion_matrix'] = mtrx.tolist()\n",
      "            json_dict['multiclass_stats'] = stats\n",
      "            print(json.dumps(json_dict))\n",
      "        else:\n",
      "            mtrx = mtrx.astype('str')\n",
      "            labels = np.array(list(mapping.keys())).reshape(-1, 1)\n",
      "            mtrx = np.concatenate((labels, mtrx), axis=1).astype('str')\n",
      "            max_TP_len, max_FP_len, max_TN_len, max_FN_len = 0, 0, 0, 0\n",
      "            max_class_name_len = len('target') + 2\n",
      "            for classs in mapping.keys():\n",
      "                max_class_name_len = max(max_class_name_len, len(classs))\n",
      "            for key in stats.keys():\n",
      "                class_stats = stats[key]\n",
      "                max_TP_len, max_FP_len, max_TN_len, max_FN_len = max(max_TP_len, len(str(class_stats['TP']))), max(max_FP_len, len(str(class_stats['FP']))), max(\n",
      "                    max_TN_len, len(str(class_stats['TN']))), max(max_FN_len, len(str(class_stats['FN'])))\n",
      "            print()\n",
      "            print(\"Confusion Matrix:\")\n",
      "            print()\n",
      "            max_len_value = int(np.max(np.vectorize(len)(mtrx)))\n",
      "            max_pred_len = (int(mtrx.shape[1]) - 1) * max_len_value\n",
      "\n",
      "            print(\" \" * 4 + \"{:>{}} |{:^{}}\".format(\"Actual\", max_class_name_len, \"Predicted\", max_pred_len))\n",
      "            print(\" \" * 4 + \"-\" * (max_class_name_len + max_pred_len + mtrx.shape[1] + 1))\n",
      "            for row in mtrx:\n",
      "                print(str(\" \" * 4 + \"{:>{}}\".format(row[0], max_class_name_len)) + \" |\" + \"{:^{}}\".format(\n",
      "                    (' '.join([str('{:>{}}'.format(i, max_len_value)) for i in row[1:]])), max_pred_len))\n",
      "            print()\n",
      "            print(\"Accuracy by Class:\")\n",
      "            print()\n",
      "            print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format('target',\n",
      "                                                                                                              max_class_name_len,\n",
      "                                                                                                              'TP', max_TP_len,\n",
      "                                                                                                              'FP', max_FP_len,\n",
      "                                                                                                              'TN', max_TN_len,\n",
      "                                                                                                              'FN', max_FN_len,\n",
      "                                                                                                              'TPR', 'TNR',\n",
      "                                                                                                              'PPV', 'NPV',\n",
      "                                                                                                              'F1', 'TS'))\n",
      "            print(\" \" * 4 + \"-\" * max_class_name_len + ' | ' + \"-\" * (\n",
      "                max_TP_len) + ' ' + \"-\" * max_FP_len + ' ' + \"-\" * max_TN_len + ' ' + \"-\" * max_FN_len + (' ' + 7 * \"-\") * 6)\n",
      "            for raw_class in mapping.keys():\n",
      "                class_stats = stats[int(mapping[raw_class])]\n",
      "                TPR = class_stats['TP'] / (class_stats['TP'] + class_stats['FN']) if int(\n",
      "                    class_stats['TP'] + class_stats['FN']) != 0 else 0\n",
      "                TNR = class_stats['TN'] / (class_stats['TN'] + class_stats['FP']) if int(\n",
      "                    class_stats['TN'] + class_stats['FP']) != 0 else 0\n",
      "                PPV = class_stats['TP'] / (class_stats['TP'] + class_stats['FP']) if int(\n",
      "                    class_stats['TP'] + class_stats['FP']) != 0 else 0\n",
      "                NPV = class_stats['TN'] / (class_stats['TN'] + class_stats['FN']) if int(\n",
      "                    class_stats['TN'] + class_stats['FN']) != 0 else 0\n",
      "                F1 = 2 * class_stats['TP'] / (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                TS = class_stats['TP'] / (class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format(raw_class,\n",
      "                                                                                                                  max_class_name_len,\n",
      "                                                                                                                  class_stats['TP'],\n",
      "                                                                                                                  max_TP_len,\n",
      "                                                                                                                  class_stats['FP'],\n",
      "                                                                                                                  max_FP_len,\n",
      "                                                                                                                  class_stats['TN'],\n",
      "                                                                                                                  max_TN_len,\n",
      "                                                                                                                  class_stats['FN'],\n",
      "                                                                                                                  max_FN_len,\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TPR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TNR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * PPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * NPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * F1, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TS, 2))))\n",
      "            \n",
      "    os.remove(cleanfile)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('RF_predictor.py', 'r') as data:\n",
    "    print(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da3aed",
   "metadata": {},
   "source": [
    "## 3. Neural Network\n",
    "Force the selection of Neural Network by using the **-f NN** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1d27cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "\u001b[01;1mBrainome Table Compiler v1.006-14-prod\u001b[0m\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:                 y Demo User  (Evaluation)\n",
      "Expiration Date:             2021-12-12   131 days left\n",
      "Maximum File Size:           100 MB\n",
      "Maximum Instances:           20000\n",
      "Maximum Attributes:          100\n",
      "Maximum Classes:             unlimited\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\u001b[01;1mCommand:\u001b[0m\n",
      "    btc data/titanic_train.csv -f NN -y -o NN_predictor.py\n",
      "\n",
      "Start Time:                 08/03/2021, 15:49 PDT\n",
      "\n",
      "Cleaning...done. \n",
      "Splitting into training and validation...done. \n",
      "Pre-training measurements...done. \n",
      "\n",
      "\n",
      "\u001b[01;1mPre-training Measurements\u001b[0m\n",
      "Data:\n",
      "    Input:                      data/titanic_train.csv\n",
      "    Target Column:              Survived\n",
      "    Number of instances:        800\n",
      "    Number of attributes:        11 out of 11\n",
      "    Number of classes:            2\n",
      "\n",
      "Class Balance:                \n",
      "                            died: 61.50%\n",
      "                        survived: 38.50%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          61.50%\n",
      "    Data Sufficiency:             Maybe enough data to generalize. [yellow]\n",
      "\n",
      "Capacity Progression:             at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Ideal Machine Learner:              6,   7,   8,   8,   9,   9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expected Generalization:\n",
      "    Decision Tree:                 2.02 bits/bit\n",
      "    Neural Network:                6.52 bits/bit\n",
      "    Random Forest:                10.13 bits/bit\n",
      "\n",
      "Expected Accuracy:              Training            Validation\n",
      "    Decision Tree:               100.00%                52.50%\n",
      "    Neural Network:                 ----                  ----\n",
      "    Random Forest:               100.00%                80.25%\n",
      "\n",
      "Recommendations:\n",
      "    Warning: Data has high information density. Using effort 5 and larger ( -e 5 ) can improve results.\n",
      "    We recommend using Random Forest -f RF.\n",
      "    If predictor accuracy is insufficient, try using the option -rank to automatically select the important attributes.\n",
      "    If predictor accuracy is insufficient, try using the effort option -e with a value of 5 or more to increase training time.\n",
      "    Model type NN given by user. \n",
      "\n",
      "\n",
      "Architecting model...done. \n",
      "Priming model...done. \n",
      "Compiling predictor...done. \n",
      "Validating predictor...done. \n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        NN_predictor.py\n",
      "    Classifier Type:              Neural Network\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          62.83% (301/479 correct)\n",
      "      Validation Accuracy:        61.68% (198/321 correct)\n",
      "      Combined Model Accuracy:    62.37% (499/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         27    bits\n",
      "\n",
      "    Generalization Ratio:         10.70 bits/bit\n",
      "    Percent of Data Memorized:    18.97%\n",
      "    Resilience to Noise:          -1.05 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  294    1 \n",
      "            survived |  177    7 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  195    2 \n",
      "            survived |  121    3 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  294  177    7    1   99.66%    3.80%   62.42%   87.50%   76.76%   62.29%\n",
      "            survived |    7    1  294  177    3.80%   99.66%   87.50%   62.42%    7.29%    3.78%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  195  121    3    2   98.98%    2.42%   61.71%   60.00%   76.02%   61.32%\n",
      "            survived |    3    2  195  121    2.42%   98.98%   60.00%   61.71%    4.65%    2.38%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "End Time:           08/03/2021, 15:50 PDT\n",
      "Runtime Duration:   33s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!brainome data/titanic_train.csv -f NN -y -o NN_predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e96ba",
   "metadata": {},
   "source": [
    "View Neural Network Predictor Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd74fd4",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "with open('NN_predictor.py', 'r') as data:\n",
    "    print(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c78f6",
   "metadata": {},
   "source": [
    "## 4. Decision Tree\n",
    "Force the selection of Decision Tree by using the **-f DT** parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87de717",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brainome data/titanic_train.csv -f DT -y -o DT_predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b3abe2",
   "metadata": {},
   "source": [
    "View Decision Tree Predictor Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9a6e8",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "with open('DT_predictor.py', 'r') as data:\n",
    "    print(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c337d6",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Check out [Brainome 104_Using_Predictor](./brainome_102_Using_CLI.ipynb)\n",
    "- Check out [Brainome 201 Measurements](./brainome_201_Measurements.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
