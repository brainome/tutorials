{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895a5bb2",
   "metadata": {},
   "source": [
    "![](https://www.brainome.ai/wp-content/uploads/2020/08/brainome_logo.png)\n",
    "# 104 Using Brainome's Predictors\n",
    "The python predictor generated by Brainome has three modes.\n",
    "1. Validate\n",
    "2. Classify\n",
    "3. Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b880e",
   "metadata": {},
   "source": [
    "## Install brainome and generate a predictor\n",
    "The predictor filename is 104_predictor.py\n",
    "The data sets are titanic_train.csv, titanic_validate.csv, and titanic_predict.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57882e55",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: brainome in /opt/conda/lib/python3.9/site-packages (1.6.68)\n",
      "Requirement already satisfied: brainome-linux-python3.9==1.6.* in /opt/conda/lib/python3.9/site-packages (from brainome) (1.6.14)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (1.21.1)\n",
      "Requirement already satisfied: Jinja2>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (3.0.1)\n",
      "Requirement already satisfied: xgboost==1.4.2 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (0.24.2)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (1.9.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from xgboost==1.4.2->brainome-linux-python3.9==1.6.*->brainome) (1.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2>=3.0.0->brainome-linux-python3.9==1.6.*->brainome) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-linux-python3.9==1.6.*->brainome) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-linux-python3.9==1.6.*->brainome) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.4.0->brainome-linux-python3.9==1.6.*->brainome) (3.10.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (2021.5.30)\n",
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "\u001b[01;1mBrainome Table Compiler v1.006-14-prod\u001b[0m\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:                 y Demo User  (Evaluation)\n",
      "Expiration Date:             2021-12-12   129 days left\n",
      "Maximum File Size:           100 MB\n",
      "Maximum Instances:           20000\n",
      "Maximum Attributes:          100\n",
      "Maximum Classes:             unlimited\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\u001b[01;1mCommand:\u001b[0m\n",
      "    btc data/titanic_train.csv -y -o 104_predictor.py\n",
      "\n",
      "Start Time:                 08/05/2021, 20:22 UTC\n",
      "\n",
      "Cleaning...done. \n",
      "Splitting into training and validation...done. \n",
      "Pre-training measurements...done. \n",
      "\n",
      "\n",
      "\u001b[01;1mPre-training Measurements\u001b[0m\n",
      "Data:\n",
      "    Input:                      data/titanic_train.csv\n",
      "    Target Column:              Survived\n",
      "    Number of instances:        800\n",
      "    Number of attributes:        11 out of 11\n",
      "    Number of classes:            2\n",
      "\n",
      "Class Balance:                \n",
      "                            died: 61.50%\n",
      "                        survived: 38.50%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          61.50%\n",
      "    Data Sufficiency:             Maybe enough data to generalize. [yellow]\n",
      "\n",
      "Capacity Progression:             at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Ideal Machine Learner:              6,   7,   8,   8,   9,   9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expected Generalization:\n",
      "    Decision Tree:                 2.02 bits/bit\n",
      "    Neural Network:                6.52 bits/bit\n",
      "    Random Forest:                10.13 bits/bit\n",
      "\n",
      "Expected Accuracy:              Training            Validation\n",
      "    Decision Tree:               100.00%                52.50%\n",
      "    Neural Network:                 ----                  ----\n",
      "    Random Forest:               100.00%                80.25%\n",
      "\n",
      "Recommendations:\n",
      "    Warning: Data has high information density. Using effort 5 and larger ( -e 5 ) can improve results.\n",
      "    We recommend using Random Forest -f RF.\n",
      "    If predictor accuracy is insufficient, try using the option -rank to automatically select the important attributes.\n",
      "    If predictor accuracy is insufficient, try using the effort option -e with a value of 5 or more to increase training time.\n",
      "    Defaulting to RF model. Model can be forced with -f parameter. \n",
      "\n",
      "\n",
      "Building classifier...done. \n",
      "Compiling predictor...done. \n",
      "Validating predictor...done. \n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        104_predictor.py\n",
      "    Classifier Type:              Random Forest\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          86.84% (416/479 correct)\n",
      "      Validation Accuracy:        80.99% (260/321 correct)\n",
      "      Combined Model Accuracy:    84.50% (676/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         41    bits\n",
      "\n",
      "    Generalization Ratio:          9.74 bits/bit\n",
      "    Percent of Data Memorized:    20.84%\n",
      "    Resilience to Noise:          -1.01 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  279   16 \n",
      "            survived |   47  137 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  175   22 \n",
      "            survived |   39   85 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  279   47  137   16   94.58%   74.46%   85.58%   89.54%   89.86%   81.58%\n",
      "            survived |  137   16  279   47   74.46%   94.58%   89.54%   85.58%   81.31%   68.50%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  175   39   85   22   88.83%   68.55%   81.78%   79.44%   85.16%   74.15%\n",
      "            survived |   85   22  175   39   68.55%   88.83%   79.44%   81.78%   73.59%   58.22%\n",
      "\n",
      "\n",
      "    Attribute Ranking:\n",
      "                                      Feature | Relative Importance\n",
      "                                          Sex :   0.4912\n",
      "                                  Cabin_Class :   0.1242\n",
      "                                 Cabin_Number :   0.0664\n",
      "                              Parent_Children :   0.0599\n",
      "                                          Age :   0.0599\n",
      "                                Ticket_Number :   0.0414\n",
      "                                         Fare :   0.0379\n",
      "                                  PassengerId :   0.0332\n",
      "                               Sibling_Spouse :   0.0298\n",
      "                                         Name :   0.0288\n",
      "                          Port_of_Embarkation :   0.0273\n",
      "         \n",
      "\n",
      "\n",
      "\n",
      "End Time:           08/05/2021, 20:22 UTC\n",
      "Runtime Duration:   9s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install brainome \n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade brainome\n",
    "!brainome data/titanic_train.csv -y -o 104_predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d5739",
   "metadata": {},
   "source": [
    "## 1. Validate a test data set\n",
    "The predictor can take an data set identical to the training data set and compare outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781a9407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Type:                    Random Forest\r\n",
      "System Type:                        2-way classifier\r\n",
      "\r\n",
      "Accuracy:\r\n",
      "    Best-guess accuracy:            61.25%\r\n",
      "    Model accuracy:                 81.25% (65/80 correct)\r\n",
      "    Improvement over best guess:    20.00% (of possible 38.75%)\r\n",
      "\r\n",
      "Model capacity (MEC):               41 bits\r\n",
      "Generalization ratio:               1.52 bits/bit\r\n",
      "\r\n",
      "Confusion Matrix:\r\n",
      "\r\n",
      "      Actual |   Predicted    \r\n",
      "    ----------------------------\r\n",
      "        died |      44        5\r\n",
      "    survived |      10       21\r\n",
      "\r\n",
      "Accuracy by Class:\r\n",
      "\r\n",
      "      target | TP FP TN FN     TPR     TNR     PPV     NPV      F1      TS\r\n",
      "    -------- | -- -- -- -- ------- ------- ------- ------- ------- -------\r\n",
      "        died | 44 10 21  5  89.80%  67.74%  81.48%  80.77%  85.44%  74.58%\r\n",
      "    survived | 21  5 44 10  67.74%  89.80%  80.77%  81.48%  73.68%  58.33%\r\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} 104_predictor.py -validate data/titanic_validate.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05028412",
   "metadata": {},
   "source": [
    "## 2. Classify\n",
    "The predictor can classify a similar data set but without the target column.\n",
    "It is stored into 104_classifications.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478ea137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/work/jupyter_notebooks/104_predictor.py\", line 557, in <module>\r\n",
      "    __clean(args.csvfile, cleanfile, args.headerless, is_testfile, trim=args.trim)\r\n",
      "  File \"/work/jupyter_notebooks/104_predictor.py\", line 213, in __clean\r\n",
      "    with open(filename, encoding='utf-8') as csv_file, open(outfile, \"w+\", encoding='utf-8') as f:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/titanic_predict.csv'\r\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} 104_predictor.py data/titanic_predict.csv > 104_classifications.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a2bf9",
   "metadata": {},
   "source": [
    "### View 104_classifications.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c857ef2e",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '104_classifications.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_954/3467204662.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifications_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'104_classifications.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclassifications_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '104_classifications.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "classifications_data = pd.read_csv('104_classifications.csv')\n",
    "classifications_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da3aed",
   "metadata": {},
   "source": [
    "## 3.Classification Probabilities\n",
    "Brianome can generate prediction probabilities. \n",
    "This example shows how to use the predictor within your own project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1d27cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2953378861.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_954/2953378861.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from 104_predictor import predict\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from 104_predictor import predict\n",
    "predict_data = pd.read_csv('data/titanic_predict.csv')\n",
    "predict_probs = predict(predict_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e96ba",
   "metadata": {},
   "source": [
    "View Neural Network Predictor Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd74fd4",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "with open('NN_predictor.py', 'r') as data:\n",
    "    print(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c78f6",
   "metadata": {},
   "source": [
    "## 4. Decision Tree\n",
    "Force the selection of Decision Tree by using the **-f DT** parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87de717",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brainome data/titanic_train.csv -f DT -y -o DT_predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b3abe2",
   "metadata": {},
   "source": [
    "View Decision Tree Predictor Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9a6e8",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "with open('DT_predictor.py', 'r') as data:\n",
    "    print(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c337d6",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Check out [Brainome 104_Using_Predictor](./brainome_102_Using_CLI.ipynb)\n",
    "- Check out [Brainome 201 Measurements](./brainome_201_Measurements.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
