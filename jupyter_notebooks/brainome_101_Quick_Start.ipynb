{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703b5e61",
   "metadata": {},
   "source": [
    "![](https://www.brainome.ai/wp-content/uploads/2020/08/brainome_logo.png)\n",
    "# 101 Quick Start\n",
    "Running brainome in four easy steps\n",
    "1. Install brainome from scratch\n",
    "2. Download data sets\n",
    "3. Run brainome to build a predictor.py\n",
    "4. Validate the predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5d236",
   "metadata": {},
   "source": [
    "## 1. Install brainome via pip\n",
    "includes dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01365cb1",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: brainome in /usr/local/lib/python3.9/site-packages (1.6.68)\r\n",
      "Collecting pandas\r\n",
      "  Downloading pandas-1.3.1-cp39-cp39-macosx_10_9_x86_64.whl (11.3 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 11.3 MB 7.3 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: jupyter-book in /usr/local/lib/python3.9/site-packages (from -r ./requirements.txt (line 3)) (0.11.2)\r\n",
      "Requirement already satisfied: brainome-mac-python3.9==1.6.* in /usr/local/lib/python3.9/site-packages (from brainome) (1.6.14)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (0.24.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (2.5.4.1)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (1.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (1.20.0)\r\n",
      "Requirement already satisfied: Jinja2>=3.0.0 in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (3.0.1)\r\n",
      "Requirement already satisfied: xgboost==1.4.2 in /usr/local/lib/python3.9/site-packages (from brainome-mac-python3.9==1.6.*->brainome) (1.4.2)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/site-packages (from xgboost==1.4.2->brainome-mac-python3.9==1.6.*->brainome) (1.7.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from Jinja2>=3.0.0->brainome-mac-python3.9==1.6.*->brainome) (2.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-mac-python3.9==1.6.*->brainome) (2.2.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-mac-python3.9==1.6.*->brainome) (1.0.1)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torch>=1.4.0->brainome-mac-python3.9==1.6.*->brainome) (3.10.0.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas->-r ./requirements.txt (line 2)) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas->-r ./requirements.txt (line 2)) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->-r ./requirements.txt (line 2)) (1.16.0)\r\n",
      "Requirement already satisfied: sphinx-togglebutton in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.2.3)\r\n",
      "Requirement already satisfied: linkify-it-py~=1.0.1 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (1.0.1)\r\n",
      "Requirement already satisfied: jupytext<1.11,>=1.8 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (1.10.3)\r\n",
      "Requirement already satisfied: sphinxcontrib-bibtex~=2.2.0 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (2.2.1)\r\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (3.2.0)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (7.1.2)\r\n",
      "Requirement already satisfied: sphinx-multitoc-numbering~=0.1.3 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.1.3)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (5.4.1)\r\n",
      "Requirement already satisfied: sphinx-comments in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.0.3)\r\n",
      "Requirement already satisfied: sphinx-copybutton in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.4.0)\r\n",
      "Requirement already satisfied: sphinx-jupyterbook-latex~=0.4.2 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.4.2)\r\n",
      "Requirement already satisfied: sphinx-panels~=0.5.2 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.5.2)\r\n",
      "Requirement already satisfied: docutils<0.17,>=0.15 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.16)\r\n",
      "Requirement already satisfied: sphinx-external-toc~=0.2.1 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.2.3)\r\n",
      "Requirement already satisfied: myst-nb~=0.12.0 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.12.3)\r\n",
      "Requirement already satisfied: sphinx-thebe~=0.0.8 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.0.8)\r\n",
      "Requirement already satisfied: sphinx-book-theme~=0.1.0 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (0.1.1)\r\n",
      "Requirement already satisfied: sphinx<4,>=2 in /usr/local/lib/python3.9/site-packages (from jupyter-book->-r ./requirements.txt (line 3)) (3.5.4)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/site-packages (from jupytext<1.11,>=1.8->jupyter-book->-r ./requirements.txt (line 3)) (0.10.2)\r\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.9/site-packages (from jupytext<1.11,>=1.8->jupyter-book->-r ./requirements.txt (line 3)) (5.1.3)\r\n",
      "Requirement already satisfied: markdown-it-py~=0.6.0 in /usr/local/lib/python3.9/site-packages (from jupytext<1.11,>=1.8->jupyter-book->-r ./requirements.txt (line 3)) (0.6.2)\r\n",
      "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.9/site-packages (from linkify-it-py~=1.0.1->jupyter-book->-r ./requirements.txt (line 3)) (1.0.1)\r\n",
      "Requirement already satisfied: attrs<21,>=19 in /usr/local/lib/python3.9/site-packages (from markdown-it-py~=0.6.0->jupytext<1.11,>=1.8->jupyter-book->-r ./requirements.txt (line 3)) (20.3.0)\r\n",
      "Requirement already satisfied: mdit-py-plugins~=0.2.1 in /usr/local/lib/python3.9/site-packages (from markdown-it-py~=0.6.0->jupytext<1.11,>=1.8->jupyter-book->-r ./requirements.txt (line 3)) (0.2.6)\r\n",
      "Requirement already satisfied: jupyter-cache~=0.4.1 in /usr/local/lib/python3.9/site-packages (from myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.4.3)\r\n",
      "Requirement already satisfied: ipywidgets<8,>=7.0.0 in /usr/local/lib/python3.9/site-packages (from myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (7.6.3)\r\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/site-packages (from myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (4.6.1)\r\n",
      "Requirement already satisfied: nbconvert~=5.6 in /usr/local/lib/python3.9/site-packages (from myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (5.6.1)\r\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.9/site-packages (from myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (7.25.0)\r\n",
      "Requirement already satisfied: jupyter-sphinx~=0.3.2 in /usr/local/lib/python3.9/site-packages (from myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.3.2)\r\n",
      "Requirement already satisfied: myst-parser~=0.13.5 in /usr/local/lib/python3.9/site-packages (from myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.13.7)\r\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/site-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.0.0)\r\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.9/site-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (3.5.1)\r\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/site-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (6.0.3)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/site-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (5.0.5)\r\n",
      "Requirement already satisfied: appnope in /usr/local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.1.2)\r\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /usr/local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (6.1)\r\n",
      "Requirement already satisfied: jupyter-client<7.0 in /usr/local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (6.1.12)\r\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.1.2)\r\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.4.1)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/site-packages (from ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.2.0)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/site-packages (from ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.18.0)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (5.0.9)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/site-packages (from ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.7.5)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/site-packages (from ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (57.0.0)\r\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.9/site-packages (from ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (2.9.0)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/site-packages (from ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (3.0.19)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/site-packages (from ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (4.8.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/site-packages (from jedi>=0.16->ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.8.2)\r\n",
      "Requirement already satisfied: nbdime in /usr/local/lib/python3.9/site-packages (from jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (3.1.0)\r\n",
      "Requirement already satisfied: nbclient<0.6,>=0.2 in /usr/local/lib/python3.9/site-packages (from jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.5.3)\r\n",
      "Requirement already satisfied: sqlalchemy<1.5,>=1.3.12 in /usr/local/lib/python3.9/site-packages (from jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.4.22)\r\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.9/site-packages (from jupyter-client<7.0->ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (4.7.1)\r\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.9/site-packages (from jupyter-client<7.0->ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (22.1.0)\r\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/site-packages (from nbclient<0.6,>=0.2->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.5.1)\r\n",
      "Requirement already satisfied: async-generator in /usr/local/lib/python3.9/site-packages (from nbclient<0.6,>=0.2->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.10)\r\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/site-packages (from nbconvert~=5.6->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.8.4)\r\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.9/site-packages (from nbconvert~=5.6->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.5.0)\r\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/site-packages (from nbconvert~=5.6->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.3)\r\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.9/site-packages (from nbconvert~=5.6->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (3.3.1)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/site-packages (from nbconvert~=5.6->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.4.3)\r\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/site-packages (from nbconvert~=5.6->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.7.1)\r\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.9/site-packages (from nbformat->jupytext<1.11,>=1.8->jupyter-book->-r ./requirements.txt (line 3)) (0.2.0)\r\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.9/site-packages (from jsonschema->jupyter-book->-r ./requirements.txt (line 3)) (0.18.0)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/site-packages (from pexpect>4.3->ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.2.5)\r\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (0.7.12)\r\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (2.0.0)\r\n",
      "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (2.9.1)\r\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (1.2.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (1.1.5)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (21.0)\r\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (2.1.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (1.0.2)\r\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (1.0.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (1.0.3)\r\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.9/site-packages (from sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (1.0.2)\r\n",
      "Requirement already satisfied: pydata-sphinx-theme~=0.6.0 in /usr/local/lib/python3.9/site-packages (from sphinx-book-theme~=0.1.0->jupyter-book->-r ./requirements.txt (line 3)) (0.6.3)\r\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.6.1 in /usr/local/lib/python3.9/site-packages (from sphinx-book-theme~=0.1.0->jupyter-book->-r ./requirements.txt (line 3)) (4.9.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/site-packages (from beautifulsoup4<5,>=4.6.1->sphinx-book-theme~=0.1.0->jupyter-book->-r ./requirements.txt (line 3)) (2.2.1)\r\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/site-packages (from sphinx-togglebutton->jupyter-book->-r ./requirements.txt (line 3)) (0.36.2)\r\n",
      "Requirement already satisfied: pybtex-docutils>=1.0.0 in /usr/local/lib/python3.9/site-packages (from sphinxcontrib-bibtex~=2.2.0->jupyter-book->-r ./requirements.txt (line 3)) (1.0.1)\r\n",
      "Requirement already satisfied: pybtex>=0.20 in /usr/local/lib/python3.9/site-packages (from sphinxcontrib-bibtex~=2.2.0->jupyter-book->-r ./requirements.txt (line 3)) (0.24.0)\r\n",
      "Requirement already satisfied: latexcodec>=1.0.4 in /usr/local/lib/python3.9/site-packages (from pybtex>=0.20->sphinxcontrib-bibtex~=2.2.0->jupyter-book->-r ./requirements.txt (line 3)) (2.0.1)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/site-packages (from sqlalchemy<1.5,>=1.3.12->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.1.0)\r\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/site-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (6.4.0)\r\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (20.1.0)\r\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.11.0)\r\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /usr/local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.7.1)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.10.1)\r\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.14.6)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7.0.0->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (2.20)\r\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/site-packages (from bleach->nbconvert~=5.6->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.5.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/site-packages (from importlib-metadata->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (3.5.0)\r\n",
      "Requirement already satisfied: jupyter-server-mathjax>=0.2.2 in /usr/local/lib/python3.9/site-packages (from nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.2.3)\r\n",
      "Requirement already satisfied: GitPython!=2.1.4,!=2.1.5,!=2.1.6 in /usr/local/lib/python3.9/site-packages (from nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (3.1.18)\r\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.9/site-packages (from nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.4.4)\r\n",
      "Requirement already satisfied: jupyter-server in /usr/local/lib/python3.9/site-packages (from nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.10.1)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/site-packages (from GitPython!=2.1.4,!=2.1.5,!=2.1.6->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (4.0.7)\r\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=2.1.4,!=2.1.5,!=2.1.6->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (4.0.0)\r\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.9/site-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.1.0)\r\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.9/site-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (3.3.0)\r\n",
      "Requirement already satisfied: requests-unixsocket in /usr/local/lib/python3.9/site-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (0.2.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/site-packages (from anyio<4,>=3.1.0->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (3.2)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/site-packages (from anyio<4,>=3.1.0->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging->sphinx<4,>=2->jupyter-book->-r ./requirements.txt (line 3)) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3>=1.8 in /usr/local/lib/python3.9/site-packages (from requests-unixsocket->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.12.0->jupyter-book->-r ./requirements.txt (line 3)) (1.26.6)\r\n",
      "Installing collected packages: pandas\r\n",
      "Successfully installed pandas-1.3.1\r\n",
      "\u001B[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\r\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# pip install brainome \n",
    "import sys\n",
    "!{sys.executable} -m pip install brainome -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae71992",
   "metadata": {},
   "source": [
    "## 2. Download titanic training and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "446e86f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "response1 = request.urlretrieve('https://download.brainome.ai/data/public/titanic_train.csv', 'data/titanic_train.csv')\n",
    "response2 = request.urlretrieve('https://download.brainome.ai/data/public/titanic_validate.csv', 'data/titanic_validate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a3b84",
   "metadata": {},
   "source": [
    "## Preview training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea0d21b6",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   PassengerId  Cabin_Class  \\\n0            1            3   \n1            2            1   \n2            3            3   \n3            4            1   \n4            5            3   \n\n                                                Name     Sex   Age  \\\n0                            Braund, Mr. Owen Harris    male  22.0   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n2                             Heikkinen, Miss. Laina  female  26.0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n4                           Allen, Mr. William Henry    male  35.0   \n\n   Sibling_Spouse  Parent_Children     Ticket_Number     Fare Cabin_Number  \\\n0               1                0         A/5 21171   7.2500          NaN   \n1               1                0          PC 17599  71.2833          C85   \n2               0                0  STON/O2. 3101282   7.9250          NaN   \n3               1                0            113803  53.1000         C123   \n4               0                0            373450   8.0500          NaN   \n\n  Port_of_Embarkation  Survived  \n0                   S      died  \n1                   C  survived  \n2                   S  survived  \n3                   S  survived  \n4                   S      died  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Cabin_Class</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Sibling_Spouse</th>\n      <th>Parent_Children</th>\n      <th>Ticket_Number</th>\n      <th>Fare</th>\n      <th>Cabin_Number</th>\n      <th>Port_of_Embarkation</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>died</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n      <td>survived</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>survived</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n      <td>survived</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>died</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_data = pd.read_csv('data/titanic_train.csv')\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc2102",
   "metadata": {},
   "source": [
    "## 3. Run brainome to measure and build a predictor.py\n",
    "This command will measure your data set and produce a python predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0dde1a",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\r\n",
      "\r\n",
      "\u001B[01;1mBrainome Table Compiler v1.006-14-prod\u001B[0m\r\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\r\n",
      "Licensed to:                 y Demo User  (Evaluation)\r\n",
      "Expiration Date:             2021-12-12   131 days left\r\n",
      "Maximum File Size:           100 MB\r\n",
      "Maximum Instances:           20000\r\n",
      "Maximum Attributes:          100\r\n",
      "Maximum Classes:             unlimited\r\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\r\n",
      "\r\n",
      "\u001B[01;1mCommand:\u001B[0m\r\n",
      "    btc data/titanic_train.csv -rank -f DT -split 90 --yes -o predictor.py\r\n",
      "\r\n",
      "Start Time:                 08/03/2021, 14:49 PDT\r\n",
      "\r\n",
      "Cleaning...-\bdone. \r\n",
      "Ranking attributes...-\b/\b|\bdone. \r\n",
      "\r\n",
      "\u001B[01;1mAttribute Ranking:\u001B[0m\r\n",
      "    Columns selected:           Sex, Sibling_Spouse, Parent_Children, Cabin_Class\r\n",
      "    Risk of coincidental column correlation:    0.0%\r\n",
      "    \r\n",
      "    Test Accuracy Progression:\r\n",
      "                                          Sex :   78.75%\r\n",
      "                               Sibling_Spouse :   79.62% change   +0.88%\r\n",
      "                              Parent_Children :   80.25% change   +0.62%\r\n",
      "                                  Cabin_Class :   80.88% change   +0.63%\r\n",
      "         \r\n",
      "Splitting into training and validation...-\bdone. \r\n",
      "Pre-training measurements...-\b/\b|\b\\\b-\bdone. \r\n",
      "\r\n",
      "\r\n",
      "\u001B[01;1mPre-training Measurements\u001B[0m\r\n",
      "Data:\r\n",
      "    Input:                      data/titanic_train.csv\r\n",
      "    Target Column:              Survived\r\n",
      "    Number of instances:        800\r\n",
      "    Number of attributes:         4 out of 11\r\n",
      "    Number of classes:            2\r\n",
      "\r\n",
      "Class Balance:                \r\n",
      "                            died: 61.50%\r\n",
      "                        survived: 38.50%\r\n",
      "\r\n",
      "Learnability:\r\n",
      "    Best guess accuracy:          61.50%\r\n",
      "    Data Sufficiency:             Not enough data to generalize. [red]\r\n",
      "\r\n",
      "Capacity Progression:             at [ 5%, 10%, 20%, 40%, 80%, 100% ]\r\n",
      "    Ideal Machine Learner:              4,   5,   7,   7,   7,   8\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Expected Generalization:\r\n",
      "    Decision Tree:               623.05 bits/bit\r\n",
      "    Neural Network:               15.70 bits/bit\r\n",
      "    Random Forest:                40.00 bits/bit\r\n",
      "\r\n",
      "Expected Accuracy:              Training            Validation\r\n",
      "    Decision Tree:                81.00%                80.88%\r\n",
      "    Neural Network:                 ----                  ----\r\n",
      "    Random Forest:                83.75%                79.00%\r\n",
      "\r\n",
      "Recommendations:\r\n",
      "    We recommend using Random Forest -f RF.\r\n",
      "    If predictor accuracy is insufficient, try using the option -rank to automatically select the important attributes.\r\n",
      "    If predictor accuracy is insufficient, try using the effort option -e with a value of 5 or more to increase training time.\r\n",
      "    Model type DT given by user. \r\n",
      "\r\n",
      "Time to Build Estimates:\r\n",
      "    Decision Tree:                a few seconds\r\n",
      "\r\n",
      "\r\n",
      "Building classifier...-\b/\b|\b\\\b-\b/\b|\b\\\b-\b/\b|\b\\\b-\b/\b|\b\\\b-\b/\b|\b\\\b-\b/\b|\b\\\b-\b/\b|\b\\\b-\b/\b|\b\\\b-\b/\b|\b\\\b-\b/\b|\b\\\b-\b/\b|\b\\\b-\b/\b|\b\\\b-\bdone. \r\n",
      "Compiling predictor...-\bdone. \r\n",
      "Validating predictor...-\b/\b|\b\\\b-\b/\b|\bdone. \r\n",
      "\r\n",
      "\u001B[01;1mPredictor:\u001B[0m                        predictor.py\r\n",
      "    Classifier Type:              Decision Tree\r\n",
      "    System Type:                  Binary classifier\r\n",
      "    Training / Validation Split:  90% : 10%\r\n",
      "    Accuracy:\r\n",
      "      Best-guess accuracy:        61.50%\r\n",
      "      Training accuracy:          80.80% (581/719 correct)\r\n",
      "      Validation Accuracy:        82.71% (67/81 correct)\r\n",
      "      Combined Model Accuracy:    81.00% (648/800 correct)\r\n",
      "\r\n",
      "    Model Capacity (MEC):          1    bits\r\n",
      "\r\n",
      "    Generalization Ratio:        558.73 bits/bit\r\n",
      "    Percent of Data Memorized:     0.36%\r\n",
      "    Resilience to Noise:          -2.76 dB\r\n",
      "\r\n",
      "\r\n",
      "    Training Confusion Matrix:\r\n",
      "              Actual | Predicted\r\n",
      "              ------ | ---------\r\n",
      "                died |  397   45 \r\n",
      "            survived |   93  184 \r\n",
      "\r\n",
      "    Validation Confusion Matrix:\r\n",
      "              Actual | Predicted\r\n",
      "              ------ | ---------\r\n",
      "                died |   44    6 \r\n",
      "            survived |    8   23 \r\n",
      "\r\n",
      "    Training Accuracy by Class:\r\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \r\n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\r\n",
      "                died |  397   93  184   45   89.82%   66.43%   81.02%   80.35%   85.19%   74.21%\r\n",
      "            survived |  184   45  397   93   66.43%   89.82%   80.35%   81.02%   72.73%   57.14%\r\n",
      "\r\n",
      "    Validation Accuracy by Class:\r\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \r\n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\r\n",
      "                died |   44    8   23    6   88.00%   74.19%   84.62%   79.31%   86.27%   75.86%\r\n",
      "            survived |   23    6   44    8   74.19%   88.00%   79.31%   84.62%   76.67%   62.16%\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "End Time:           08/03/2021, 14:49 PDT\r\n",
      "Runtime Duration:   8s\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!brainome data/titanic_train.csv -rank -f DT -split 90 --yes -o predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a821c333",
   "metadata": {},
   "source": [
    "## View predictor.py source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a00b7660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "#\n",
      "# This code has been produced by a free evaluation version of Brainome(tm).\n",
      "# Portions of this code copyright (c) 2019-2021 by Brainome, Inc. All Rights Reserved.\n",
      "# Brainome, Inc grants an exclusive (subject to our continuing rights to use and modify models),\n",
      "# worldwide, non-sublicensable, and non-transferable limited license to use and modify this\n",
      "# predictor produced through the input of your data:\n",
      "# (i) for users accessing the service through a free evaluation account, solely for your\n",
      "# own non-commercial purposes, including for the purpose of evaluating this service, and\n",
      "# (ii) for users accessing the service through a paid, commercial use account, for your\n",
      "# own internal  and commercial purposes.\n",
      "# Please contact support@brainome.ai with any questions.\n",
      "# Use of predictions results at your own risk.\n",
      "#\n",
      "# Output of Brainome v1.006-14-prod.\n",
      "# Invocation: brainome data/titanic_train.csv -rank -f DT -split 90 --yes -o predictor.py\n",
      "# Total compiler execution time: 0:00:08.87. Finished on: Aug-03-2021 14:49:39.\n",
      "# This source code requires Python 3.\n",
      "#\n",
      "\"\"\"\n",
      "\n",
      "\u001B[01;1mPredictor:\u001B[0m                        predictor.py\n",
      "    Classifier Type:              Decision Tree\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  90% : 10%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          80.80% (581/719 correct)\n",
      "      Validation Accuracy:        82.71% (67/81 correct)\n",
      "      Combined Model Accuracy:    81.00% (648/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):          1    bits\n",
      "\n",
      "    Generalization Ratio:        558.73 bits/bit\n",
      "    Percent of Data Memorized:     0.36%\n",
      "    Resilience to Noise:          -2.76 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  397   45 \n",
      "            survived |   93  184 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |   44    6 \n",
      "            survived |    8   23 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  397   93  184   45   89.82%   66.43%   81.02%   80.35%   85.19%   74.21%\n",
      "            survived |  184   45  397   93   66.43%   89.82%   80.35%   81.02%   72.73%   57.14%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |   44    8   23    6   88.00%   74.19%   84.62%   79.31%   86.27%   75.86%\n",
      "            survived |   23    6   44    8   74.19%   88.00%   79.31%   84.62%   76.67%   62.16%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "import math\n",
      "import os\n",
      "import argparse\n",
      "import tempfile\n",
      "import csv\n",
      "import binascii\n",
      "import faulthandler\n",
      "import json\n",
      "from io import StringIO\n",
      "try:\n",
      "    import numpy as np # For numpy see: http://numpy.org\n",
      "    from numpy import array\n",
      "except:\n",
      "    print(\"This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.\")\n",
      "    sys.exit(1)\n",
      "try:\n",
      "    from scipy.sparse import coo_matrix\n",
      "    report_cmat = True\n",
      "except:\n",
      "    print(\"Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.\")\n",
      "    report_cmat = False\n",
      "\n",
      "IOBUF = 100000000\n",
      "sys.setrecursionlimit(1000000)\n",
      "TRAINFILE = ['data/titanic_train.csv']\n",
      "mapping = {'died': 0, 'survived': 1}\n",
      "ignorelabels = []\n",
      "ignorecolumns = ['PassengerId', 'Name', 'Age', 'Ticket_Number', 'Fare', 'Cabin_Number', 'Port_of_Embarkation']\n",
      "target = '' \n",
      "target_column = 11\n",
      "important_idxs = [1, 3, 5, 6]\n",
      "ignore_idxs = [0, 2, 4, 7, 8, 9, 10]\n",
      "classifier_type = 'DT'\n",
      "num_attr = 11\n",
      "n_classes = 2\n",
      "model_cap = 1\n",
      "energy_thresholds = [1249151602.5]\n",
      "start_label = 0\n",
      "\n",
      "\n",
      "def __convert(cell):\n",
      "    value = str(cell)\n",
      "    try:\n",
      "        result = int(value)\n",
      "        return result\n",
      "    except ValueError:\n",
      "        try:\n",
      "            result=float(value)\n",
      "            if math.isnan(result):\n",
      "                print('NaN value found. Aborting.')\n",
      "                sys.exit(1)\n",
      "            return result\n",
      "        except ValueError:\n",
      "            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def __get_key(val, dictionary):\n",
      "    if dictionary == {}:\n",
      "        return val\n",
      "    for key, value in dictionary.items(): \n",
      "        if val == value:\n",
      "            return key\n",
      "    if val not in dictionary.values:\n",
      "        print(\"Label key does not exist\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __convertclassid(cell, classlist=[]):\n",
      "\n",
      "    value = str(cell)\n",
      "    \n",
      "    if value == '':\n",
      "        print('Empty value encountered for a class label. Aborting.')\n",
      "        sys.exit(1)\n",
      "    \n",
      "    if mapping != {}:\n",
      "        result = -1\n",
      "        try:\n",
      "            result = mapping[cell]\n",
      "        except KeyError:\n",
      "            print(f\"The class label {value} does not exist in the class mapping. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "        return result\n",
      "    \n",
      "    try:\n",
      "        result = float(cell)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "    except:\n",
      "        result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "        if result in classlist:\n",
      "            result = classlist.index(result)\n",
      "        else:\n",
      "            classlist.append(str(result))\n",
      "            result = classlist.index(result)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "    finally:\n",
      "        if result < 0:\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to non-negative integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "    return result\n",
      "\n",
      "\n",
      "def __clean(filename, outfile, headerless=False, testfile=False, trim=False):\n",
      "    classlist = []\n",
      "    outbuf = []\n",
      "    remove_bad_chars = lambda x: x.replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    \n",
      "    with open(filename, encoding='utf-8') as csv_file, open(outfile, \"w+\", encoding='utf-8') as f:\n",
      "        \n",
      "        reader = csv.reader(csv_file)\n",
      "        if not headerless:\n",
      "            next(reader, None)\n",
      "        \n",
      "        for i, row in enumerate(reader):\n",
      "\n",
      "            if row == []:\n",
      "                continue\n",
      "\n",
      "            if trim:\n",
      "                partial_row = [element for i, element in enumerate(row) if i in important_idxs]\n",
      "                if not testfile:\n",
      "                    row = partial_row + [row[-1]]\n",
      "                else:\n",
      "                    row = partial_row\n",
      "            \n",
      "            expected_row_length = len(important_idxs)\n",
      "            if not trim:\n",
      "                expected_row_length += len(ignorecolumns)\n",
      "            if not testfile:\n",
      "                expected_row_length += 1\n",
      "            actual_row_length = len(row)\n",
      "\n",
      "            if testfile and actual_row_length == expected_row_length + 1:\n",
      "                error_str = f\"We found {actual_row_length} columns but expected {expected_row_length} columns at row {i}. \"\n",
      "                error_str += f\"Please check that the CSV contains no target column otherwise use -validate. Aborting.\"\n",
      "                print(error_str)\n",
      "                sys.exit(1)\n",
      "            \n",
      "            if actual_row_length != expected_row_length:\n",
      "                print(f\"We found {actual_row_length} columns but expected {expected_row_length} columns.\")\n",
      "                sys.exit(1)            \n",
      "\n",
      "            if testfile:\n",
      "                if len(row) == 1:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(row[0])))]\n",
      "                else:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convert(remove_bad_chars(row[-1])))]         \n",
      "            else:\n",
      "                converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convertclassid(row[-1], classlist))]\n",
      "            outbuf.extend(converted_row)\n",
      "\n",
      "            if len(outbuf) < IOBUF:\n",
      "                outbuf.append(os.linesep)\n",
      "            else:\n",
      "                print(''.join(outbuf), file=f)\n",
      "                outbuf = []\n",
      "        \n",
      "        print(''.join(outbuf), end=\"\", file=f)\n",
      "\n",
      "    n_classes_found = len(classlist)\n",
      "    if not testfile and n_classes_found < 2:\n",
      "        print(f\"Only {n_classes_found} classes were found. Aborting.\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):\n",
      "    stats = {}\n",
      "    if labels is None:\n",
      "        labels = np.array(list(set(list(y_true.astype('int')))))\n",
      "    else:\n",
      "        labels = np.asarray(labels)\n",
      "        if np.all([l not in y_true for l in labels]):\n",
      "            raise ValueError(\"At least one label specified must be in y_true\")\n",
      "    n_labels = labels.size\n",
      "\n",
      "    for class_i in range(n_labels):\n",
      "        stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}\n",
      "        class_i_indices = np.argwhere(y_true==class_i)\n",
      "        not_class_i_indices = np.argwhere(y_true!=class_i)\n",
      "        stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices] == class_i))\n",
      "        stats[int(class_i)]['FN'] = int(np.sum(y_pred[class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['FP'] = int(np.sum(y_pred[not_class_i_indices] == class_i))\n",
      "\n",
      "    if not report_cmat:\n",
      "        if json:\n",
      "            return np.array([]), stats\n",
      "        else:\n",
      "            sys.exit(0)\n",
      "\n",
      "    if sample_weight is None:\n",
      "        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n",
      "    else:\n",
      "        sample_weight = np.asarray(sample_weight)\n",
      "    if y_true.shape[0]!=y_pred.shape[0]:\n",
      "        raise ValueError(\"y_true and y_pred must be of the same length\")\n",
      "\n",
      "    if normalize not in ['true', 'pred', 'all', None]:\n",
      "        raise ValueError(\"normalize must be one of {'true', 'pred', 'all', None}\")\n",
      "\n",
      "\n",
      "    label_to_ind = {y: x for x, y in enumerate(labels)}\n",
      "    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\n",
      "    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\n",
      "    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n",
      "    y_pred = y_pred[ind]\n",
      "    y_true = y_true[ind]\n",
      "\n",
      "    sample_weight = sample_weight[ind]\n",
      "    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\n",
      "        dtype = np.int64\n",
      "    else:\n",
      "        dtype = np.float64\n",
      "    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()\n",
      "\n",
      "    with np.errstate(all='ignore'):\n",
      "        if normalize == 'true':\n",
      "            cm = cm / cm.sum(axis=1, keepdims=True)\n",
      "        elif normalize == 'pred':\n",
      "            cm = cm / cm.sum(axis=0, keepdims=True)\n",
      "        elif normalize == 'all':\n",
      "            cm = cm / cm.sum()\n",
      "        cm = np.nan_to_num(cm)\n",
      "    return cm, stats\n",
      "\n",
      "\n",
      "def __predict(arr, headerless, csvfile, trim=False):\n",
      "    with open(csvfile, 'r', encoding='utf-8') as csvinput:\n",
      "        reader = csv.reader(csvinput)\n",
      "        if not headerless:\n",
      "            if trim:\n",
      "                header = ','.join([x for i, x in enumerate(next(reader, None)) if i in important_idxs] + ['Prediction'])\n",
      "            else:\n",
      "                header = ','.join(next(reader, None) + ['Prediction'])\n",
      "            print(header)\n",
      "        outputs = __classify(arr)\n",
      "        for i, row in enumerate(reader):\n",
      "            pred = str(__get_key(int(outputs[i]), mapping))\n",
      "            if trim:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]\n",
      "            else:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for field in row]            \n",
      "            row.append(pred)\n",
      "            print(','.join(row))\n",
      "\n",
      "\n",
      "def __preprocess_and_clean_in_memory(arr):\n",
      "    if not isinstance(arr, list) and not isinstance(arr, np.ndarray):\n",
      "        print(f'The input to \\'predict\\' must be a list or np.ndarray but an input of type {type(arr).__name__} was found.')\n",
      "        sys.exit(1)\n",
      "    clean_arr = np.zeros((len(arr), len(important_idxs)))\n",
      "    for i, row in enumerate(arr):\n",
      "        try:\n",
      "            row_used_cols_only = [row[i] for i in important_idxs]\n",
      "        except IndexError:\n",
      "            error_str = f\"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {num_attr}).\"\n",
      "            if len(arr) == num_attr and len(arr[0]) != num_attr:\n",
      "                error_str += \"\\n\\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' \"\n",
      "                error_str += \"rather than as an element of a list. Make sure that even single instances \"\n",
      "                error_str += \"are enclosed in a list. Example: predict_in_memory(0) is invalid but \"\n",
      "                error_str += \"predict_in_memory([0]) is valid.\"\n",
      "            print(error_str)\n",
      "            sys.exit(1)\n",
      "        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]\n",
      "    return clean_arr\n",
      "\n",
      "\n",
      "def __classify(rows):\n",
      "    try:\n",
      "        energys = np.sum(rows, axis=1, dtype=np.float128)\n",
      "    except:\n",
      "        energys = np.sum(rows, axis=1, dtype=np.longdouble)\n",
      "    numers = np.searchsorted(energy_thresholds, energys, side='left') - 1\n",
      "    indys = np.argwhere(np.logical_and(numers <= len(energy_thresholds), numers >= 0)).reshape(-1)\n",
      "    defaultindys = np.argwhere(np.logical_not(np.logical_and(numers <= len(energy_thresholds), numers >= 0))).reshape(-1)\n",
      "    output = np.zeros(energys.shape[0])\n",
      "    output[indys] = (numers[indys] + 0) % 2\n",
      "    if list(defaultindys):\n",
      "        output[defaultindys] = 1\n",
      "    return output\n",
      "\n",
      "\n",
      "def __validate_kwargs(kwargs):\n",
      "    for key in kwargs:\n",
      "        if key not in []: \n",
      "            print(f'{key} is not a keyword argument for Brainome\\'s {classifier_type} predictor. Please see the documentation.')\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def predict(arr, remap=True, **kwargs):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : list[list]\n",
      "        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'.\n",
      "\n",
      "    remap : bool\n",
      "        If True, remaps the output to the original class label.\n",
      "    \n",
      "    **kwargs :\n",
      "        None\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    output : np.ndarray\n",
      "        A numpy array of predictions.\n",
      "    \"\"\"\n",
      "    kwargs = kwargs or {}\n",
      "    __validate_kwargs(kwargs)\n",
      "    remove_bad_chars = lambda x: str(x).replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    arr = [[remove_bad_chars(field) for field in row] for row in arr]\n",
      "    arr = __preprocess_and_clean_in_memory(arr)\n",
      "    output = __classify(arr, **kwargs)\n",
      "    if remap:\n",
      "        if len(output.shape) > 1: # probabilities were returned\n",
      "            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)\n",
      "            output = np.concatenate((header, output), axis=0)\n",
      "        else:\n",
      "            output = np.array([__get_key(prediction, mapping) for prediction in output])\n",
      "    return output\n",
      "\n",
      "\n",
      "def validate(cleanarr):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    cleanarr : np.ndarray\n",
      "        An array of float values that has undergone each pre-\n",
      "        prediction step.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    count : int\n",
      "        A count of the number of instances in cleanarr.\n",
      "\n",
      "    correct_count : int\n",
      "        A count of the number of correctly classified instances in\n",
      "        cleanarr.\n",
      "\n",
      "    numeachclass : dict\n",
      "        A dictionary mapping each class to its number of instances.\n",
      "\n",
      "    outputs : np.ndarray\n",
      "        The output of the predictor's '__classify' method on cleanarr.\n",
      "    \"\"\"\n",
      "    outputs = __classify(cleanarr[:, :-1])\n",
      "    count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0\n",
      "    correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))\n",
      "    count = outputs.shape[0]\n",
      "    num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))\n",
      "    num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))\n",
      "    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs\n",
      "    \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))\n",
      "    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')\n",
      "    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')\n",
      "    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')\n",
      "    parser.add_argument('-json', action=\"store_true\", default=False, help=\"report measurements as json\")\n",
      "    parser.add_argument('-trim', action=\"store_true\", help=\"If true, the prediction will not output ignored columns.\")\n",
      "    args = parser.parse_args()\n",
      "    faulthandler.enable()\n",
      "\n",
      "    if args.validate:\n",
      "        args.trim = True\n",
      "\n",
      "    is_testfile = not args.validate\n",
      "    \n",
      "    cleanfile = tempfile.NamedTemporaryFile().name\n",
      "    __clean(args.csvfile, cleanfile, args.headerless, is_testfile, trim=args.trim)\n",
      "    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')\n",
      "    if len(cleanarr.shape) == 1:\n",
      "        if args.trim and len(important_idxs) == 1:\n",
      "            cleanarr = cleanarr.reshape(-1, 1)\n",
      "        elif len(open(cleanfile, 'r').read().splitlines()) == 1:\n",
      "            cleanarr = cleanarr.reshape(1, -1)\n",
      "\n",
      "    if not args.trim and ignorecolumns != []:\n",
      "        cleanarr = cleanarr[:, important_idxs].reshape(-1, len(important_idxs))\n",
      "\n",
      "    if not args.validate:\n",
      "        __predict(cleanarr, args.headerless, args.csvfile, trim=args.trim)\n",
      "    else:\n",
      "        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(cleanarr)\n",
      "        \n",
      "        true_labels = cleanarr[:, -1]\n",
      "        classcounts = np.bincount(cleanarr[:, -1].astype('int32')).reshape(-1)\n",
      "        classbalance = (classcounts[np.argwhere(classcounts > 0)] / cleanarr.shape[0]).reshape(-1).tolist()\n",
      "        best_guess = round(100.0 * np.max(classbalance), 2)\n",
      "        H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))\n",
      "        modelacc = int(float(correct_count * 10000) / count) / 100.0\n",
      "\n",
      "        if args.json:\n",
      "            FN = float(num_FN) * 100.0 / float(count)\n",
      "            FP = float(num_FP) * 100.0 / float(count)\n",
      "            TN = float(num_TN) * 100.0 / float(count)\n",
      "            TP = float(num_TP) * 100.0 / float(count)\n",
      "\n",
      "            if int(num_TP + num_FN) != 0:\n",
      "                TPR = num_TP / (num_TP + num_FN)  # Sensitivity, Recall\n",
      "            if int(num_TN + num_FP) != 0:\n",
      "                TNR = num_TN / (num_TN + num_FP)  # Specificity\n",
      "            if int(num_TP + num_FP) != 0:\n",
      "                PPV = num_TP / (num_TP + num_FP)  # Recall\n",
      "            if int(num_FN + num_TP) != 0:\n",
      "                FNR = num_FN / (num_FN + num_TP)  # Miss rate\n",
      "            if int(2 * num_TP + num_FP + num_FN) != 0:\n",
      "                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN)  # F1 Score\n",
      "            if int(num_TP + num_FN + num_FP) != 0:\n",
      "                TS = num_TP / (num_TP + num_FN + num_FP)  # Critical Success Index\n",
      "            json_dict = {'instance_count': count,\n",
      "                         'classifier_type': classifier_type,\n",
      "                         'classes': n_classes,\n",
      "                         'number_correct': correct_count,\n",
      "                         'accuracy': {\n",
      "                             'best_guess': best_guess,\n",
      "                             'improvement': modelacc - best_guess,\n",
      "                             'model_accuracy': modelacc,\n",
      "                         },\n",
      "                         'false_negative_instances': num_FN,\n",
      "                         'false_positive_instances': num_FP,\n",
      "                         'true_positive_instances': num_TP,\n",
      "                         'true_negative_instances': num_TN,\n",
      "                         'false_negatives': FN,\n",
      "                         'false_positives': FP,\n",
      "                         'true_negatives': TN,\n",
      "                         'true_positives': TP,\n",
      "                         'model_capacity': model_cap,\n",
      "                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,\n",
      "                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,\n",
      "                         'shannon_entropy_of_labels': H,\n",
      "                         'classbalance': classbalance} \n",
      "        else:\n",
      "            print(\"Classifier Type:                    Decision Tree\")\n",
      "            print(f\"System Type:                        {n_classes}-way classifier\")\n",
      "            print()\n",
      "            print(\"Accuracy:\")\n",
      "            print(\"    Best-guess accuracy:            {:.2f}%\".format(best_guess))\n",
      "            print(\"    Model accuracy:                 {:.2f}%\".format(modelacc) + \" (\" + str(int(correct_count)) + \"/\" + str(count) + \" correct)\")\n",
      "            print(\"    Improvement over best guess:    {:.2f}%\".format(modelacc - best_guess) + \" (of possible \" + str(round(100 - best_guess, 2)) + \"%)\")\n",
      "            print()\n",
      "            print(\"Model capacity (MEC):               {:.0f} bits\".format(model_cap))\n",
      "            if classifier_type == '\\'NN\\'':\n",
      "                print(\"Model Capacity Utilized:            {:.0f} bits\".format(cap_utilized))  # noqa\n",
      "            print(\"Generalization ratio:               {:.2f}\".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + \" bits/bit\")\n",
      "\n",
      "        mtrx, stats = __confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)\n",
      "\n",
      "        if args.json:\n",
      "            json_dict['confusion_matrix'] = mtrx.tolist()\n",
      "            json_dict['multiclass_stats'] = stats\n",
      "            print(json.dumps(json_dict))\n",
      "        else:\n",
      "            mtrx = mtrx.astype('str')\n",
      "            labels = np.array(list(mapping.keys())).reshape(-1, 1)\n",
      "            mtrx = np.concatenate((labels, mtrx), axis=1).astype('str')\n",
      "            max_TP_len, max_FP_len, max_TN_len, max_FN_len = 0, 0, 0, 0\n",
      "            max_class_name_len = len('target') + 2\n",
      "            for classs in mapping.keys():\n",
      "                max_class_name_len = max(max_class_name_len, len(classs))\n",
      "            for key in stats.keys():\n",
      "                class_stats = stats[key]\n",
      "                max_TP_len, max_FP_len, max_TN_len, max_FN_len = max(max_TP_len, len(str(class_stats['TP']))), max(max_FP_len, len(str(class_stats['FP']))), max(\n",
      "                    max_TN_len, len(str(class_stats['TN']))), max(max_FN_len, len(str(class_stats['FN'])))\n",
      "            print()\n",
      "            print(\"Confusion Matrix:\")\n",
      "            print()\n",
      "            max_len_value = int(np.max(np.vectorize(len)(mtrx)))\n",
      "            max_pred_len = (int(mtrx.shape[1]) - 1) * max_len_value\n",
      "\n",
      "            print(\" \" * 4 + \"{:>{}} |{:^{}}\".format(\"Actual\", max_class_name_len, \"Predicted\", max_pred_len))\n",
      "            print(\" \" * 4 + \"-\" * (max_class_name_len + max_pred_len + mtrx.shape[1] + 1))\n",
      "            for row in mtrx:\n",
      "                print(str(\" \" * 4 + \"{:>{}}\".format(row[0], max_class_name_len)) + \" |\" + \"{:^{}}\".format(\n",
      "                    (' '.join([str('{:>{}}'.format(i, max_len_value)) for i in row[1:]])), max_pred_len))\n",
      "            print()\n",
      "            print(\"Accuracy by Class:\")\n",
      "            print()\n",
      "            print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format('target',\n",
      "                                                                                                              max_class_name_len,\n",
      "                                                                                                              'TP', max_TP_len,\n",
      "                                                                                                              'FP', max_FP_len,\n",
      "                                                                                                              'TN', max_TN_len,\n",
      "                                                                                                              'FN', max_FN_len,\n",
      "                                                                                                              'TPR', 'TNR',\n",
      "                                                                                                              'PPV', 'NPV',\n",
      "                                                                                                              'F1', 'TS'))\n",
      "            print(\" \" * 4 + \"-\" * max_class_name_len + ' | ' + \"-\" * (\n",
      "                max_TP_len) + ' ' + \"-\" * max_FP_len + ' ' + \"-\" * max_TN_len + ' ' + \"-\" * max_FN_len + (' ' + 7 * \"-\") * 6)\n",
      "            for raw_class in mapping.keys():\n",
      "                class_stats = stats[int(mapping[raw_class])]\n",
      "                TPR = class_stats['TP'] / (class_stats['TP'] + class_stats['FN']) if int(\n",
      "                    class_stats['TP'] + class_stats['FN']) != 0 else 0\n",
      "                TNR = class_stats['TN'] / (class_stats['TN'] + class_stats['FP']) if int(\n",
      "                    class_stats['TN'] + class_stats['FP']) != 0 else 0\n",
      "                PPV = class_stats['TP'] / (class_stats['TP'] + class_stats['FP']) if int(\n",
      "                    class_stats['TP'] + class_stats['FP']) != 0 else 0\n",
      "                NPV = class_stats['TN'] / (class_stats['TN'] + class_stats['FN']) if int(\n",
      "                    class_stats['TN'] + class_stats['FN']) != 0 else 0\n",
      "                F1 = 2 * class_stats['TP'] / (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                TS = class_stats['TP'] / (class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format(raw_class,\n",
      "                                                                                                                  max_class_name_len,\n",
      "                                                                                                                  class_stats['TP'],\n",
      "                                                                                                                  max_TP_len,\n",
      "                                                                                                                  class_stats['FP'],\n",
      "                                                                                                                  max_FP_len,\n",
      "                                                                                                                  class_stats['TN'],\n",
      "                                                                                                                  max_TN_len,\n",
      "                                                                                                                  class_stats['FN'],\n",
      "                                                                                                                  max_FN_len,\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TPR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TNR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * PPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * NPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * F1, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TS, 2))))\n",
      "            \n",
      "    os.remove(cleanfile)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('predictor.py', 'r') as data:\n",
    "    print(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37fc6e",
   "metadata": {},
   "source": [
    "## 4. Validate predictor\n",
    "Running your predictor on a validation data set demonstrates its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7adb3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Type:                    Decision Tree\r\n",
      "System Type:                        2-way classifier\r\n",
      "\r\n",
      "Accuracy:\r\n",
      "    Best-guess accuracy:            61.25%\r\n",
      "    Model accuracy:                 81.25% (65/80 correct)\r\n",
      "    Improvement over best guess:    20.00% (of possible 38.75%)\r\n",
      "\r\n",
      "Model capacity (MEC):               1 bits\r\n",
      "Generalization ratio:               62.61 bits/bit\r\n",
      "\r\n",
      "Confusion Matrix:\r\n",
      "\r\n",
      "      Actual |   Predicted    \r\n",
      "    ----------------------------\r\n",
      "        died |      45        4\r\n",
      "    survived |      11       20\r\n",
      "\r\n",
      "Accuracy by Class:\r\n",
      "\r\n",
      "      target | TP FP TN FN     TPR     TNR     PPV     NPV      F1      TS\r\n",
      "    -------- | -- -- -- -- ------- ------- ------- ------- ------- -------\r\n",
      "        died | 45 11 20  4  91.84%  64.52%  80.36%  83.33%  85.71%  75.00%\r\n",
      "    survived | 20  4 45 11  64.52%  91.84%  83.33%  80.36%  72.73%  57.14%\r\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} predictor.py -validate titanic_validate.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bc793",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "Check out [Brainome 201 Measurements](./brainome_201_Measurements.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4624b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}