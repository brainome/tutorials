{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703b5e61",
   "metadata": {},
   "source": [
    "![](https://www.brainome.ai/wp-content/uploads/2020/08/brainome_logo.png)\n",
    "# 101 Quick Start\n",
    "Running brainome in four easy steps\n",
    "1. Install brainome from scratch\n",
    "2. Download data sets\n",
    "3. Run brainome to build a predictor.py\n",
    "4. Validate the predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5d236",
   "metadata": {},
   "source": [
    "## 1. Install brainome via pip\n",
    "includes dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01365cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: brainome in /opt/conda/lib/python3.9/site-packages (1.5.61)\n",
      "Requirement already satisfied: brainome-linux-python3.9==1.5.* in /opt/conda/lib/python3.9/site-packages (from brainome) (1.5.7)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.5.*->brainome) (1.21.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.5.*->brainome) (0.24.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.5.*->brainome) (2.26.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.5.*->brainome) (1.9.0)\n",
      "Requirement already satisfied: xgboost==1.4.2 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.5.*->brainome) (1.4.2)\n",
      "Requirement already satisfied: Jinja2>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.5.*->brainome) (3.0.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from xgboost==1.4.2->brainome-linux-python3.9==1.5.*->brainome) (1.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2>=3.0.0->brainome-linux-python3.9==1.5.*->brainome) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-linux-python3.9==1.5.*->brainome) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-linux-python3.9==1.5.*->brainome) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.4.0->brainome-linux-python3.9==1.5.*->brainome) (3.10.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.5.*->brainome) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.5.*->brainome) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.5.*->brainome) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.5.*->brainome) (3.1)\n"
     ]
    }
   ],
   "source": [
    "# pip install brainome \n",
    "import sys\n",
    "!{sys.executable} -m pip install brainome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae71992",
   "metadata": {},
   "source": [
    "## 2. Download titanic training and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "446e86f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-02 22:33:26--  https://download.brainome.ai/data/public/titanic_train.csv\n",
      "Resolving download.brainome.ai (download.brainome.ai)... 143.204.128.57, 143.204.128.2, 143.204.128.92, ...\n",
      "Connecting to download.brainome.ai (download.brainome.ai)|143.204.128.57|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘titanic_train.csv’ not modified on server. Omitting download.\n",
      "\n",
      "--2021-08-02 22:33:27--  https://download.brainome.ai/data/public/titanic_validate.csv\n",
      "Resolving download.brainome.ai (download.brainome.ai)... 143.204.128.57, 143.204.128.2, 143.204.128.92, ...\n",
      "Connecting to download.brainome.ai (download.brainome.ai)|143.204.128.57|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘titanic_validate.csv’ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -N https://download.brainome.ai/data/public/titanic_train.csv\n",
    "!wget -N https://download.brainome.ai/data/public/titanic_validate.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a3b84",
   "metadata": {},
   "source": [
    "## Preview training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea0d21b6",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Cabin_Class</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sibling_Spouse</th>\n",
       "      <th>Parent_Children</th>\n",
       "      <th>Ticket_Number</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin_Number</th>\n",
       "      <th>Port_of_Embarkation</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>survived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>survived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>survived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>died</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Cabin_Class  \\\n",
       "0            1            3   \n",
       "1            2            1   \n",
       "2            3            3   \n",
       "3            4            1   \n",
       "4            5            3   \n",
       "\n",
       "                                                Name     Sex   Age  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
       "4                           Allen, Mr. William Henry    male  35.0   \n",
       "\n",
       "   Sibling_Spouse  Parent_Children     Ticket_Number     Fare Cabin_Number  \\\n",
       "0               1                0         A/5 21171   7.2500          NaN   \n",
       "1               1                0          PC 17599  71.2833          C85   \n",
       "2               0                0  STON/O2. 3101282   7.9250          NaN   \n",
       "3               1                0            113803  53.1000         C123   \n",
       "4               0                0            373450   8.0500          NaN   \n",
       "\n",
       "  Port_of_Embarkation  Survived  \n",
       "0                   S      died  \n",
       "1                   C  survived  \n",
       "2                   S  survived  \n",
       "3                   S  survived  \n",
       "4                   S      died  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_data = pd.read_csv('titanic_train.csv')\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc2102",
   "metadata": {},
   "source": [
    "## 3. Run brainome to measure and build a predictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d0dde1a",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "\u001b[01;1mBrainome Table Compiler v1.005-7-prod\u001b[0m\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:                 y Demo User  (Evaluation)\n",
      "Expiration Date:             2021-12-12   132 days left\n",
      "Maximum File Size:           100 MB\n",
      "Maximum Instances:           20000\n",
      "Maximum Attributes:          100\n",
      "Maximum Classes:             unlimited\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\u001b[01;1mCommand:\u001b[0m\n",
      "    btc titanic_train.csv -rank --yes -o predictor.py\n",
      "\n",
      "Start Time:                 08/02/2021, 22:34 UTC\n",
      "\n",
      "Cleaning...done. \n",
      "Ranking attributes...done. \n",
      "\n",
      "\u001b[01;1mAttribute Ranking:\u001b[0m\n",
      "    Columns selected:           Sex, Sibling_Spouse, Parent_Children, Cabin_Class\n",
      "    Risk of coincidental column correlation:    0.0%\n",
      "    \n",
      "    Test Accuracy Progression:\n",
      "                                          Sex :   78.75%\n",
      "                               Sibling_Spouse :   79.62% change   +0.88%\n",
      "                              Parent_Children :   80.25% change   +0.62%\n",
      "                                  Cabin_Class :   80.88% change   +0.63%\n",
      "         \n",
      "Splitting into training and validation...done. \n",
      "Pre-training measurements...done. \n",
      "\n",
      "\n",
      "\u001b[01;1mPre-training Measurements\u001b[0m\n",
      "Data:\n",
      "    Input:                      titanic_train.csv\n",
      "    Target Column:              Survived\n",
      "    Number of instances:        800\n",
      "    Number of attributes:         4 out of 11\n",
      "    Number of classes:            2\n",
      "\n",
      "Class Balance:                \n",
      "                            died: 61.50%\n",
      "                        survived: 38.50%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          61.50%\n",
      "    Data Sufficiency:             Not enough data to generalize. [red]\n",
      "\n",
      "Capacity Progression:             at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Ideal Machine Learner:              4,   5,   7,   7,   7,   8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expected Generalization:\n",
      "    Decision Tree:               623.05 bits/bit\n",
      "    Neural Network:               15.70 bits/bit\n",
      "    Random Forest:                40.00 bits/bit\n",
      "\n",
      "Expected Accuracy:              Training            Validation\n",
      "    Decision Tree:                81.00%                80.88%\n",
      "    Neural Network:                 ----                  ----\n",
      "    Random Forest:                83.75%                79.00%\n",
      "\n",
      "Recommendations:\n",
      "    We recommend using Random Forest -f RF.\n",
      "    If predictor accuracy is insufficient, try using the option -rank to automatically select the important attributes.\n",
      "    If predictor accuracy is insufficient, try using the effort option -e with a value of 5 or more to increase training time.\n",
      "    Defaulting to RF model. Model can be forced with -f parameter. \n",
      "\n",
      "\n",
      "Building classifier...done. \n",
      "Compiling predictor...done. \n",
      "Validating predictor...done. \n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        predictor.py\n",
      "    Classifier Type:              Random Forest\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  50% : 50%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          82.00% (328/400 correct)\n",
      "      Validation Accuracy:        79.50% (318/400 correct)\n",
      "      Combined Model Accuracy:    80.75% (646/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         17    bits\n",
      "\n",
      "    Generalization Ratio:         18.55 bits/bit\n",
      "    Percent of Data Memorized:    10.95%\n",
      "    Resilience to Noise:          -1.29 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  221   25 \n",
      "            survived |   47  107 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  217   29 \n",
      "            survived |   53  101 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  221   47  107   25   89.84%   69.48%   82.46%   81.06%   85.99%   75.43%\n",
      "            survived |  107   25  221   47   69.48%   89.84%   81.06%   82.46%   74.83%   59.78%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  217   53  101   29   88.21%   65.58%   80.37%   77.69%   84.11%   72.58%\n",
      "            survived |  101   29  217   53   65.58%   88.21%   77.69%   80.37%   71.13%   55.19%\n",
      "\n",
      "\n",
      "    Attribute Ranking:\n",
      "                                      Feature | Relative Importance\n",
      "                                          Sex :   0.6698\n",
      "                                  Cabin_Class :   0.2146\n",
      "                              Parent_Children :   0.0622\n",
      "                               Sibling_Spouse :   0.0533\n",
      "         \n",
      "\n",
      "\n",
      "\n",
      "End Time:           08/02/2021, 22:35 UTC\n",
      "Runtime Duration:   10s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!brainome titanic_train.csv -rank -f DT -split 90 --yes -o predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a821c333",
   "metadata": {},
   "source": [
    "## View predictor.py source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a00b7660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "#\n",
      "# This code has been produced by a free evaluation version of Brainome(tm).\n",
      "# Portions of this code copyright (c) 2019-2021 by Brainome, Inc. All Rights Reserved.\n",
      "# Brainome, Inc grants an exclusive (subject to our continuing rights to use and modify models),\n",
      "# worldwide, non-sublicensable, and non-transferable limited license to use and modify this\n",
      "# predictor produced through the input of your data:\n",
      "# (i) for users accessing the service through a free evaluation account, solely for your\n",
      "# own non-commercial purposes, including for the purpose of evaluating this service, and\n",
      "# (ii) for users accessing the service through a paid, commercial use account, for your\n",
      "# own internal  and commercial purposes.\n",
      "# Please contact support@brainome.ai with any questions.\n",
      "# Use of predictions results at your own risk.\n",
      "#\n",
      "# Output of Brainome v1.005-7-prod.\n",
      "# Invocation: brainome titanic_train.csv -rank --yes -o predictor.py\n",
      "# Total compiler execution time: 0:00:10.55. Finished on: Aug-02-2021 22:35:06.\n",
      "# This source code requires Python 3.\n",
      "#\n",
      "\"\"\"\n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        predictor.py\n",
      "    Classifier Type:              Random Forest\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  50% : 50%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          82.00% (328/400 correct)\n",
      "      Validation Accuracy:        79.50% (318/400 correct)\n",
      "      Combined Model Accuracy:    80.75% (646/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         17    bits\n",
      "\n",
      "    Generalization Ratio:         18.55 bits/bit\n",
      "    Percent of Data Memorized:    10.95%\n",
      "    Resilience to Noise:          -1.29 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  221   25 \n",
      "            survived |   47  107 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  217   29 \n",
      "            survived |   53  101 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  221   47  107   25   89.84%   69.48%   82.46%   81.06%   85.99%   75.43%\n",
      "            survived |  107   25  221   47   69.48%   89.84%   81.06%   82.46%   74.83%   59.78%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  217   53  101   29   88.21%   65.58%   80.37%   77.69%   84.11%   72.58%\n",
      "            survived |  101   29  217   53   65.58%   88.21%   77.69%   80.37%   71.13%   55.19%\n",
      "\n",
      "\n",
      "    Attribute Ranking:\n",
      "                                      Feature | Relative Importance\n",
      "                                          Sex :   0.6698\n",
      "                                  Cabin_Class :   0.2146\n",
      "                              Parent_Children :   0.0622\n",
      "                               Sibling_Spouse :   0.0533\n",
      "         \n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "import math\n",
      "import os\n",
      "import argparse\n",
      "import tempfile\n",
      "import csv\n",
      "import binascii\n",
      "import faulthandler\n",
      "import json\n",
      "from io import StringIO\n",
      "try:\n",
      "    import numpy as np # For numpy see: http://numpy.org\n",
      "    from numpy import array\n",
      "except:\n",
      "    print(\"This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.\")\n",
      "    sys.exit(1)\n",
      "try:\n",
      "    from scipy.sparse import coo_matrix\n",
      "    report_cmat = True\n",
      "except:\n",
      "    print(\"Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.\")\n",
      "    report_cmat = False\n",
      "try:\n",
      "    import multiprocessing\n",
      "    var_dict = {}\n",
      "    default_to_serial = False\n",
      "except:\n",
      "    default_to_serial = True\n",
      "\n",
      "IOBUF = 100000000\n",
      "sys.setrecursionlimit(1000000)\n",
      "TRAINFILE = ['titanic_train.csv']\n",
      "mapping = {'died': 0, 'survived': 1}\n",
      "ignorelabels = []\n",
      "ignorecolumns = ['PassengerId', 'Name', 'Age', 'Ticket_Number', 'Fare', 'Cabin_Number', 'Port_of_Embarkation']\n",
      "target = '' \n",
      "target_column = 11\n",
      "important_idxs = [1, 3, 5, 6]\n",
      "ignore_idxs = [0, 2, 4, 7, 8, 9, 10]\n",
      "classifier_type = 'RF'\n",
      "num_attr = 11\n",
      "n_classes = 2\n",
      "model_cap = 17\n",
      "logits_dict = {0: array([0.0, 0.0, 0.0, -0.815487981, -0.0624225363, 0.306210905, 0.629472435]), 1: array([0.0, 0.0, 0.0, 0.815487981, 0.0624225363, -0.306210905, -0.629472435]), 2: array([0.0, 0.0, 0.0, -0.267614931, 0.26338464, 0.237916529, -0.0341751762]), 3: array([0.0, 0.0, 0.0, 0.26761499, -0.263384581, -0.237916559, 0.0341751203]), 4: array([0.0, 0.0, 0.0, -0.460301042, -0.0762317702, 0.0469896123, 0.415659159]), 5: array([0.0, 0.0, 0.0, 0.460301042, 0.0762316659, -0.0469896272, -0.415659159]), 6: array([0.0, 0.0, 0.510881126, 0.0372499265, -0.195344761]), 7: array([0.0, 0.0, -0.510881126, -0.0372499339, 0.195344731]), 8: array([0.0, 0.0, 0.0, 0.00147240213, -0.301541865, 0.0310076736, 0.456388742]), 9: array([0.0, 0.0, 0.0, -0.00147240481, 0.301541835, -0.0310076941, -0.456388682]), 10: array([0.0, 0.0, 0.334737897, -0.088131465, 0.0232687984]), 11: array([0.0, 0.0, -0.334737867, 0.0881314501, -0.0232686996]), 12: array([0.0, 0.0, 0.0, 0.0231478736, -0.1034908, -0.00216557761, 0.343483001]), 13: array([0.0, 0.0, 0.0, -0.0231478512, 0.10349077, 0.00216553523, -0.343483031]), 14: array([0.0, 0.0, 0.255434483, -0.0519496016, 0.0236502942]), 15: array([0.0, 0.0, -0.255434453, 0.0519496426, -0.0236502588]), 16: array([0.0, 0.0, 0.0, -0.00728081493, 0.119237758, 0.0343075022, 0.253715575]), 17: array([0.0, 0.0, 0.0, 0.00728078187, -0.119237795, -0.0343075246, -0.253715575]), 18: array([0.0, 0.0, 0.0, -0.280211717, 0.0478284508, 0.0903734937, -0.0372801647]), 19: array([0.0, 0.0, 0.0, 0.280211687, -0.047828462, -0.0903734714, 0.0372803174]), 20: array([0.0, 0.0, 0.0, -0.149634793, 0.381767213, 0.0382172801, -0.0855898932]), 21: array([0.0, 0.0, 0.0, 0.149634749, -0.381767184, -0.0382173434, 0.0855899155]), 22: array([0.0, 0.0, 0.126601383, -0.028810069, 0.0144990087]), 23: array([0.0, 0.0, -0.126601398, 0.0288101044, -0.014499086]), 24: array([0.0, 0.0, 0.0, 0.00739858346, -0.0395297147, 0.1461505, -0.00215910468]), 25: array([0.0, 0.0, 0.0, -0.00739857182, 0.0395297371, -0.14615047, 0.00215913751])}\n",
      "right_children_dict = {0: array([1, 3, 5, -1, -1, -1, -1]), 1: array([1, 3, 5, -1, -1, -1, -1]), 2: array([1, 3, 5, -1, -1, -1, -1]), 3: array([1, 3, 5, -1, -1, -1, -1]), 4: array([1, 3, 5, -1, -1, -1, -1]), 5: array([1, 3, 5, -1, -1, -1, -1]), 6: array([1, 3, -1, -1, -1]), 7: array([1, 3, -1, -1, -1]), 8: array([1, 3, 5, -1, -1, -1, -1]), 9: array([1, 3, 5, -1, -1, -1, -1]), 10: array([1, 3, -1, -1, -1]), 11: array([1, 3, -1, -1, -1]), 12: array([1, 3, 5, -1, -1, -1, -1]), 13: array([1, 3, 5, -1, -1, -1, -1]), 14: array([1, 3, -1, -1, -1]), 15: array([1, 3, -1, -1, -1]), 16: array([1, 3, 5, -1, -1, -1, -1]), 17: array([1, 3, 5, -1, -1, -1, -1]), 18: array([1, 3, 5, -1, -1, -1, -1]), 19: array([1, 3, 5, -1, -1, -1, -1]), 20: array([1, 3, 5, -1, -1, -1, -1]), 21: array([1, 3, 5, -1, -1, -1, -1]), 22: array([1, 3, -1, -1, -1]), 23: array([1, 3, -1, -1, -1]), 24: array([1, 3, 5, -1, -1, -1, -1]), 25: array([1, 3, 5, -1, -1, -1, -1])}\n",
      "split_feats_dict = {0: array([1, 0, 0, 0, 0, 0, 0]), 1: array([1, 0, 0, 0, 0, 0, 0]), 2: array([1, 3, 2, 0, 0, 0, 0]), 3: array([1, 3, 2, 0, 0, 0, 0]), 4: array([0, 1, 2, 0, 0, 0, 0]), 5: array([0, 1, 2, 0, 0, 0, 0]), 6: array([3, 3, 0, 0, 0]), 7: array([3, 3, 0, 0, 0]), 8: array([2, 2, 1, 0, 0, 0, 0]), 9: array([2, 2, 1, 0, 0, 0, 0]), 10: array([3, 0, 0, 0, 0]), 11: array([3, 0, 0, 0, 0]), 12: array([2, 3, 1, 0, 0, 0, 0]), 13: array([2, 3, 1, 0, 0, 0, 0]), 14: array([3, 1, 0, 0, 0]), 15: array([3, 1, 0, 0, 0]), 16: array([2, 3, 1, 0, 0, 0, 0]), 17: array([2, 3, 1, 0, 0, 0, 0]), 18: array([0, 1, 1, 0, 0, 0, 0]), 19: array([0, 1, 1, 0, 0, 0, 0]), 20: array([0, 3, 3, 0, 0, 0, 0]), 21: array([0, 3, 3, 0, 0, 0, 0]), 22: array([3, 1, 0, 0, 0]), 23: array([3, 1, 0, 0, 0]), 24: array([2, 3, 3, 0, 0, 0, 0]), 25: array([2, 3, 3, 0, 0, 0, 0])}\n",
      "split_vals_dict = {0: array([1342256510.0, 2.5, 1.5, 0.0, 0.0, 0.0, 0.0]), 1: array([1342256510.0, 2.5, 1.5, 0.0, 0.0, 0.0, 0.0]), 2: array([1342256510.0, 1.5, 0.5, 0.0, 0.0, 0.0, 0.0]), 3: array([1342256510.0, 1.5, 0.5, 0.0, 0.0, 0.0, 0.0]), 4: array([1.5, 1342256510.0, 2.5, 0.0, 0.0, 0.0, 0.0]), 5: array([1.5, 1342256510.0, 2.5, 0.0, 0.0, 0.0, 0.0]), 6: array([3.5, 0.5, 0.0, 0.0, 0.0]), 7: array([3.5, 0.5, 0.0, 0.0, 0.0]), 8: array([2.5, 1.5, 1342256510.0, 0.0, 0.0, 0.0, 0.0]), 9: array([2.5, 1.5, 1342256510.0, 0.0, 0.0, 0.0, 0.0]), 10: array([3.5, 1.5, 0.0, 0.0, 0.0]), 11: array([3.5, 1.5, 0.0, 0.0, 0.0]), 12: array([2.5, 0.5, 1342256510.0, 0.0, 0.0, 0.0, 0.0]), 13: array([2.5, 0.5, 1342256510.0, 0.0, 0.0, 0.0, 0.0]), 14: array([2.5, 1342256510.0, 0.0, 0.0, 0.0]), 15: array([2.5, 1342256510.0, 0.0, 0.0, 0.0]), 16: array([2.5, 2.5, 1342256510.0, 0.0, 0.0, 0.0, 0.0]), 17: array([2.5, 2.5, 1342256510.0, 0.0, 0.0, 0.0, 0.0]), 18: array([2.5, 1342256510.0, 1342256510.0, 0.0, 0.0, 0.0, 0.0]), 19: array([2.5, 1342256510.0, 1342256510.0, 0.0, 0.0, 0.0, 0.0]), 20: array([1.5, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0]), 21: array([1.5, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0]), 22: array([2.5, 1342256510.0, 0.0, 0.0, 0.0]), 23: array([2.5, 1342256510.0, 0.0, 0.0, 0.0]), 24: array([2.5, 0.5, 1.5, 0.0, 0.0, 0.0, 0.0]), 25: array([2.5, 0.5, 1.5, 0.0, 0.0, 0.0, 0.0])}\n",
      "\n",
      "\n",
      "def __convert(cell):\n",
      "    value = str(cell)\n",
      "    try:\n",
      "        result = int(value)\n",
      "        return result\n",
      "    except ValueError:\n",
      "        try:\n",
      "            result=float(value)\n",
      "            if math.isnan(result):\n",
      "                print('NaN value found. Aborting.')\n",
      "                sys.exit(1)\n",
      "            return result\n",
      "        except ValueError:\n",
      "            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def __get_key(val, dictionary):\n",
      "    if dictionary == {}:\n",
      "        return val\n",
      "    for key, value in dictionary.items(): \n",
      "        if val == value:\n",
      "            return key\n",
      "    if val not in dictionary.values:\n",
      "        print(\"Label key does not exist\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __convertclassid(cell, classlist=[]):\n",
      "\n",
      "    value = str(cell)\n",
      "    \n",
      "    if value == '':\n",
      "        print('Empty value encountered for a class label. Aborting.')\n",
      "        sys.exit(1)\n",
      "    \n",
      "    if mapping != {}:\n",
      "        result = -1\n",
      "        try:\n",
      "            result = mapping[cell]\n",
      "        except KeyError:\n",
      "            print(f\"The class label {value} does not exist in the class mapping. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        except Exception as e:\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "        return result\n",
      "    \n",
      "    try:\n",
      "        result = float(cell)\n",
      "        if str(result) not in classlist:\n",
      "            classlist.append(str(result))\n",
      "    except:\n",
      "        result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\n",
      "        if result in classlist:\n",
      "            result = classlist.index(result)\n",
      "        else:\n",
      "            classlist.append(str(result))\n",
      "            result = classlist.index(result)\n",
      "        if result != int(result):\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "    finally:\n",
      "        if result < 0:\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to non-negative integers. Aborting.\")\n",
      "            sys.exit(1)\n",
      "\n",
      "    return result\n",
      "\n",
      "\n",
      "def __clean(filename, outfile, headerless=False, testfile=False, trim=False):\n",
      "    classlist = []\n",
      "    outbuf = []\n",
      "    remove_bad_chars = lambda x: x.replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    \n",
      "    with open(filename, encoding='utf-8') as csv_file, open(outfile, \"w+\", encoding='utf-8') as f:\n",
      "        \n",
      "        reader = csv.reader(csv_file)\n",
      "        if not headerless:\n",
      "            next(reader, None)\n",
      "        \n",
      "        for i, row in enumerate(reader):\n",
      "\n",
      "            if row == []:\n",
      "                continue\n",
      "\n",
      "            if trim:\n",
      "                partial_row = [element for i, element in enumerate(row) if i in important_idxs]\n",
      "                if not testfile:\n",
      "                    row = partial_row + [row[-1]]\n",
      "                else:\n",
      "                    row = partial_row\n",
      "            \n",
      "            expected_row_length = len(important_idxs)\n",
      "            if not trim:\n",
      "                expected_row_length += len(ignorecolumns)\n",
      "            if not testfile:\n",
      "                expected_row_length += 1\n",
      "            actual_row_length = len(row)\n",
      "\n",
      "            if testfile and actual_row_length == expected_row_length + 1:\n",
      "                error_str = f\"We found {actual_row_length} columns but expected {expected_row_length} columns at row {i}. \"\n",
      "                error_str += f\"Please check that the CSV contains no target column otherwise use -validate. Aborting.\"\n",
      "                print(error_str)\n",
      "                sys.exit(1)\n",
      "            \n",
      "            if actual_row_length != expected_row_length:\n",
      "                print(f\"We found {actual_row_length} columns but expected {expected_row_length} columns.\")\n",
      "                sys.exit(1)            \n",
      "\n",
      "            if testfile:\n",
      "                if len(row) == 1:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(row[0])))]\n",
      "                else:\n",
      "                    converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convert(remove_bad_chars(row[-1])))]         \n",
      "            else:\n",
      "                converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convertclassid(row[-1], classlist))]\n",
      "            outbuf.extend(converted_row)\n",
      "\n",
      "            if len(outbuf) < IOBUF:\n",
      "                outbuf.append(os.linesep)\n",
      "            else:\n",
      "                print(''.join(outbuf), file=f)\n",
      "                outbuf = []\n",
      "        \n",
      "        print(''.join(outbuf), end=\"\", file=f)\n",
      "\n",
      "    n_classes_found = len(classlist)\n",
      "    if not testfile and n_classes_found < 2:\n",
      "        print(f\"Only {n_classes_found} classes were found. Aborting.\")\n",
      "        sys.exit(1)\n",
      "\n",
      "\n",
      "def __confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):\n",
      "    stats = {}\n",
      "    if labels is None:\n",
      "        labels = np.array(list(set(list(y_true.astype('int')))))\n",
      "    else:\n",
      "        labels = np.asarray(labels)\n",
      "        if np.all([l not in y_true for l in labels]):\n",
      "            raise ValueError(\"At least one label specified must be in y_true\")\n",
      "    n_labels = labels.size\n",
      "\n",
      "    for class_i in range(n_labels):\n",
      "        stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}\n",
      "        class_i_indices = np.argwhere(y_true==class_i)\n",
      "        not_class_i_indices = np.argwhere(y_true!=class_i)\n",
      "        stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices] == class_i))\n",
      "        stats[int(class_i)]['FN'] = int(np.sum(y_pred[class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices] != class_i))\n",
      "        stats[int(class_i)]['FP'] = int(np.sum(y_pred[not_class_i_indices] == class_i))\n",
      "\n",
      "    if not report_cmat:\n",
      "        if json:\n",
      "            return np.array([]), stats\n",
      "        else:\n",
      "            sys.exit(0)\n",
      "\n",
      "    if sample_weight is None:\n",
      "        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n",
      "    else:\n",
      "        sample_weight = np.asarray(sample_weight)\n",
      "    if y_true.shape[0]!=y_pred.shape[0]:\n",
      "        raise ValueError(\"y_true and y_pred must be of the same length\")\n",
      "\n",
      "    if normalize not in ['true', 'pred', 'all', None]:\n",
      "        raise ValueError(\"normalize must be one of {'true', 'pred', 'all', None}\")\n",
      "\n",
      "\n",
      "    label_to_ind = {y: x for x, y in enumerate(labels)}\n",
      "    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\n",
      "    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\n",
      "    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n",
      "    y_pred = y_pred[ind]\n",
      "    y_true = y_true[ind]\n",
      "\n",
      "    sample_weight = sample_weight[ind]\n",
      "    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\n",
      "        dtype = np.int64\n",
      "    else:\n",
      "        dtype = np.float64\n",
      "    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()\n",
      "\n",
      "    with np.errstate(all='ignore'):\n",
      "        if normalize == 'true':\n",
      "            cm = cm / cm.sum(axis=1, keepdims=True)\n",
      "        elif normalize == 'pred':\n",
      "            cm = cm / cm.sum(axis=0, keepdims=True)\n",
      "        elif normalize == 'all':\n",
      "            cm = cm / cm.sum()\n",
      "        cm = np.nan_to_num(cm)\n",
      "    return cm, stats\n",
      "\n",
      "\n",
      "def __predict(arr, headerless, csvfile, trim=False):\n",
      "    with open(csvfile, 'r', encoding='utf-8') as csvinput:\n",
      "        reader = csv.reader(csvinput)\n",
      "        if not headerless:\n",
      "            if trim:\n",
      "                header = ','.join([x for i, x in enumerate(next(reader, None)) if i in important_idxs] + ['Prediction'])\n",
      "            else:\n",
      "                header = ','.join(next(reader, None) + ['Prediction'])\n",
      "            print(header)\n",
      "        outputs = __classify(arr)\n",
      "        for i, row in enumerate(reader):\n",
      "            pred = str(__get_key(int(outputs[i]), mapping))\n",
      "            if trim:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]\n",
      "            else:\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for field in row]            \n",
      "            row.append(pred)\n",
      "            print(','.join(row))\n",
      "\n",
      "\n",
      "def __preprocess_and_clean_in_memory(arr):\n",
      "    if not isinstance(arr, list) and not isinstance(arr, np.ndarray):\n",
      "        print(f'The input to \\'predict\\' must be a list or np.ndarray but an input of type {type(arr).__name__} was found.')\n",
      "        sys.exit(1)\n",
      "    clean_arr = np.zeros((len(arr), len(important_idxs)))\n",
      "    for i, row in enumerate(arr):\n",
      "        try:\n",
      "            row_used_cols_only = [row[i] for i in important_idxs]\n",
      "        except IndexError:\n",
      "            error_str = f\"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {num_attr}).\"\n",
      "            if len(arr) == num_attr and len(arr[0]) != num_attr:\n",
      "                error_str += \"\\n\\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' \"\n",
      "                error_str += \"rather than as an element of a list. Make sure that even single instances \"\n",
      "                error_str += \"are enclosed in a list. Example: predict_in_memory(0) is invalid but \"\n",
      "                error_str += \"predict_in_memory([0]) is valid.\"\n",
      "            print(error_str)\n",
      "            sys.exit(1)\n",
      "        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]\n",
      "    return clean_arr\n",
      "\n",
      "\n",
      "def __evaluate_tree(xs, split_vals, split_feats, right_children, logits):\n",
      "    if xs is None:\n",
      "        xs = np.frombuffer(var_dict['X']).reshape(var_dict['X_shape'])\n",
      "\n",
      "    current_node_per_row = np.zeros(xs.shape[0]).astype('int')\n",
      "    values = np.empty(xs.shape[0])\n",
      "    values.fill(np.nan)\n",
      "\n",
      "    while np.isnan(values).any():\n",
      "\n",
      "        row_idxs_at_leaf = np.argwhere(np.logical_and(right_children[current_node_per_row] == -1, np.isnan(values))).reshape(-1)\n",
      "        row_idxs_at_branch = np.argwhere(right_children[current_node_per_row] != -1).reshape(-1)\n",
      "\n",
      "        if row_idxs_at_leaf.shape[0] > 0:\n",
      "\n",
      "            values[row_idxs_at_leaf] = logits[current_node_per_row[row_idxs_at_leaf]].reshape(-1)\n",
      "            current_node_per_row[row_idxs_at_leaf] = -1\n",
      "\n",
      "        if row_idxs_at_branch.shape[0] > 0:\n",
      "\n",
      "            split_values_per_row = split_vals[current_node_per_row[row_idxs_at_branch]].astype('float64')\n",
      "            split_features_per_row = split_feats[current_node_per_row[row_idxs_at_branch]].astype('int')\n",
      "            feature_val_per_row = xs[row_idxs_at_branch, split_features_per_row].reshape(-1)\n",
      "\n",
      "            branch_nodes = current_node_per_row[row_idxs_at_branch]\n",
      "            current_node_per_row[row_idxs_at_branch] = np.where(feature_val_per_row < split_values_per_row,\n",
      "                                                                right_children[branch_nodes].astype('int'),\n",
      "                                                                (right_children[branch_nodes] + 1).astype('int'))\n",
      "\n",
      "    return values\n",
      "\n",
      "\n",
      "def __build_logit_func(n_trees, clss):\n",
      "\n",
      "    def __logit_func(xs, serial, data_shape, pool=None):\n",
      "        if serial:\n",
      "            sum_of_leaf_values = np.zeros(xs.shape[0])\n",
      "            for booster_index in range(clss, n_trees, n_classes):\n",
      "                sum_of_leaf_values += __evaluate_tree(xs, split_vals_dict[booster_index], split_feats_dict[booster_index],\n",
      "                                                right_children_dict[booster_index], logits_dict[booster_index])\n",
      "        else:\n",
      "            sum_of_leaf_values = np.sum(list(pool.starmap(__evaluate_tree,\n",
      "                                            [(None, split_vals_dict[booster_index], split_feats_dict[booster_index],\n",
      "                                              right_children_dict[booster_index], logits_dict[booster_index])\n",
      "                                    for booster_index in range(clss, n_trees, n_classes)])), axis=0)\n",
      "        return sum_of_leaf_values\n",
      "\n",
      "    return __logit_func\n",
      "\n",
      "\n",
      "def __init_worker(X, X_shape):\n",
      "    var_dict['X'] = X\n",
      "    var_dict['X_shape'] = X_shape\n",
      "\n",
      "\n",
      "def __classify(rows, return_probabilities=False, force_serial=False):\n",
      "    if force_serial:\n",
      "        serial = True\n",
      "    else:\n",
      "        serial = default_to_serial\n",
      "    if isinstance(rows, list):\n",
      "        rows = np.array(rows)\n",
      "\n",
      "    logits = [__build_logit_func(26, clss) for clss in range(n_classes)]\n",
      "\n",
      "    if serial:\n",
      "        o = np.array([logits[class_index](rows, True, rows.shape) for class_index in range(n_classes)]).T\n",
      "    else:\n",
      "        shared_arr = multiprocessing.RawArray('d', rows.shape[0] * rows.shape[1])\n",
      "        shared_arr_np = np.frombuffer(shared_arr, dtype=rows.dtype).reshape(rows.shape)\n",
      "        np.copyto(shared_arr_np, rows)\n",
      "\n",
      "        procs = multiprocessing.cpu_count()\n",
      "        pool = multiprocessing.Pool(processes=procs, initializer=__init_worker, initargs=(shared_arr, rows.shape))\n",
      "        o = np.array([logits[class_index](None, False, rows.shape, pool) for class_index in range(n_classes)]).T\n",
      "\n",
      "    if return_probabilities:\n",
      "        \n",
      "        argument = o[:, 0] - o[:, 1]\n",
      "        p0 = 1.0 / (1.0 + np.exp(-argument)).reshape(-1, 1)\n",
      "        p1 = 1.0 - p0\n",
      "        output = np.concatenate((p0, p1), axis=1)\n",
      "        \n",
      "    else:\n",
      "        output = np.argmax(o,axis=1)\n",
      "    return output\n",
      "\n",
      "\n",
      "def __validate_kwargs(kwargs):\n",
      "    for key in kwargs:\n",
      "        if key not in ['return_probabilities', 'force_serial']:\n",
      "        \n",
      "            print(f'{key} is not a keyword argument for Brainome\\'s {classifier_type} predictor. Please see the documentation.')\n",
      "            sys.exit(1)\n",
      "\n",
      "\n",
      "def predict(arr, remap=True, **kwargs):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : list[list]\n",
      "        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'.\n",
      "\n",
      "    remap : bool\n",
      "        If True and 'return_probs' is False, remaps the output to the original class\n",
      "        label. If 'return_probs' is True this instead adds a header indicating which\n",
      "        original class label each column of output corresponds to.\n",
      "\n",
      "    **kwargs : --\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    output : np.ndarray\n",
      "        A numpy array of\n",
      "\n",
      "            1. Class predictions if 'return_probabilities' is False.\n",
      "            2. Class probabilities if 'return_probabilities' is True.\n",
      "    \"\"\"\n",
      "    kwargs = kwargs or {}\n",
      "    __validate_kwargs(kwargs)\n",
      "    remove_bad_chars = lambda x: str(x).replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\n",
      "    arr = [[remove_bad_chars(field) for field in row] for row in arr]\n",
      "    arr = __preprocess_and_clean_in_memory(arr)\n",
      "    output = __classify(arr, **kwargs)\n",
      "    if remap:\n",
      "        if len(output.shape) > 1: # probabilities were returned\n",
      "            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)\n",
      "            output = np.concatenate((header, output), axis=0)\n",
      "        else:\n",
      "            output = np.array([__get_key(prediction, mapping) for prediction in output])\n",
      "    return output\n",
      "\n",
      "\n",
      "def validate(cleanarr):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    cleanarr : np.ndarray\n",
      "        An array of float values that has undergone each pre-\n",
      "        prediction step.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    count : int\n",
      "        A count of the number of instances in cleanarr.\n",
      "\n",
      "    correct_count : int\n",
      "        A count of the number of correctly classified instances in\n",
      "        cleanarr.\n",
      "\n",
      "    numeachclass : dict\n",
      "        A dictionary mapping each class to its number of instances.\n",
      "\n",
      "    outputs : np.ndarray\n",
      "        The output of the predictor's '__classify' method on cleanarr.\n",
      "    \"\"\"\n",
      "    outputs = __classify(cleanarr[:, :-1])\n",
      "    count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0\n",
      "    correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))\n",
      "    count = outputs.shape[0]\n",
      "    num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))\n",
      "    num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))\n",
      "    num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))\n",
      "    num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))\n",
      "    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs\n",
      "    \n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))\n",
      "    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')\n",
      "    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')\n",
      "    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')\n",
      "    parser.add_argument('-json', action=\"store_true\", default=False, help=\"report measurements as json\")\n",
      "    parser.add_argument('-trim', action=\"store_true\", help=\"If true, the prediction will not output ignored columns.\")\n",
      "    args = parser.parse_args()\n",
      "    faulthandler.enable()\n",
      "\n",
      "    if args.validate:\n",
      "        args.trim = True\n",
      "\n",
      "    is_testfile = not args.validate\n",
      "    \n",
      "    cleanfile = tempfile.NamedTemporaryFile().name\n",
      "    __clean(args.csvfile, cleanfile, args.headerless, is_testfile, trim=args.trim)\n",
      "    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')\n",
      "    if len(cleanarr.shape) == 1:\n",
      "        if args.trim and len(important_idxs) == 1:\n",
      "            cleanarr = cleanarr.reshape(-1, 1)\n",
      "        elif len(open(cleanfile, 'r').read().splitlines()) == 1:\n",
      "            cleanarr = cleanarr.reshape(1, -1)\n",
      "\n",
      "    if not args.trim and ignorecolumns != []:\n",
      "        cleanarr = cleanarr[:, important_idxs].reshape(-1, len(important_idxs))\n",
      "\n",
      "    if not args.validate:\n",
      "        __predict(cleanarr, args.headerless, args.csvfile, trim=args.trim)\n",
      "    else:\n",
      "        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(cleanarr)\n",
      "        \n",
      "        true_labels = cleanarr[:, -1]\n",
      "        classcounts = np.bincount(cleanarr[:, -1].astype('int32')).reshape(-1)\n",
      "        classbalance = (classcounts[np.argwhere(classcounts > 0)] / cleanarr.shape[0]).reshape(-1).tolist()\n",
      "        best_guess = round(100.0 * np.max(classbalance), 2)\n",
      "        H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))\n",
      "        modelacc = int(float(correct_count * 10000) / count) / 100.0\n",
      "\n",
      "        if args.json:\n",
      "            FN = float(num_FN) * 100.0 / float(count)\n",
      "            FP = float(num_FP) * 100.0 / float(count)\n",
      "            TN = float(num_TN) * 100.0 / float(count)\n",
      "            TP = float(num_TP) * 100.0 / float(count)\n",
      "\n",
      "            if int(num_TP + num_FN) != 0:\n",
      "                TPR = num_TP / (num_TP + num_FN)  # Sensitivity, Recall\n",
      "            if int(num_TN + num_FP) != 0:\n",
      "                TNR = num_TN / (num_TN + num_FP)  # Specificity\n",
      "            if int(num_TP + num_FP) != 0:\n",
      "                PPV = num_TP / (num_TP + num_FP)  # Recall\n",
      "            if int(num_FN + num_TP) != 0:\n",
      "                FNR = num_FN / (num_FN + num_TP)  # Miss rate\n",
      "            if int(2 * num_TP + num_FP + num_FN) != 0:\n",
      "                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN)  # F1 Score\n",
      "            if int(num_TP + num_FN + num_FP) != 0:\n",
      "                TS = num_TP / (num_TP + num_FN + num_FP)  # Critical Success Index\n",
      "            json_dict = {'instance_count': count,\n",
      "                         'classifier_type': classifier_type,\n",
      "                         'classes': n_classes,\n",
      "                         'number_correct': correct_count,\n",
      "                         'accuracy': {\n",
      "                             'best_guess': best_guess,\n",
      "                             'improvement': modelacc - best_guess,\n",
      "                             'model_accuracy': modelacc,\n",
      "                         },\n",
      "                         'false_negative_instances': num_FN,\n",
      "                         'false_positive_instances': num_FP,\n",
      "                         'true_positive_instances': num_TP,\n",
      "                         'true_negative_instances': num_TN,\n",
      "                         'false_negatives': FN,\n",
      "                         'false_positives': FP,\n",
      "                         'true_negatives': TN,\n",
      "                         'true_positives': TP,\n",
      "                         'model_capacity': model_cap,\n",
      "                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,\n",
      "                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,\n",
      "                         'shannon_entropy_of_labels': H,\n",
      "                         'classbalance': classbalance} \n",
      "        else:\n",
      "            print(\"Classifier Type:                    Random Forest\")\n",
      "            print(f\"System Type:                        {n_classes}-way classifier\")\n",
      "            print()\n",
      "            print(\"Accuracy:\")\n",
      "            print(\"    Best-guess accuracy:            {:.2f}%\".format(best_guess))\n",
      "            print(\"    Model accuracy:                 {:.2f}%\".format(modelacc) + \" (\" + str(int(correct_count)) + \"/\" + str(count) + \" correct)\")\n",
      "            print(\"    Improvement over best guess:    {:.2f}%\".format(modelacc - best_guess) + \" (of possible \" + str(round(100 - best_guess, 2)) + \"%)\")\n",
      "            print()\n",
      "            print(\"Model capacity (MEC):               {:.0f} bits\".format(model_cap))\n",
      "            if classifier_type == '\\'NN\\'':\n",
      "                print(\"Model Capacity Utilized:            {:.0f} bits\".format(cap_utilized))  # noqa\n",
      "            print(\"Generalization ratio:               {:.2f}\".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + \" bits/bit\")\n",
      "\n",
      "        mtrx, stats = __confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)\n",
      "\n",
      "        if args.json:\n",
      "            json_dict['confusion_matrix'] = mtrx.tolist()\n",
      "            json_dict['multiclass_stats'] = stats\n",
      "            print(json.dumps(json_dict))\n",
      "        else:\n",
      "            mtrx = mtrx.astype('str')\n",
      "            labels = np.array(list(mapping.keys())).reshape(-1, 1)\n",
      "            mtrx = np.concatenate((labels, mtrx), axis=1).astype('str')\n",
      "            max_TP_len, max_FP_len, max_TN_len, max_FN_len = 0, 0, 0, 0\n",
      "            max_class_name_len = len('target') + 2\n",
      "            for classs in mapping.keys():\n",
      "                max_class_name_len = max(max_class_name_len, len(classs))\n",
      "            for key in stats.keys():\n",
      "                class_stats = stats[key]\n",
      "                max_TP_len, max_FP_len, max_TN_len, max_FN_len = max(max_TP_len, len(str(class_stats['TP']))), max(max_FP_len, len(str(class_stats['FP']))), max(\n",
      "                    max_TN_len, len(str(class_stats['TN']))), max(max_FN_len, len(str(class_stats['FN'])))\n",
      "            print()\n",
      "            print(\"Confusion Matrix:\")\n",
      "            print()\n",
      "            max_len_value = int(np.max(np.vectorize(len)(mtrx)))\n",
      "            max_pred_len = (int(mtrx.shape[1]) - 1) * max_len_value\n",
      "\n",
      "            print(\" \" * 4 + \"{:>{}} |{:^{}}\".format(\"Actual\", max_class_name_len, \"Predicted\", max_pred_len))\n",
      "            print(\" \" * 4 + \"-\" * (max_class_name_len + max_pred_len + mtrx.shape[1] + 1))\n",
      "            for row in mtrx:\n",
      "                print(str(\" \" * 4 + \"{:>{}}\".format(row[0], max_class_name_len)) + \" |\" + \"{:^{}}\".format(\n",
      "                    (' '.join([str('{:>{}}'.format(i, max_len_value)) for i in row[1:]])), max_pred_len))\n",
      "            print()\n",
      "            print(\"Accuracy by Class:\")\n",
      "            print()\n",
      "            print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format('target',\n",
      "                                                                                                              max_class_name_len,\n",
      "                                                                                                              'TP', max_TP_len,\n",
      "                                                                                                              'FP', max_FP_len,\n",
      "                                                                                                              'TN', max_TN_len,\n",
      "                                                                                                              'FN', max_FN_len,\n",
      "                                                                                                              'TPR', 'TNR',\n",
      "                                                                                                              'PPV', 'NPV',\n",
      "                                                                                                              'F1', 'TS'))\n",
      "            print(\" \" * 4 + \"-\" * max_class_name_len + ' | ' + \"-\" * (\n",
      "                max_TP_len) + ' ' + \"-\" * max_FP_len + ' ' + \"-\" * max_TN_len + ' ' + \"-\" * max_FN_len + (' ' + 7 * \"-\") * 6)\n",
      "            for raw_class in mapping.keys():\n",
      "                class_stats = stats[int(mapping[raw_class])]\n",
      "                TPR = class_stats['TP'] / (class_stats['TP'] + class_stats['FN']) if int(\n",
      "                    class_stats['TP'] + class_stats['FN']) != 0 else 0\n",
      "                TNR = class_stats['TN'] / (class_stats['TN'] + class_stats['FP']) if int(\n",
      "                    class_stats['TN'] + class_stats['FP']) != 0 else 0\n",
      "                PPV = class_stats['TP'] / (class_stats['TP'] + class_stats['FP']) if int(\n",
      "                    class_stats['TP'] + class_stats['FP']) != 0 else 0\n",
      "                NPV = class_stats['TN'] / (class_stats['TN'] + class_stats['FN']) if int(\n",
      "                    class_stats['TN'] + class_stats['FN']) != 0 else 0\n",
      "                F1 = 2 * class_stats['TP'] / (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                TS = class_stats['TP'] / (class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\n",
      "                    (class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\n",
      "                print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format(raw_class,\n",
      "                                                                                                                  max_class_name_len,\n",
      "                                                                                                                  class_stats['TP'],\n",
      "                                                                                                                  max_TP_len,\n",
      "                                                                                                                  class_stats['FP'],\n",
      "                                                                                                                  max_FP_len,\n",
      "                                                                                                                  class_stats['TN'],\n",
      "                                                                                                                  max_TN_len,\n",
      "                                                                                                                  class_stats['FN'],\n",
      "                                                                                                                  max_FN_len,\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TPR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TNR, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * PPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * NPV, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * F1, 2)),\n",
      "                                                                                                                  \"{:0.2f}%\".format(\n",
      "                                                                                                                      round(100.0 * TS, 2))))\n",
      "            \n",
      "    os.remove(cleanfile)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('predictor.py', 'r') as data:\n",
    "    print(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37fc6e",
   "metadata": {},
   "source": [
    "## 4. Validate predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7adb3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Type:                    Random Forest\r\n",
      "System Type:                        2-way classifier\r\n",
      "\r\n",
      "Accuracy:\r\n",
      "    Best-guess accuracy:            61.25%\r\n",
      "    Model accuracy:                 80.00% (64/80 correct)\r\n",
      "    Improvement over best guess:    18.75% (of possible 38.75%)\r\n",
      "\r\n",
      "Model capacity (MEC):               17 bits\r\n",
      "Generalization ratio:               3.62 bits/bit\r\n",
      "\r\n",
      "Confusion Matrix:\r\n",
      "\r\n",
      "      Actual |   Predicted    \r\n",
      "    ----------------------------\r\n",
      "        died |      45        4\r\n",
      "    survived |      12       19\r\n",
      "\r\n",
      "Accuracy by Class:\r\n",
      "\r\n",
      "      target | TP FP TN FN     TPR     TNR     PPV     NPV      F1      TS\r\n",
      "    -------- | -- -- -- -- ------- ------- ------- ------- ------- -------\r\n",
      "        died | 45 12 19  4  91.84%  61.29%  78.95%  82.61%  84.91%  73.77%\r\n",
      "    survived | 19  4 45 12  61.29%  91.84%  82.61%  78.95%  70.37%  54.29%\r\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} predictor.py -validate titanic_validate.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bc793",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "Check out [Brainome 201 Measurements](./brainome_201_Measurements.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b57b9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
