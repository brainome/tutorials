{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895a5bb2",
   "metadata": {},
   "source": [
    "![](https://www.brainome.ai/wp-content/uploads/2020/08/brainome_logo.png)\n",
    "# 105 Using Brainome's Predictors\n",
    "The python predictor generated by Brainome has three modes.\n",
    "1. Command line options\n",
    "2. Validate\n",
    "3. Classify\n",
    "4. Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b880e",
   "metadata": {},
   "source": [
    "## Install brainome and generate a predictor\n",
    "The predictor filename is predictor_104.py\n",
    "The data sets are titanic_train.csv, titanic_validate.csv, and titanic_predict.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57882e55",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: brainome in /opt/conda/lib/python3.9/site-packages (1.6.68)\n",
      "Requirement already satisfied: brainome-linux-python3.9==1.6.* in /opt/conda/lib/python3.9/site-packages (from brainome) (1.6.14)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (2.26.0)\n",
      "Requirement already satisfied: xgboost==1.4.2 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (0.24.2)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (1.9.0)\n",
      "Requirement already satisfied: Jinja2>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.9/site-packages (from brainome-linux-python3.9==1.6.*->brainome) (1.21.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from xgboost==1.4.2->brainome-linux-python3.9==1.6.*->brainome) (1.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2>=3.0.0->brainome-linux-python3.9==1.6.*->brainome) (2.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-linux-python3.9==1.6.*->brainome) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.22.1->brainome-linux-python3.9==1.6.*->brainome) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.4.0->brainome-linux-python3.9==1.6.*->brainome) (3.10.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->brainome-linux-python3.9==1.6.*->brainome) (2021.5.30)\n",
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "\u001b[01;1mBrainome Table Compiler v1.006-14-prod\u001b[0m\n",
      "Copyright (c) 2019-2021 Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:                 y Demo User  (Evaluation)\n",
      "Expiration Date:             2021-12-12   124 days left\n",
      "Maximum File Size:           100 MB\n",
      "Maximum Instances:           20000\n",
      "Maximum Attributes:          100\n",
      "Maximum Classes:             unlimited\n",
      "Connected to:                daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\u001b[01;1mCommand:\u001b[0m\n",
      "    btc data/titanic_train.csv -y -o predictor_104.py -modelonly\n",
      "\n",
      "Start Time:                 08/10/2021, 00:11 UTC\n",
      "\n",
      "Cleaning...done. \n",
      "Splitting into training and validation...done. \n",
      "Pre-training measurements...done. \n",
      "\n",
      "Building classifier...done. \n",
      "Compiling predictor...done. \n",
      "Validating predictor...done. \n",
      "\n",
      "\u001b[01;1mPredictor:\u001b[0m                        predictor_104.py\n",
      "    Classifier Type:              Random Forest\n",
      "    System Type:                  Binary classifier\n",
      "    Training / Validation Split:  60% : 40%\n",
      "    Accuracy:\n",
      "      Best-guess accuracy:        61.50%\n",
      "      Training accuracy:          86.84% (416/479 correct)\n",
      "      Validation Accuracy:        80.99% (260/321 correct)\n",
      "      Combined Model Accuracy:    84.50% (676/800 correct)\n",
      "\n",
      "    Model Capacity (MEC):         41    bits\n",
      "\n",
      "    Generalization Ratio:          9.74 bits/bit\n",
      "    Percent of Data Memorized:    20.84%\n",
      "    Resilience to Noise:          -1.01 dB\n",
      "\n",
      "\n",
      "    Training Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  279   16 \n",
      "            survived |   47  137 \n",
      "\n",
      "    Validation Confusion Matrix:\n",
      "              Actual | Predicted\n",
      "              ------ | ---------\n",
      "                died |  175   22 \n",
      "            survived |   39   85 \n",
      "\n",
      "    Training Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  279   47  137   16   94.58%   74.46%   85.58%   89.54%   89.86%   81.58%\n",
      "            survived |  137   16  279   47   74.46%   94.58%   89.54%   85.58%   81.31%   68.50%\n",
      "\n",
      "    Validation Accuracy by Class:\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\n",
      "                died |  175   39   85   22   88.83%   68.55%   81.78%   79.44%   85.16%   74.15%\n",
      "            survived |   85   22  175   39   68.55%   88.83%   79.44%   81.78%   73.59%   58.22%\n",
      "\n",
      "\n",
      "    Attribute Ranking:\n",
      "                                      Feature | Relative Importance\n",
      "                                          Sex :   0.4912\n",
      "                                  Cabin_Class :   0.1242\n",
      "                                 Cabin_Number :   0.0664\n",
      "                              Parent_Children :   0.0599\n",
      "                                          Age :   0.0599\n",
      "                                Ticket_Number :   0.0414\n",
      "                                         Fare :   0.0379\n",
      "                                  PassengerId :   0.0332\n",
      "                               Sibling_Spouse :   0.0298\n",
      "                                         Name :   0.0288\n",
      "                          Port_of_Embarkation :   0.0273\n",
      "         \n",
      "\n",
      "\n",
      "\n",
      "End Time:           08/10/2021, 00:11 UTC\n",
      "Runtime Duration:   8s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install brainome \n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade brainome\n",
    "!brainome data/titanic_train.csv -y -o predictor_104.py -modelonly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d5739",
   "metadata": {},
   "source": [
    "## 1. Validate a test data set\n",
    "The predictor can take an data set identical to the training data set and compare outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781a9407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Type:                    Random Forest\n",
      "System Type:                        2-way classifier\n",
      "\n",
      "Accuracy:\n",
      "    Best-guess accuracy:            61.25%\n",
      "    Model accuracy:                 81.25% (65/80 correct)\n",
      "    Improvement over best guess:    20.00% (of possible 38.75%)\n",
      "\n",
      "Model capacity (MEC):               41 bits\n",
      "Generalization ratio:               1.52 bits/bit\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "      Actual |   Predicted    \n",
      "    ----------------------------\n",
      "        died |      44        5\n",
      "    survived |      10       21\n",
      "\n",
      "Accuracy by Class:\n",
      "\n",
      "      target | TP FP TN FN     TPR     TNR     PPV     NPV      F1      TS\n",
      "    -------- | -- -- -- -- ------- ------- ------- ------- ------- -------\n",
      "        died | 44 10 21  5  89.80%  67.74%  81.48%  80.77%  85.44%  74.58%\n",
      "    survived | 21  5 44 10  67.74%  89.80%  80.77%  81.48%  73.68%  58.33%\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} predictor_104.py -validate data/titanic_validate.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c35599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\r\n",
      "#\r\n",
      "# This code has been produced by a free evaluation version of Brainome(tm).\r\n",
      "# Portions of this code copyright (c) 2019-2021 by Brainome, Inc. All Rights Reserved.\r\n",
      "# Brainome, Inc grants an exclusive (subject to our continuing rights to use and modify models),\r\n",
      "# worldwide, non-sublicensable, and non-transferable limited license to use and modify this\r\n",
      "# predictor produced through the input of your data:\r\n",
      "# (i) for users accessing the service through a free evaluation account, solely for your\r\n",
      "# own non-commercial purposes, including for the purpose of evaluating this service, and\r\n",
      "# (ii) for users accessing the service through a paid, commercial use account, for your\r\n",
      "# own internal  and commercial purposes.\r\n",
      "# Please contact support@brainome.ai with any questions.\r\n",
      "# Use of predictions results at your own risk.\r\n",
      "#\r\n",
      "# Output of Brainome v1.006-14-prod.\r\n",
      "# Invocation: brainome data/titanic_train.csv -y -o predictor_104.py -modelonly\r\n",
      "# Total compiler execution time: 0:00:07.67. Finished on: Aug-10-2021 00:11:14.\r\n",
      "# This source code requires Python 3.\r\n",
      "#\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "\u001b[01;1mPredictor:\u001b[0m                        predictor_104.py\r\n",
      "    Classifier Type:              Random Forest\r\n",
      "    System Type:                  Binary classifier\r\n",
      "    Training / Validation Split:  60% : 40%\r\n",
      "    Accuracy:\r\n",
      "      Best-guess accuracy:        61.50%\r\n",
      "      Training accuracy:          86.84% (416/479 correct)\r\n",
      "      Validation Accuracy:        80.99% (260/321 correct)\r\n",
      "      Combined Model Accuracy:    84.50% (676/800 correct)\r\n",
      "\r\n",
      "    Model Capacity (MEC):         41    bits\r\n",
      "\r\n",
      "    Generalization Ratio:          9.74 bits/bit\r\n",
      "    Percent of Data Memorized:    20.84%\r\n",
      "    Resilience to Noise:          -1.01 dB\r\n",
      "\r\n",
      "\r\n",
      "    Training Confusion Matrix:\r\n",
      "              Actual | Predicted\r\n",
      "              ------ | ---------\r\n",
      "                died |  279   16 \r\n",
      "            survived |   47  137 \r\n",
      "\r\n",
      "    Validation Confusion Matrix:\r\n",
      "              Actual | Predicted\r\n",
      "              ------ | ---------\r\n",
      "                died |  175   22 \r\n",
      "            survived |   39   85 \r\n",
      "\r\n",
      "    Training Accuracy by Class:\r\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \r\n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\r\n",
      "                died |  279   47  137   16   94.58%   74.46%   85.58%   89.54%   89.86%   81.58%\r\n",
      "            survived |  137   16  279   47   74.46%   94.58%   89.54%   85.58%   81.31%   68.50%\r\n",
      "\r\n",
      "    Validation Accuracy by Class:\r\n",
      "            Survived |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS \r\n",
      "            -------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------\r\n",
      "                died |  175   39   85   22   88.83%   68.55%   81.78%   79.44%   85.16%   74.15%\r\n",
      "            survived |   85   22  175   39   68.55%   88.83%   79.44%   81.78%   73.59%   58.22%\r\n",
      "\r\n",
      "\r\n",
      "    Attribute Ranking:\r\n",
      "                                      Feature | Relative Importance\r\n",
      "                                          Sex :   0.4912\r\n",
      "                                  Cabin_Class :   0.1242\r\n",
      "                                 Cabin_Number :   0.0664\r\n",
      "                              Parent_Children :   0.0599\r\n",
      "                                          Age :   0.0599\r\n",
      "                                Ticket_Number :   0.0414\r\n",
      "                                         Fare :   0.0379\r\n",
      "                                  PassengerId :   0.0332\r\n",
      "                               Sibling_Spouse :   0.0298\r\n",
      "                                         Name :   0.0288\r\n",
      "                          Port_of_Embarkation :   0.0273\r\n",
      "         \r\n",
      "\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "import sys\r\n",
      "import math\r\n",
      "import os\r\n",
      "import argparse\r\n",
      "import tempfile\r\n",
      "import csv\r\n",
      "import binascii\r\n",
      "import faulthandler\r\n",
      "import json\r\n",
      "from io import StringIO\r\n",
      "try:\r\n",
      "    import numpy as np # For numpy see: http://numpy.org\r\n",
      "    from numpy import array\r\n",
      "except:\r\n",
      "    print(\"This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.\")\r\n",
      "    sys.exit(1)\r\n",
      "try:\r\n",
      "    from scipy.sparse import coo_matrix\r\n",
      "    report_cmat = True\r\n",
      "except:\r\n",
      "    print(\"Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.\")\r\n",
      "    report_cmat = False\r\n",
      "try:\r\n",
      "    import multiprocessing\r\n",
      "    var_dict = {}\r\n",
      "    default_to_serial = False\r\n",
      "except:\r\n",
      "    default_to_serial = True\r\n",
      "\r\n",
      "IOBUF = 100000000\r\n",
      "sys.setrecursionlimit(1000000)\r\n",
      "TRAINFILE = ['data/titanic_train.csv']\r\n",
      "mapping = {'died': 0, 'survived': 1}\r\n",
      "ignorelabels = []\r\n",
      "ignorecolumns = []\r\n",
      "target = '' \r\n",
      "target_column = 11\r\n",
      "important_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n",
      "ignore_idxs = []\r\n",
      "classifier_type = 'RF'\r\n",
      "num_attr = 11\r\n",
      "n_classes = 2\r\n",
      "model_cap = 41\r\n",
      "logits_dict = {0: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.36425662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.243536353, 0.191349998, 0.0098128207, 0.331673324, -0.0, -0.255133331, 0.0883153826, 0.298732072, -0.287025005, 0.0765400007, 0.185177416, 0.0153079992]), 1: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.36425662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.243536353, -0.191349998, -0.0098128207, -0.331673324, -0.0, 0.255133331, -0.0883153826, -0.298732072, 0.287025005, -0.0765400007, -0.185177416, -0.0153079992]), 2: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.265477031, 0.0, 0.270277321, 0.0, 0.0, 0.0, 0.0, -0.150360435, 0.00850328244, 0.185843408, -0.0649351925, -0.116907045, 0.0839446634, -0.0411008634, 0.232498676, 0.0817588419, 0.235827729]), 3: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.265477002, 0.0, -0.270277321, 0.0, 0.0, 0.0, 0.0, 0.15036045, -0.00850327779, -0.185843378, 0.0649352148, 0.116907068, -0.0839446336, 0.041100882, -0.232498676, -0.0817588121, -0.235827684]), 4: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.254825503, 0.0, 0.0, -0.123233855, -0.235735834, 0.039184168, -0.188743472, -0.153517619, 0.112402298, 0.196248844, -0.115611948, -0.191678584, 0.120147459, 0.160565287, 0.0143993665]), 5: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.254825503, 0.0, 0.0, 0.123233832, 0.235735834, -0.039184168, 0.188743442, 0.153517663, -0.112402275, -0.196248844, 0.11561197, 0.191678569, -0.120147429, -0.160565287, -0.0143993739]), 6: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.214141831, 0.0, 0.0, 0.0, 0.222953677, 0.0, 0.0, 0.0810240507, -0.186694652, 0.0149209201, -0.165880695, 0.0454831272, 0.233664706, 0.164316222, -0.0880754292, 0.105792671, -0.078477487, 0.0124567663, 0.118342534]), 7: array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.214141831, 0.0, 0.0, 0.0, -0.222953692, 0.0, 0.0, -0.0810240358, 0.186694652, -0.0149209267, 0.16588068, -0.0454831496, -0.233664706, -0.164316207, 0.0880754441, -0.105792664, 0.078477487, -0.0124568064, -0.118342534])}\r\n",
      "right_children_dict = {0: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 1: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 2: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, -1, 17, 19, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 3: array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, -1, 17, 19, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 4: array([1, 3, 5, 7, 9, 11, 13, -1, 15, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 5: array([1, 3, 5, 7, 9, 11, 13, -1, 15, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 6: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 7: array([1, 3, 5, 7, 9, 11, 13, 15, -1, 17, 19, 21, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])}\r\n",
      "split_feats_dict = {0: array([3, 1, 9, 7, 4, 7, 7, 0, 0, 7, 8, 0, 7, 9, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 1: array([3, 1, 9, 7, 4, 7, 7, 0, 0, 7, 8, 0, 7, 9, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 2: array([3, 1, 7, 7, 4, 8, 1, 0, 0, 8, 0, 6, 7, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 3: array([3, 1, 7, 7, 4, 8, 1, 0, 0, 8, 0, 6, 7, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 4: array([3, 1, 8, 7, 4, 7, 4, 0, 8, 7, 8, 0, 0, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 5: array([3, 1, 8, 7, 4, 7, 4, 0, 8, 7, 8, 0, 0, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 6: array([3, 1, 8, 0, 8, 7, 7, 8, 0, 7, 2, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 7: array([3, 1, 8, 0, 8, 7, 7, 8, 0, 7, 2, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\r\n",
      "split_vals_dict = {0: array([1342256510.0, 2.5, 99463000.0, 11753.5, 6.5, 2621.5, 17458.0, 0.0, 0.0, 1855777020.0, 17.5999985, 340.0, 2666.0, 1777625860.0, 113785.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 1: array([1342256510.0, 2.5, 99463000.0, 11753.5, 6.5, 2621.5, 17458.0, 0.0, 0.0, 1855777020.0, 17.5999985, 340.0, 2666.0, 1777625860.0, 113785.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 2: array([1342256510.0, 2.5, 314014.0, 11753.5, 36.5, 26.2749996, 1.5, 0.0, 0.0, 8.03960037, 0.0, 0.5, 28396.0, 2686373380.0, 299527200.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 3: array([1342256510.0, 2.5, 314014.0, 11753.5, 36.5, 26.2749996, 1.5, 0.0, 0.0, 8.03960037, 0.0, 0.5, 28396.0, 2686373380.0, 299527200.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 4: array([1342256510.0, 2.5, 7.76249981, 11753.5, 6.5, 2666.0, 13.0, 0.0, 13.25, 6683.5, 7.69999981, 543.0, 0.0, 314014.0, 669928000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 5: array([1342256510.0, 2.5, 7.76249981, 11753.5, 6.5, 2666.0, 13.0, 0.0, 13.25, 6683.5, 7.69999981, 543.0, 0.0, 314014.0, 669928000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 6: array([1342256510.0, 2.5, 7.76249981, 208.0, 24.8083496, 2666.0, 27627.5, 22.0, 0.0, 366226.0, 2345232900.0, 543.0, 0.0, 14.8520498, 11.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 7: array([1342256510.0, 2.5, 7.76249981, 208.0, 24.8083496, 2666.0, 27627.5, 22.0, 0.0, 366226.0, 2345232900.0, 543.0, 0.0, 14.8520498, 11.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])}\r\n",
      "\r\n",
      "\r\n",
      "def __convert(cell):\r\n",
      "    value = str(cell)\r\n",
      "    try:\r\n",
      "        result = int(value)\r\n",
      "        return result\r\n",
      "    except ValueError:\r\n",
      "        try:\r\n",
      "            result=float(value)\r\n",
      "            if math.isnan(result):\r\n",
      "                print('NaN value found. Aborting.')\r\n",
      "                sys.exit(1)\r\n",
      "            return result\r\n",
      "        except ValueError:\r\n",
      "            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\r\n",
      "            return result\r\n",
      "        except Exception as e:\r\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\r\n",
      "            sys.exit(1)\r\n",
      "\r\n",
      "\r\n",
      "def __get_key(val, dictionary):\r\n",
      "    if dictionary == {}:\r\n",
      "        return val\r\n",
      "    for key, value in dictionary.items(): \r\n",
      "        if val == value:\r\n",
      "            return key\r\n",
      "    if val not in dictionary.values:\r\n",
      "        print(\"Label key does not exist\")\r\n",
      "        sys.exit(1)\r\n",
      "\r\n",
      "\r\n",
      "def __convertclassid(cell, classlist=[]):\r\n",
      "\r\n",
      "    value = str(cell)\r\n",
      "    \r\n",
      "    if value == '':\r\n",
      "        print('Empty value encountered for a class label. Aborting.')\r\n",
      "        sys.exit(1)\r\n",
      "    \r\n",
      "    if mapping != {}:\r\n",
      "        result = -1\r\n",
      "        try:\r\n",
      "            result = mapping[cell]\r\n",
      "        except KeyError:\r\n",
      "            print(f\"The class label {value} does not exist in the class mapping. Aborting.\")\r\n",
      "            sys.exit(1)\r\n",
      "        except Exception as e:\r\n",
      "            print(f\"An exception of type {type(e).__name__} was encountered. Aborting.\")\r\n",
      "            sys.exit(1)\r\n",
      "        if result != int(result):\r\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\r\n",
      "            sys.exit(1)\r\n",
      "        if str(result) not in classlist:\r\n",
      "            classlist.append(str(result))\r\n",
      "        return result\r\n",
      "    \r\n",
      "    try:\r\n",
      "        result = float(cell)\r\n",
      "        if str(result) not in classlist:\r\n",
      "            classlist.append(str(result))\r\n",
      "    except:\r\n",
      "        result = (binascii.crc32(value.encode('utf8')) % (1 << 32))\r\n",
      "        if result in classlist:\r\n",
      "            result = classlist.index(result)\r\n",
      "        else:\r\n",
      "            classlist.append(str(result))\r\n",
      "            result = classlist.index(result)\r\n",
      "        if result != int(result):\r\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to integers. Aborting.\")\r\n",
      "            sys.exit(1)\r\n",
      "    finally:\r\n",
      "        if result < 0:\r\n",
      "            print(f\"The label {value} is mapped to {result} but class labels must be mapped to non-negative integers. Aborting.\")\r\n",
      "            sys.exit(1)\r\n",
      "\r\n",
      "    return result\r\n",
      "\r\n",
      "\r\n",
      "def __clean(filename, outfile, headerless=False, testfile=False, trim=False):\r\n",
      "    classlist = []\r\n",
      "    outbuf = []\r\n",
      "    remove_bad_chars = lambda x: x.replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\r\n",
      "    \r\n",
      "    with open(filename, encoding='utf-8') as csv_file, open(outfile, \"w+\", encoding='utf-8') as f:\r\n",
      "        \r\n",
      "        reader = csv.reader(csv_file)\r\n",
      "        if not headerless:\r\n",
      "            next(reader, None)\r\n",
      "        \r\n",
      "        for i, row in enumerate(reader):\r\n",
      "\r\n",
      "            if row == []:\r\n",
      "                continue\r\n",
      "\r\n",
      "            \r\n",
      "            expected_row_length = len(important_idxs)\r\n",
      "            if not trim:\r\n",
      "                expected_row_length += len(ignorecolumns)\r\n",
      "            if not testfile:\r\n",
      "                expected_row_length += 1\r\n",
      "            actual_row_length = len(row)\r\n",
      "\r\n",
      "            if testfile and actual_row_length == expected_row_length + 1:\r\n",
      "                error_str = f\"We found {actual_row_length} columns but expected {expected_row_length} columns at row {i}. \"\r\n",
      "                error_str += f\"Please check that the CSV contains no target column otherwise use -validate. Aborting.\"\r\n",
      "                print(error_str)\r\n",
      "                sys.exit(1)\r\n",
      "            \r\n",
      "            if actual_row_length != expected_row_length:\r\n",
      "                print(f\"We found {actual_row_length} columns but expected {expected_row_length} columns.\")\r\n",
      "                sys.exit(1)            \r\n",
      "\r\n",
      "            if testfile:\r\n",
      "                if len(row) == 1:\r\n",
      "                    converted_row = [str(__convert(remove_bad_chars(row[0])))]\r\n",
      "                else:\r\n",
      "                    converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convert(remove_bad_chars(row[-1])))]         \r\n",
      "            else:\r\n",
      "                converted_row = [str(__convert(remove_bad_chars(element))) + \",\" for element in row[:-1]] + [str(__convertclassid(row[-1], classlist))]\r\n",
      "            outbuf.extend(converted_row)\r\n",
      "\r\n",
      "            if len(outbuf) < IOBUF:\r\n",
      "                outbuf.append(os.linesep)\r\n",
      "            else:\r\n",
      "                print(''.join(outbuf), file=f)\r\n",
      "                outbuf = []\r\n",
      "        \r\n",
      "        print(''.join(outbuf), end=\"\", file=f)\r\n",
      "\r\n",
      "    n_classes_found = len(classlist)\r\n",
      "    if not testfile and n_classes_found < 2:\r\n",
      "        print(f\"Only {n_classes_found} classes were found. Aborting.\")\r\n",
      "        sys.exit(1)\r\n",
      "\r\n",
      "\r\n",
      "def __confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):\r\n",
      "    stats = {}\r\n",
      "    if labels is None:\r\n",
      "        labels = np.array(list(set(list(y_true.astype('int')))))\r\n",
      "    else:\r\n",
      "        labels = np.asarray(labels)\r\n",
      "        if np.all([l not in y_true for l in labels]):\r\n",
      "            raise ValueError(\"At least one label specified must be in y_true\")\r\n",
      "    n_labels = labels.size\r\n",
      "\r\n",
      "    for class_i in range(n_labels):\r\n",
      "        stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}\r\n",
      "        class_i_indices = np.argwhere(y_true==class_i)\r\n",
      "        not_class_i_indices = np.argwhere(y_true!=class_i)\r\n",
      "        stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices] == class_i))\r\n",
      "        stats[int(class_i)]['FN'] = int(np.sum(y_pred[class_i_indices] != class_i))\r\n",
      "        stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices] != class_i))\r\n",
      "        stats[int(class_i)]['FP'] = int(np.sum(y_pred[not_class_i_indices] == class_i))\r\n",
      "\r\n",
      "    if not report_cmat:\r\n",
      "        if json:\r\n",
      "            return np.array([]), stats\r\n",
      "        else:\r\n",
      "            sys.exit(0)\r\n",
      "\r\n",
      "    if sample_weight is None:\r\n",
      "        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\r\n",
      "    else:\r\n",
      "        sample_weight = np.asarray(sample_weight)\r\n",
      "    if y_true.shape[0]!=y_pred.shape[0]:\r\n",
      "        raise ValueError(\"y_true and y_pred must be of the same length\")\r\n",
      "\r\n",
      "    if normalize not in ['true', 'pred', 'all', None]:\r\n",
      "        raise ValueError(\"normalize must be one of {'true', 'pred', 'all', None}\")\r\n",
      "\r\n",
      "\r\n",
      "    label_to_ind = {y: x for x, y in enumerate(labels)}\r\n",
      "    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\r\n",
      "    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\r\n",
      "    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\r\n",
      "    y_pred = y_pred[ind]\r\n",
      "    y_true = y_true[ind]\r\n",
      "\r\n",
      "    sample_weight = sample_weight[ind]\r\n",
      "    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\r\n",
      "        dtype = np.int64\r\n",
      "    else:\r\n",
      "        dtype = np.float64\r\n",
      "    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()\r\n",
      "\r\n",
      "    with np.errstate(all='ignore'):\r\n",
      "        if normalize == 'true':\r\n",
      "            cm = cm / cm.sum(axis=1, keepdims=True)\r\n",
      "        elif normalize == 'pred':\r\n",
      "            cm = cm / cm.sum(axis=0, keepdims=True)\r\n",
      "        elif normalize == 'all':\r\n",
      "            cm = cm / cm.sum()\r\n",
      "        cm = np.nan_to_num(cm)\r\n",
      "    return cm, stats\r\n",
      "\r\n",
      "\r\n",
      "def __predict(arr, headerless, csvfile, trim=False):\r\n",
      "    with open(csvfile, 'r', encoding='utf-8') as csvinput:\r\n",
      "        reader = csv.reader(csvinput)\r\n",
      "        if not headerless:\r\n",
      "            if trim:\r\n",
      "                header = ','.join([x for i, x in enumerate(next(reader, None)) if i in important_idxs] + ['Prediction'])\r\n",
      "            else:\r\n",
      "                header = ','.join(next(reader, None) + ['Prediction'])\r\n",
      "            print(header)\r\n",
      "        outputs = __classify(arr)\r\n",
      "        for i, row in enumerate(reader):\r\n",
      "            pred = str(__get_key(int(outputs[i]), mapping))\r\n",
      "            if trim:\r\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]\r\n",
      "            else:\r\n",
      "                row = ['\"' + field + '\"' if ',' in field else field for field in row]            \r\n",
      "            row.append(pred)\r\n",
      "            print(','.join(row))\r\n",
      "\r\n",
      "\r\n",
      "def __preprocess_and_clean_in_memory(arr):\r\n",
      "    if not isinstance(arr, list) and not isinstance(arr, np.ndarray):\r\n",
      "        print(f'The input to \\'predict\\' must be a list or np.ndarray but an input of type {type(arr).__name__} was found.')\r\n",
      "        sys.exit(1)\r\n",
      "    clean_arr = np.zeros((len(arr), len(important_idxs)))\r\n",
      "    for i, row in enumerate(arr):\r\n",
      "        try:\r\n",
      "            row_used_cols_only = [row[i] for i in important_idxs]\r\n",
      "        except IndexError:\r\n",
      "            error_str = f\"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {num_attr}).\"\r\n",
      "            if len(arr) == num_attr and len(arr[0]) != num_attr:\r\n",
      "                error_str += \"\\n\\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' \"\r\n",
      "                error_str += \"rather than as an element of a list. Make sure that even single instances \"\r\n",
      "                error_str += \"are enclosed in a list. Example: predict_in_memory(0) is invalid but \"\r\n",
      "                error_str += \"predict_in_memory([0]) is valid.\"\r\n",
      "            print(error_str)\r\n",
      "            sys.exit(1)\r\n",
      "        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]\r\n",
      "    return clean_arr\r\n",
      "\r\n",
      "\r\n",
      "def __evaluate_tree(xs, split_vals, split_feats, right_children, logits):\r\n",
      "    if xs is None:\r\n",
      "        xs = np.frombuffer(var_dict['X']).reshape(var_dict['X_shape'])\r\n",
      "\r\n",
      "    current_node_per_row = np.zeros(xs.shape[0]).astype('int')\r\n",
      "    values = np.empty(xs.shape[0])\r\n",
      "    values.fill(np.nan)\r\n",
      "\r\n",
      "    while np.isnan(values).any():\r\n",
      "\r\n",
      "        row_idxs_at_leaf = np.argwhere(np.logical_and(right_children[current_node_per_row] == -1, np.isnan(values))).reshape(-1)\r\n",
      "        row_idxs_at_branch = np.argwhere(right_children[current_node_per_row] != -1).reshape(-1)\r\n",
      "\r\n",
      "        if row_idxs_at_leaf.shape[0] > 0:\r\n",
      "\r\n",
      "            values[row_idxs_at_leaf] = logits[current_node_per_row[row_idxs_at_leaf]].reshape(-1)\r\n",
      "            current_node_per_row[row_idxs_at_leaf] = -1\r\n",
      "\r\n",
      "        if row_idxs_at_branch.shape[0] > 0:\r\n",
      "\r\n",
      "            split_values_per_row = split_vals[current_node_per_row[row_idxs_at_branch]].astype('float64')\r\n",
      "            split_features_per_row = split_feats[current_node_per_row[row_idxs_at_branch]].astype('int')\r\n",
      "            feature_val_per_row = xs[row_idxs_at_branch, split_features_per_row].reshape(-1)\r\n",
      "\r\n",
      "            branch_nodes = current_node_per_row[row_idxs_at_branch]\r\n",
      "            current_node_per_row[row_idxs_at_branch] = np.where(feature_val_per_row < split_values_per_row,\r\n",
      "                                                                right_children[branch_nodes].astype('int'),\r\n",
      "                                                                (right_children[branch_nodes] + 1).astype('int'))\r\n",
      "\r\n",
      "    return values\r\n",
      "\r\n",
      "\r\n",
      "def __build_logit_func(n_trees, clss):\r\n",
      "\r\n",
      "    def __logit_func(xs, serial, data_shape, pool=None):\r\n",
      "        if serial:\r\n",
      "            sum_of_leaf_values = np.zeros(xs.shape[0])\r\n",
      "            for booster_index in range(clss, n_trees, n_classes):\r\n",
      "                sum_of_leaf_values += __evaluate_tree(xs, split_vals_dict[booster_index], split_feats_dict[booster_index],\r\n",
      "                                                right_children_dict[booster_index], logits_dict[booster_index])\r\n",
      "        else:\r\n",
      "            sum_of_leaf_values = np.sum(list(pool.starmap(__evaluate_tree,\r\n",
      "                                            [(None, split_vals_dict[booster_index], split_feats_dict[booster_index],\r\n",
      "                                              right_children_dict[booster_index], logits_dict[booster_index])\r\n",
      "                                    for booster_index in range(clss, n_trees, n_classes)])), axis=0)\r\n",
      "        return sum_of_leaf_values\r\n",
      "\r\n",
      "    return __logit_func\r\n",
      "\r\n",
      "\r\n",
      "def __init_worker(X, X_shape):\r\n",
      "    var_dict['X'] = X\r\n",
      "    var_dict['X_shape'] = X_shape\r\n",
      "\r\n",
      "\r\n",
      "def __classify(rows, return_probabilities=False, force_serial=False):\r\n",
      "    if force_serial:\r\n",
      "        serial = True\r\n",
      "    else:\r\n",
      "        serial = default_to_serial\r\n",
      "    if isinstance(rows, list):\r\n",
      "        rows = np.array(rows)\r\n",
      "\r\n",
      "    logits = [__build_logit_func(8, clss) for clss in range(n_classes)]\r\n",
      "\r\n",
      "    if serial:\r\n",
      "        o = np.array([logits[class_index](rows, True, rows.shape) for class_index in range(n_classes)]).T\r\n",
      "    else:\r\n",
      "        shared_arr = multiprocessing.RawArray('d', rows.shape[0] * rows.shape[1])\r\n",
      "        shared_arr_np = np.frombuffer(shared_arr, dtype=rows.dtype).reshape(rows.shape)\r\n",
      "        np.copyto(shared_arr_np, rows)\r\n",
      "\r\n",
      "        procs = multiprocessing.cpu_count()\r\n",
      "        pool = multiprocessing.Pool(processes=procs, initializer=__init_worker, initargs=(shared_arr, rows.shape))\r\n",
      "        o = np.array([logits[class_index](None, False, rows.shape, pool) for class_index in range(n_classes)]).T\r\n",
      "\r\n",
      "    if return_probabilities:\r\n",
      "        \r\n",
      "        argument = o[:, 0] - o[:, 1]\r\n",
      "        p0 = 1.0 / (1.0 + np.exp(-argument)).reshape(-1, 1)\r\n",
      "        p1 = 1.0 - p0\r\n",
      "        output = np.concatenate((p0, p1), axis=1)\r\n",
      "        \r\n",
      "    else:\r\n",
      "        output = np.argmax(o,axis=1)\r\n",
      "    return output\r\n",
      "\r\n",
      "\r\n",
      "def __validate_kwargs(kwargs):\r\n",
      "    for key in kwargs:\r\n",
      "        if key not in ['return_probabilities', 'force_serial']:\r\n",
      "        \r\n",
      "            print(f'{key} is not a keyword argument for Brainome\\'s {classifier_type} predictor. Please see the documentation.')\r\n",
      "            sys.exit(1)\r\n",
      "\r\n",
      "\r\n",
      "def predict(arr, remap=True, **kwargs):\r\n",
      "    \"\"\"\r\n",
      "    Parameters\r\n",
      "    ----------\r\n",
      "    arr : list[list]\r\n",
      "        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'.\r\n",
      "\r\n",
      "    remap : bool\r\n",
      "        If True and 'return_probs' is False, remaps the output to the original class\r\n",
      "        label. If 'return_probs' is True this instead adds a header indicating which\r\n",
      "        original class label each column of output corresponds to.\r\n",
      "    \r\n",
      "    **kwargs :\r\n",
      "        return_probabilities : bool\r\n",
      "            If true, return class membership probabilities instead of classifications.\r\n",
      "        force_serial : bool\r\n",
      "            If true, model inference is done in serial rather than in parallel. This is\r\n",
      "            useful if calling \"predict\" repeatedly inside a for-loop.\r\n",
      "        \r\n",
      "    Returns\r\n",
      "    -------\r\n",
      "    output : np.ndarray\r\n",
      "        A numpy array of\r\n",
      "\r\n",
      "            1. Class predictions if 'return_probabilities' is False.\r\n",
      "            2. Class probabilities if 'return_probabilities' is True.\r\n",
      "        \"\"\"\r\n",
      "    kwargs = kwargs or {}\r\n",
      "    __validate_kwargs(kwargs)\r\n",
      "    remove_bad_chars = lambda x: str(x).replace('\"', '').replace(',', '').replace('(', '').replace(')', '')\r\n",
      "    arr = [[remove_bad_chars(field) for field in row] for row in arr]\r\n",
      "    arr = __preprocess_and_clean_in_memory(arr)\r\n",
      "    output = __classify(arr, **kwargs)\r\n",
      "    if remap:\r\n",
      "        if len(output.shape) > 1: # probabilities were returned\r\n",
      "            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)\r\n",
      "            output = np.concatenate((header, output), axis=0)\r\n",
      "        else:\r\n",
      "            output = np.array([__get_key(prediction, mapping) for prediction in output])\r\n",
      "    return output\r\n",
      "\r\n",
      "\r\n",
      "def validate(cleanarr):\r\n",
      "    \"\"\"\r\n",
      "    Parameters\r\n",
      "    ----------\r\n",
      "    cleanarr : np.ndarray\r\n",
      "        An array of float values that has undergone each pre-\r\n",
      "        prediction step.\r\n",
      "\r\n",
      "    Returns\r\n",
      "    -------\r\n",
      "    count : int\r\n",
      "        A count of the number of instances in cleanarr.\r\n",
      "\r\n",
      "    correct_count : int\r\n",
      "        A count of the number of correctly classified instances in\r\n",
      "        cleanarr.\r\n",
      "\r\n",
      "    numeachclass : dict\r\n",
      "        A dictionary mapping each class to its number of instances.\r\n",
      "\r\n",
      "    outputs : np.ndarray\r\n",
      "        The output of the predictor's '__classify' method on cleanarr.\r\n",
      "    \"\"\"\r\n",
      "    outputs = __classify(cleanarr[:, :-1])\r\n",
      "    count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0\r\n",
      "    correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))\r\n",
      "    count = outputs.shape[0]\r\n",
      "    num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))\r\n",
      "    num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))\r\n",
      "    num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))\r\n",
      "    num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))\r\n",
      "    num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))\r\n",
      "    num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))\r\n",
      "    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs\r\n",
      "    \r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))\r\n",
      "    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')\r\n",
      "    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')\r\n",
      "    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')\r\n",
      "    parser.add_argument('-json', action=\"store_true\", default=False, help=\"report measurements as json\")\r\n",
      "    parser.add_argument('-trim', action=\"store_true\", help=\"If true, the prediction will not output ignored columns.\")\r\n",
      "    args = parser.parse_args()\r\n",
      "    faulthandler.enable()\r\n",
      "\r\n",
      "    if args.validate:\r\n",
      "        args.trim = True\r\n",
      "\r\n",
      "    is_testfile = not args.validate\r\n",
      "    \r\n",
      "    cleanfile = tempfile.NamedTemporaryFile().name\r\n",
      "    __clean(args.csvfile, cleanfile, args.headerless, is_testfile, trim=args.trim)\r\n",
      "    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')\r\n",
      "    if len(cleanarr.shape) == 1:\r\n",
      "        if args.trim and len(important_idxs) == 1:\r\n",
      "            cleanarr = cleanarr.reshape(-1, 1)\r\n",
      "        elif len(open(cleanfile, 'r').read().splitlines()) == 1:\r\n",
      "            cleanarr = cleanarr.reshape(1, -1)\r\n",
      "\r\n",
      "    if not args.trim and ignorecolumns != []:\r\n",
      "        cleanarr = cleanarr[:, important_idxs].reshape(-1, len(important_idxs))\r\n",
      "\r\n",
      "    if not args.validate:\r\n",
      "        __predict(cleanarr, args.headerless, args.csvfile, trim=args.trim)\r\n",
      "    else:\r\n",
      "        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(cleanarr)\r\n",
      "        \r\n",
      "        true_labels = cleanarr[:, -1]\r\n",
      "        classcounts = np.bincount(cleanarr[:, -1].astype('int32')).reshape(-1)\r\n",
      "        classbalance = (classcounts[np.argwhere(classcounts > 0)] / cleanarr.shape[0]).reshape(-1).tolist()\r\n",
      "        best_guess = round(100.0 * np.max(classbalance), 2)\r\n",
      "        H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))\r\n",
      "        modelacc = int(float(correct_count * 10000) / count) / 100.0\r\n",
      "\r\n",
      "        if args.json:\r\n",
      "            FN = float(num_FN) * 100.0 / float(count)\r\n",
      "            FP = float(num_FP) * 100.0 / float(count)\r\n",
      "            TN = float(num_TN) * 100.0 / float(count)\r\n",
      "            TP = float(num_TP) * 100.0 / float(count)\r\n",
      "\r\n",
      "            if int(num_TP + num_FN) != 0:\r\n",
      "                TPR = num_TP / (num_TP + num_FN)  # Sensitivity, Recall\r\n",
      "            if int(num_TN + num_FP) != 0:\r\n",
      "                TNR = num_TN / (num_TN + num_FP)  # Specificity\r\n",
      "            if int(num_TP + num_FP) != 0:\r\n",
      "                PPV = num_TP / (num_TP + num_FP)  # Recall\r\n",
      "            if int(num_FN + num_TP) != 0:\r\n",
      "                FNR = num_FN / (num_FN + num_TP)  # Miss rate\r\n",
      "            if int(2 * num_TP + num_FP + num_FN) != 0:\r\n",
      "                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN)  # F1 Score\r\n",
      "            if int(num_TP + num_FN + num_FP) != 0:\r\n",
      "                TS = num_TP / (num_TP + num_FN + num_FP)  # Critical Success Index\r\n",
      "            json_dict = {'instance_count': count,\r\n",
      "                         'classifier_type': classifier_type,\r\n",
      "                         'classes': n_classes,\r\n",
      "                         'number_correct': correct_count,\r\n",
      "                         'accuracy': {\r\n",
      "                             'best_guess': best_guess,\r\n",
      "                             'improvement': modelacc - best_guess,\r\n",
      "                             'model_accuracy': modelacc,\r\n",
      "                         },\r\n",
      "                         'false_negative_instances': num_FN,\r\n",
      "                         'false_positive_instances': num_FP,\r\n",
      "                         'true_positive_instances': num_TP,\r\n",
      "                         'true_negative_instances': num_TN,\r\n",
      "                         'false_negatives': FN,\r\n",
      "                         'false_positives': FP,\r\n",
      "                         'true_negatives': TN,\r\n",
      "                         'true_positives': TP,\r\n",
      "                         'model_capacity': model_cap,\r\n",
      "                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,\r\n",
      "                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,\r\n",
      "                         'shannon_entropy_of_labels': H,\r\n",
      "                         'classbalance': classbalance} \r\n",
      "        else:\r\n",
      "            print(\"Classifier Type:                    Random Forest\")\r\n",
      "            print(f\"System Type:                        {n_classes}-way classifier\")\r\n",
      "            print()\r\n",
      "            print(\"Accuracy:\")\r\n",
      "            print(\"    Best-guess accuracy:            {:.2f}%\".format(best_guess))\r\n",
      "            print(\"    Model accuracy:                 {:.2f}%\".format(modelacc) + \" (\" + str(int(correct_count)) + \"/\" + str(count) + \" correct)\")\r\n",
      "            print(\"    Improvement over best guess:    {:.2f}%\".format(modelacc - best_guess) + \" (of possible \" + str(round(100 - best_guess, 2)) + \"%)\")\r\n",
      "            print()\r\n",
      "            print(\"Model capacity (MEC):               {:.0f} bits\".format(model_cap))\r\n",
      "            if classifier_type == '\\'NN\\'':\r\n",
      "                print(\"Model Capacity Utilized:            {:.0f} bits\".format(cap_utilized))  # noqa\r\n",
      "            print(\"Generalization ratio:               {:.2f}\".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + \" bits/bit\")\r\n",
      "\r\n",
      "        mtrx, stats = __confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)\r\n",
      "\r\n",
      "        if args.json:\r\n",
      "            json_dict['confusion_matrix'] = mtrx.tolist()\r\n",
      "            json_dict['multiclass_stats'] = stats\r\n",
      "            print(json.dumps(json_dict))\r\n",
      "        else:\r\n",
      "            mtrx = mtrx.astype('str')\r\n",
      "            labels = np.array(list(mapping.keys())).reshape(-1, 1)\r\n",
      "            mtrx = np.concatenate((labels, mtrx), axis=1).astype('str')\r\n",
      "            max_TP_len, max_FP_len, max_TN_len, max_FN_len = 0, 0, 0, 0\r\n",
      "            max_class_name_len = len('target') + 2\r\n",
      "            for classs in mapping.keys():\r\n",
      "                max_class_name_len = max(max_class_name_len, len(classs))\r\n",
      "            for key in stats.keys():\r\n",
      "                class_stats = stats[key]\r\n",
      "                max_TP_len, max_FP_len, max_TN_len, max_FN_len = max(max_TP_len, len(str(class_stats['TP']))), max(max_FP_len, len(str(class_stats['FP']))), max(\r\n",
      "                    max_TN_len, len(str(class_stats['TN']))), max(max_FN_len, len(str(class_stats['FN'])))\r\n",
      "            print()\r\n",
      "            print(\"Confusion Matrix:\")\r\n",
      "            print()\r\n",
      "            max_len_value = int(np.max(np.vectorize(len)(mtrx)))\r\n",
      "            max_pred_len = (int(mtrx.shape[1]) - 1) * max_len_value\r\n",
      "\r\n",
      "            print(\" \" * 4 + \"{:>{}} |{:^{}}\".format(\"Actual\", max_class_name_len, \"Predicted\", max_pred_len))\r\n",
      "            print(\" \" * 4 + \"-\" * (max_class_name_len + max_pred_len + mtrx.shape[1] + 1))\r\n",
      "            for row in mtrx:\r\n",
      "                print(str(\" \" * 4 + \"{:>{}}\".format(row[0], max_class_name_len)) + \" |\" + \"{:^{}}\".format(\r\n",
      "                    (' '.join([str('{:>{}}'.format(i, max_len_value)) for i in row[1:]])), max_pred_len))\r\n",
      "            print()\r\n",
      "            print(\"Accuracy by Class:\")\r\n",
      "            print()\r\n",
      "            print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format('target',\r\n",
      "                                                                                                              max_class_name_len,\r\n",
      "                                                                                                              'TP', max_TP_len,\r\n",
      "                                                                                                              'FP', max_FP_len,\r\n",
      "                                                                                                              'TN', max_TN_len,\r\n",
      "                                                                                                              'FN', max_FN_len,\r\n",
      "                                                                                                              'TPR', 'TNR',\r\n",
      "                                                                                                              'PPV', 'NPV',\r\n",
      "                                                                                                              'F1', 'TS'))\r\n",
      "            print(\" \" * 4 + \"-\" * max_class_name_len + ' | ' + \"-\" * (\r\n",
      "                max_TP_len) + ' ' + \"-\" * max_FP_len + ' ' + \"-\" * max_TN_len + ' ' + \"-\" * max_FN_len + (' ' + 7 * \"-\") * 6)\r\n",
      "            for raw_class in mapping.keys():\r\n",
      "                class_stats = stats[int(mapping[raw_class])]\r\n",
      "                TPR = class_stats['TP'] / (class_stats['TP'] + class_stats['FN']) if int(\r\n",
      "                    class_stats['TP'] + class_stats['FN']) != 0 else 0\r\n",
      "                TNR = class_stats['TN'] / (class_stats['TN'] + class_stats['FP']) if int(\r\n",
      "                    class_stats['TN'] + class_stats['FP']) != 0 else 0\r\n",
      "                PPV = class_stats['TP'] / (class_stats['TP'] + class_stats['FP']) if int(\r\n",
      "                    class_stats['TP'] + class_stats['FP']) != 0 else 0\r\n",
      "                NPV = class_stats['TN'] / (class_stats['TN'] + class_stats['FN']) if int(\r\n",
      "                    class_stats['TN'] + class_stats['FN']) != 0 else 0\r\n",
      "                F1 = 2 * class_stats['TP'] / (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\r\n",
      "                    (2 * class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\r\n",
      "                TS = class_stats['TP'] / (class_stats['TP'] + class_stats['FP'] + class_stats['FN']) if int(\r\n",
      "                    (class_stats['TP'] + class_stats['FP'] + class_stats['FN'])) != 0 else 0\r\n",
      "                print(\" \" * 4 + \"{:>{}} | {:>{}} {:>{}} {:>{}} {:>{}} {:>7} {:>7} {:>7} {:>7} {:>7} {:>7}\".format(raw_class,\r\n",
      "                                                                                                                  max_class_name_len,\r\n",
      "                                                                                                                  class_stats['TP'],\r\n",
      "                                                                                                                  max_TP_len,\r\n",
      "                                                                                                                  class_stats['FP'],\r\n",
      "                                                                                                                  max_FP_len,\r\n",
      "                                                                                                                  class_stats['TN'],\r\n",
      "                                                                                                                  max_TN_len,\r\n",
      "                                                                                                                  class_stats['FN'],\r\n",
      "                                                                                                                  max_FN_len,\r\n",
      "                                                                                                                  \"{:0.2f}%\".format(\r\n",
      "                                                                                                                      round(100.0 * TPR, 2)),\r\n",
      "                                                                                                                  \"{:0.2f}%\".format(\r\n",
      "                                                                                                                      round(100.0 * TNR, 2)),\r\n",
      "                                                                                                                  \"{:0.2f}%\".format(\r\n",
      "                                                                                                                      round(100.0 * PPV, 2)),\r\n",
      "                                                                                                                  \"{:0.2f}%\".format(\r\n",
      "                                                                                                                      round(100.0 * NPV, 2)),\r\n",
      "                                                                                                                  \"{:0.2f}%\".format(\r\n",
      "                                                                                                                      round(100.0 * F1, 2)),\r\n",
      "                                                                                                                  \"{:0.2f}%\".format(\r\n",
      "                                                                                                                      round(100.0 * TS, 2))))\r\n",
      "            \r\n",
      "    os.remove(cleanfile)\r\n",
      "    \r\n"
     ]
    }
   ],
   "source": [
    "!cat predictor_104.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05028412",
   "metadata": {},
   "source": [
    "## 2. Classify\n",
    "The predictor can classify a similar data set but without the target column.\n",
    "It is stored into 104_classifications.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ea137",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} predictor_104.py data/titanic_predict.csv > 104_classifications.csv\n",
    "! echo \"predictor exit code = \" $?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a2bf9",
   "metadata": {},
   "source": [
    "### View 104_classifications.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857ef2e",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "classifications_output = pd.read_csv('104_classifications.csv')\n",
    "classifications_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da3aed",
   "metadata": {},
   "source": [
    "## 3.Classification Probabilities\n",
    "Brianome can generate prediction probabilities. \n",
    "This example shows how to use the predictor within your own project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from predictor_104 import predict\n",
    "predict_data = pd.read_csv('data/titanic_predict.csv', na_values=[], na_filter=False)\n",
    "predict_values = predict_data.values\n",
    "probabilities_output = predict(predict_values, return_probabilities=True)\n",
    "print(probabilities_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e96ba",
   "metadata": {},
   "source": [
    "### View Prediction Probabilities with features\n",
    "Concatenate the predictions with the source features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd74fd4",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predict_header = predict_data.columns.values\n",
    "full_output = np.concatenate((\n",
    "    np.concatenate((predict_header.reshape(1, -1), predict_data)), probabilities_output), axis=1)\n",
    "# print(full_output[:10])\n",
    "pd.DataFrame(full_output).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c337d6",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Check out [Brainome 200 Measurements](./brainome_200_Measurements.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
